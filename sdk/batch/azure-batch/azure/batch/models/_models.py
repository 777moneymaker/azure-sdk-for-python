# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------

import datetime
from typing import Any, List, Optional, TYPE_CHECKING, Union

from azure.core.exceptions import HttpResponseError
import msrest.serialization

if TYPE_CHECKING:
    # pylint: disable=unused-import,ungrouped-imports
    import __init__ as _models


class AccountListSupportedImagesResult(msrest.serialization.Model):
    """The result of listing the supported Virtual Machine Images.

    :ivar value: The list of supported Virtual Machine Images.
    :vartype value: list[~azure-batch.models.ImageInformation]
    :ivar odata_next_link: The URL to get the next set of results.
    :vartype odata_next_link: str
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[ImageInformation]'},
        'odata_next_link': {'key': 'odata\\.nextLink', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["_models.ImageInformation"]] = None,
        odata_next_link: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword value: The list of supported Virtual Machine Images.
        :paramtype value: list[~azure-batch.models.ImageInformation]
        :keyword odata_next_link: The URL to get the next set of results.
        :paramtype odata_next_link: str
        """
        super(AccountListSupportedImagesResult, self).__init__(**kwargs)
        self.value = value
        self.odata_next_link = odata_next_link


class AffinityInformation(msrest.serialization.Model):
    """A locality hint that can be used by the Batch service to select a Compute Node on which to start a Task.

    All required parameters must be populated in order to send to Azure.

    :ivar affinity_id: Required. You can pass the affinityId of a Node to indicate that this Task
     needs to run on that Compute Node. Note that this is just a soft affinity. If the target
     Compute Node is busy or unavailable at the time the Task is scheduled, then the Task will be
     scheduled elsewhere.
    :vartype affinity_id: str
    """

    _validation = {
        'affinity_id': {'required': True},
    }

    _attribute_map = {
        'affinity_id': {'key': 'affinityId', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        affinity_id: str,
        **kwargs
    ):
        """
        :keyword affinity_id: Required. You can pass the affinityId of a Node to indicate that this
         Task needs to run on that Compute Node. Note that this is just a soft affinity. If the target
         Compute Node is busy or unavailable at the time the Task is scheduled, then the Task will be
         scheduled elsewhere.
        :paramtype affinity_id: str
        """
        super(AffinityInformation, self).__init__(**kwargs)
        self.affinity_id = affinity_id


class Application(msrest.serialization.Model):
    """Contains information about an application in an Azure Batch Account.

    All required parameters must be populated in order to send to Azure.

    :ivar id: Required. A string that uniquely identifies the application within the Account.
    :vartype id: str
    :ivar display_name: Required. The display name for the application.
    :vartype display_name: str
    :ivar versions: Required. The list of available versions of the application.
    :vartype versions: list[str]
    """

    _validation = {
        'id': {'required': True},
        'display_name': {'required': True},
        'versions': {'required': True},
    }

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'display_name': {'key': 'displayName', 'type': 'str'},
        'versions': {'key': 'versions', 'type': '[str]'},
    }

    def __init__(
        self,
        *,
        id: str,
        display_name: str,
        versions: List[str],
        **kwargs
    ):
        """
        :keyword id: Required. A string that uniquely identifies the application within the Account.
        :paramtype id: str
        :keyword display_name: Required. The display name for the application.
        :paramtype display_name: str
        :keyword versions: Required. The list of available versions of the application.
        :paramtype versions: list[str]
        """
        super(Application, self).__init__(**kwargs)
        self.id = id
        self.display_name = display_name
        self.versions = versions


class ApplicationListResult(msrest.serialization.Model):
    """The result of listing the applications available in an Account.

    :ivar value: The list of applications available in the Account.
    :vartype value: list[~azure-batch.models.Application]
    :ivar odata_next_link: The URL to get the next set of results.
    :vartype odata_next_link: str
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[Application]'},
        'odata_next_link': {'key': 'odata\\.nextLink', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["_models.Application"]] = None,
        odata_next_link: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword value: The list of applications available in the Account.
        :paramtype value: list[~azure-batch.models.Application]
        :keyword odata_next_link: The URL to get the next set of results.
        :paramtype odata_next_link: str
        """
        super(ApplicationListResult, self).__init__(**kwargs)
        self.value = value
        self.odata_next_link = odata_next_link


class ApplicationPackageReference(msrest.serialization.Model):
    """A reference to an Package to be deployed to Compute Nodes.

    All required parameters must be populated in order to send to Azure.

    :ivar application_id: Required. The ID of the application to deploy.
    :vartype application_id: str
    :ivar version: If this is omitted on a Pool, and no default version is specified for this
     application, the request fails with the error code InvalidApplicationPackageReferences and HTTP
     status code 409. If this is omitted on a Task, and no default version is specified for this
     application, the Task fails with a pre-processing error.
    :vartype version: str
    """

    _validation = {
        'application_id': {'required': True},
    }

    _attribute_map = {
        'application_id': {'key': 'applicationId', 'type': 'str'},
        'version': {'key': 'version', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        application_id: str,
        version: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword application_id: Required. The ID of the application to deploy.
        :paramtype application_id: str
        :keyword version: If this is omitted on a Pool, and no default version is specified for this
         application, the request fails with the error code InvalidApplicationPackageReferences and HTTP
         status code 409. If this is omitted on a Task, and no default version is specified for this
         application, the Task fails with a pre-processing error.
        :paramtype version: str
        """
        super(ApplicationPackageReference, self).__init__(**kwargs)
        self.application_id = application_id
        self.version = version


class AuthenticationTokenSettings(msrest.serialization.Model):
    """The settings for an authentication token that the Task can use to perform Batch service operations.

    :ivar access: The authentication token grants access to a limited set of Batch service
     operations. Currently the only supported value for the access property is 'job', which grants
     access to all operations related to the Job which contains the Task.
    :vartype access: list[str]
    """

    _attribute_map = {
        'access': {'key': 'access', 'type': '[str]'},
    }

    def __init__(
        self,
        *,
        access: Optional[List[str]] = None,
        **kwargs
    ):
        """
        :keyword access: The authentication token grants access to a limited set of Batch service
         operations. Currently the only supported value for the access property is 'job', which grants
         access to all operations related to the Job which contains the Task.
        :paramtype access: list[str]
        """
        super(AuthenticationTokenSettings, self).__init__(**kwargs)
        self.access = access


class AutoPoolSpecification(msrest.serialization.Model):
    """Specifies characteristics for a temporary 'auto pool'. The Batch service will create this auto Pool when the Job is submitted.

    All required parameters must be populated in order to send to Azure.

    :ivar auto_pool_id_prefix: The Batch service assigns each auto Pool a unique identifier on
     creation. To distinguish between Pools created for different purposes, you can specify this
     element to add a prefix to the ID that is assigned. The prefix can be up to 20 characters long.
    :vartype auto_pool_id_prefix: str
    :ivar pool_lifetime_option: Required. The minimum lifetime of created auto Pools, and how
     multiple Jobs on a schedule are assigned to Pools. Known values are: "jobschedule", "job".
    :vartype pool_lifetime_option: str or ~azure-batch.models.PoolLifetimeOption
    :ivar keep_alive: If false, the Batch service deletes the Pool once its lifetime (as determined
     by the poolLifetimeOption setting) expires; that is, when the Job or Job Schedule completes. If
     true, the Batch service does not delete the Pool automatically. It is up to the user to delete
     auto Pools created with this option.
    :vartype keep_alive: bool
    :ivar pool: Specification for creating a new Pool.
    :vartype pool: ~azure-batch.models.PoolSpecification
    """

    _validation = {
        'pool_lifetime_option': {'required': True},
    }

    _attribute_map = {
        'auto_pool_id_prefix': {'key': 'autoPoolIdPrefix', 'type': 'str'},
        'pool_lifetime_option': {'key': 'poolLifetimeOption', 'type': 'str'},
        'keep_alive': {'key': 'keepAlive', 'type': 'bool'},
        'pool': {'key': 'pool', 'type': 'PoolSpecification'},
    }

    def __init__(
        self,
        *,
        pool_lifetime_option: Union[str, "_models.PoolLifetimeOption"],
        auto_pool_id_prefix: Optional[str] = None,
        keep_alive: Optional[bool] = None,
        pool: Optional["_models.PoolSpecification"] = None,
        **kwargs
    ):
        """
        :keyword auto_pool_id_prefix: The Batch service assigns each auto Pool a unique identifier on
         creation. To distinguish between Pools created for different purposes, you can specify this
         element to add a prefix to the ID that is assigned. The prefix can be up to 20 characters long.
        :paramtype auto_pool_id_prefix: str
        :keyword pool_lifetime_option: Required. The minimum lifetime of created auto Pools, and how
         multiple Jobs on a schedule are assigned to Pools. Known values are: "jobschedule", "job".
        :paramtype pool_lifetime_option: str or ~azure-batch.models.PoolLifetimeOption
        :keyword keep_alive: If false, the Batch service deletes the Pool once its lifetime (as
         determined by the poolLifetimeOption setting) expires; that is, when the Job or Job Schedule
         completes. If true, the Batch service does not delete the Pool automatically. It is up to the
         user to delete auto Pools created with this option.
        :paramtype keep_alive: bool
        :keyword pool: Specification for creating a new Pool.
        :paramtype pool: ~azure-batch.models.PoolSpecification
        """
        super(AutoPoolSpecification, self).__init__(**kwargs)
        self.auto_pool_id_prefix = auto_pool_id_prefix
        self.pool_lifetime_option = pool_lifetime_option
        self.keep_alive = keep_alive
        self.pool = pool


class AutoScaleRun(msrest.serialization.Model):
    """The results and errors from an execution of a Pool autoscale formula.

    All required parameters must be populated in order to send to Azure.

    :ivar timestamp: Required. The time at which the autoscale formula was last evaluated.
    :vartype timestamp: ~datetime.datetime
    :ivar results: Each variable value is returned in the form $variable=value, and variables are
     separated by semicolons.
    :vartype results: str
    :ivar error: An error that occurred when executing or evaluating a Pool autoscale formula.
    :vartype error: ~azure-batch.models.AutoScaleRunError
    """

    _validation = {
        'timestamp': {'required': True},
    }

    _attribute_map = {
        'timestamp': {'key': 'timestamp', 'type': 'iso-8601'},
        'results': {'key': 'results', 'type': 'str'},
        'error': {'key': 'error', 'type': 'AutoScaleRunError'},
    }

    def __init__(
        self,
        *,
        timestamp: datetime.datetime,
        results: Optional[str] = None,
        error: Optional["_models.AutoScaleRunError"] = None,
        **kwargs
    ):
        """
        :keyword timestamp: Required. The time at which the autoscale formula was last evaluated.
        :paramtype timestamp: ~datetime.datetime
        :keyword results: Each variable value is returned in the form $variable=value, and variables
         are separated by semicolons.
        :paramtype results: str
        :keyword error: An error that occurred when executing or evaluating a Pool autoscale formula.
        :paramtype error: ~azure-batch.models.AutoScaleRunError
        """
        super(AutoScaleRun, self).__init__(**kwargs)
        self.timestamp = timestamp
        self.results = results
        self.error = error


class AutoScaleRunError(msrest.serialization.Model):
    """An error that occurred when executing or evaluating a Pool autoscale formula.

    :ivar code: An identifier for the autoscale error. Codes are invariant and are intended to be
     consumed programmatically.
    :vartype code: str
    :ivar message: A message describing the autoscale error, intended to be suitable for display in
     a user interface.
    :vartype message: str
    :ivar values: A list of additional error details related to the autoscale error.
    :vartype values: list[~azure-batch.models.NameValuePair]
    """

    _attribute_map = {
        'code': {'key': 'code', 'type': 'str'},
        'message': {'key': 'message', 'type': 'str'},
        'values': {'key': 'values', 'type': '[NameValuePair]'},
    }

    def __init__(
        self,
        *,
        code: Optional[str] = None,
        message: Optional[str] = None,
        values: Optional[List["_models.NameValuePair"]] = None,
        **kwargs
    ):
        """
        :keyword code: An identifier for the autoscale error. Codes are invariant and are intended to
         be consumed programmatically.
        :paramtype code: str
        :keyword message: A message describing the autoscale error, intended to be suitable for display
         in a user interface.
        :paramtype message: str
        :keyword values: A list of additional error details related to the autoscale error.
        :paramtype values: list[~azure-batch.models.NameValuePair]
        """
        super(AutoScaleRunError, self).__init__(**kwargs)
        self.code = code
        self.message = message
        self.values = values


class AutoUserSpecification(msrest.serialization.Model):
    """Specifies the parameters for the auto user that runs a Task on the Batch service.

    :ivar scope: The default value is pool. If the pool is running Windows a value of Task should
     be specified if stricter isolation between tasks is required. For example, if the task mutates
     the registry in a way which could impact other tasks, or if certificates have been specified on
     the pool which should not be accessible by normal tasks but should be accessible by StartTasks.
     Known values are: "task", "pool".
    :vartype scope: str or ~azure-batch.models.AutoUserScope
    :ivar elevation_level: The default value is nonAdmin. Known values are: "nonadmin", "admin".
    :vartype elevation_level: str or ~azure-batch.models.ElevationLevel
    """

    _attribute_map = {
        'scope': {'key': 'scope', 'type': 'str'},
        'elevation_level': {'key': 'elevationLevel', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        scope: Optional[Union[str, "_models.AutoUserScope"]] = None,
        elevation_level: Optional[Union[str, "_models.ElevationLevel"]] = None,
        **kwargs
    ):
        """
        :keyword scope: The default value is pool. If the pool is running Windows a value of Task
         should be specified if stricter isolation between tasks is required. For example, if the task
         mutates the registry in a way which could impact other tasks, or if certificates have been
         specified on the pool which should not be accessible by normal tasks but should be accessible
         by StartTasks. Known values are: "task", "pool".
        :paramtype scope: str or ~azure-batch.models.AutoUserScope
        :keyword elevation_level: The default value is nonAdmin. Known values are: "nonadmin", "admin".
        :paramtype elevation_level: str or ~azure-batch.models.ElevationLevel
        """
        super(AutoUserSpecification, self).__init__(**kwargs)
        self.scope = scope
        self.elevation_level = elevation_level


class AzureBlobFileSystemConfiguration(msrest.serialization.Model):
    """Information used to connect to an Azure Storage Container using Blobfuse.

    All required parameters must be populated in order to send to Azure.

    :ivar account_name: Required. The Azure Storage Account name.
    :vartype account_name: str
    :ivar container_name: Required. The Azure Blob Storage Container name.
    :vartype container_name: str
    :ivar account_key: This property is mutually exclusive with both sasKey and identity; exactly
     one must be specified.
    :vartype account_key: str
    :ivar sas_key: This property is mutually exclusive with both accountKey and identity; exactly
     one must be specified.
    :vartype sas_key: str
    :ivar blobfuse_options: These are 'net use' options in Windows and 'mount' options in Linux.
    :vartype blobfuse_options: str
    :ivar relative_mount_path: Required. All file systems are mounted relative to the Batch mounts
     directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
    :vartype relative_mount_path: str
    :ivar identity_reference: This property is mutually exclusive with both accountKey and sasKey;
     exactly one must be specified.
    :vartype identity_reference: ~azure-batch.models.ComputeNodeIdentityReference
    """

    _validation = {
        'account_name': {'required': True},
        'container_name': {'required': True},
        'relative_mount_path': {'required': True},
    }

    _attribute_map = {
        'account_name': {'key': 'accountName', 'type': 'str'},
        'container_name': {'key': 'containerName', 'type': 'str'},
        'account_key': {'key': 'accountKey', 'type': 'str'},
        'sas_key': {'key': 'sasKey', 'type': 'str'},
        'blobfuse_options': {'key': 'blobfuseOptions', 'type': 'str'},
        'relative_mount_path': {'key': 'relativeMountPath', 'type': 'str'},
        'identity_reference': {'key': 'identityReference', 'type': 'ComputeNodeIdentityReference'},
    }

    def __init__(
        self,
        *,
        account_name: str,
        container_name: str,
        relative_mount_path: str,
        account_key: Optional[str] = None,
        sas_key: Optional[str] = None,
        blobfuse_options: Optional[str] = None,
        identity_reference: Optional["_models.ComputeNodeIdentityReference"] = None,
        **kwargs
    ):
        """
        :keyword account_name: Required. The Azure Storage Account name.
        :paramtype account_name: str
        :keyword container_name: Required. The Azure Blob Storage Container name.
        :paramtype container_name: str
        :keyword account_key: This property is mutually exclusive with both sasKey and identity;
         exactly one must be specified.
        :paramtype account_key: str
        :keyword sas_key: This property is mutually exclusive with both accountKey and identity;
         exactly one must be specified.
        :paramtype sas_key: str
        :keyword blobfuse_options: These are 'net use' options in Windows and 'mount' options in Linux.
        :paramtype blobfuse_options: str
        :keyword relative_mount_path: Required. All file systems are mounted relative to the Batch
         mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        :paramtype relative_mount_path: str
        :keyword identity_reference: This property is mutually exclusive with both accountKey and
         sasKey; exactly one must be specified.
        :paramtype identity_reference: ~azure-batch.models.ComputeNodeIdentityReference
        """
        super(AzureBlobFileSystemConfiguration, self).__init__(**kwargs)
        self.account_name = account_name
        self.container_name = container_name
        self.account_key = account_key
        self.sas_key = sas_key
        self.blobfuse_options = blobfuse_options
        self.relative_mount_path = relative_mount_path
        self.identity_reference = identity_reference


class AzureFileShareConfiguration(msrest.serialization.Model):
    """Information used to connect to an Azure Fileshare.

    All required parameters must be populated in order to send to Azure.

    :ivar account_name: Required. The Azure Storage account name.
    :vartype account_name: str
    :ivar azure_file_url: Required. This is of the form 'https://{account}.file.core.windows.net/'.
    :vartype azure_file_url: str
    :ivar account_key: Required. The Azure Storage account key.
    :vartype account_key: str
    :ivar relative_mount_path: Required. All file systems are mounted relative to the Batch mounts
     directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
    :vartype relative_mount_path: str
    :ivar mount_options: These are 'net use' options in Windows and 'mount' options in Linux.
    :vartype mount_options: str
    """

    _validation = {
        'account_name': {'required': True},
        'azure_file_url': {'required': True},
        'account_key': {'required': True},
        'relative_mount_path': {'required': True},
    }

    _attribute_map = {
        'account_name': {'key': 'accountName', 'type': 'str'},
        'azure_file_url': {'key': 'azureFileUrl', 'type': 'str'},
        'account_key': {'key': 'accountKey', 'type': 'str'},
        'relative_mount_path': {'key': 'relativeMountPath', 'type': 'str'},
        'mount_options': {'key': 'mountOptions', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        account_name: str,
        azure_file_url: str,
        account_key: str,
        relative_mount_path: str,
        mount_options: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword account_name: Required. The Azure Storage account name.
        :paramtype account_name: str
        :keyword azure_file_url: Required. This is of the form
         'https://{account}.file.core.windows.net/'.
        :paramtype azure_file_url: str
        :keyword account_key: Required. The Azure Storage account key.
        :paramtype account_key: str
        :keyword relative_mount_path: Required. All file systems are mounted relative to the Batch
         mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        :paramtype relative_mount_path: str
        :keyword mount_options: These are 'net use' options in Windows and 'mount' options in Linux.
        :paramtype mount_options: str
        """
        super(AzureFileShareConfiguration, self).__init__(**kwargs)
        self.account_name = account_name
        self.azure_file_url = azure_file_url
        self.account_key = account_key
        self.relative_mount_path = relative_mount_path
        self.mount_options = mount_options


class BatchError(msrest.serialization.Model):
    """An error response received from the Azure Batch service.

    :ivar code: An identifier for the error. Codes are invariant and are intended to be consumed
     programmatically.
    :vartype code: str
    :ivar message: An error message received in an Azure Batch error response.
    :vartype message: ~azure-batch.models.ErrorMessage
    :ivar values: A collection of key-value pairs containing additional details about the error.
    :vartype values: list[~azure-batch.models.BatchErrorDetail]
    """

    _attribute_map = {
        'code': {'key': 'code', 'type': 'str'},
        'message': {'key': 'message', 'type': 'ErrorMessage'},
        'values': {'key': 'values', 'type': '[BatchErrorDetail]'},
    }

    def __init__(
        self,
        *,
        code: Optional[str] = None,
        message: Optional["_models.ErrorMessage"] = None,
        values: Optional[List["_models.BatchErrorDetail"]] = None,
        **kwargs
    ):
        """
        :keyword code: An identifier for the error. Codes are invariant and are intended to be consumed
         programmatically.
        :paramtype code: str
        :keyword message: An error message received in an Azure Batch error response.
        :paramtype message: ~azure-batch.models.ErrorMessage
        :keyword values: A collection of key-value pairs containing additional details about the error.
        :paramtype values: list[~azure-batch.models.BatchErrorDetail]
        """
        super(BatchError, self).__init__(**kwargs)
        self.code = code
        self.message = message
        self.values = values


class BatchErrorDetail(msrest.serialization.Model):
    """An item of additional information included in an Azure Batch error response.

    :ivar key: An identifier specifying the meaning of the Value property.
    :vartype key: str
    :ivar value: The additional information included with the error response.
    :vartype value: str
    """

    _attribute_map = {
        'key': {'key': 'key', 'type': 'str'},
        'value': {'key': 'value', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        key: Optional[str] = None,
        value: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword key: An identifier specifying the meaning of the Value property.
        :paramtype key: str
        :keyword value: The additional information included with the error response.
        :paramtype value: str
        """
        super(BatchErrorDetail, self).__init__(**kwargs)
        self.key = key
        self.value = value


class BatchJob(msrest.serialization.Model):
    """An Azure Batch Job.

    Variables are only populated by the server, and will be ignored when sending a request.

    :ivar id: The ID is case-preserving and case-insensitive (that is, you may not have two IDs
     within an Account that differ only by case).
    :vartype id: str
    :ivar display_name: The display name for the Job.
    :vartype display_name: str
    :ivar uses_task_dependencies: Whether Tasks in the Job can define dependencies on each other.
     The default is false.
    :vartype uses_task_dependencies: bool
    :ivar url: The URL of the Job.
    :vartype url: str
    :ivar e_tag: This is an opaque string. You can use it to detect whether the Job has changed
     between requests. In particular, you can be pass the ETag when updating a Job to specify that
     your changes should take effect only if nobody else has modified the Job in the meantime.
    :vartype e_tag: str
    :ivar last_modified: This is the last time at which the Job level data, such as the Job state
     or priority, changed. It does not factor in task-level changes such as adding new Tasks or
     Tasks changing state.
    :vartype last_modified: ~datetime.datetime
    :ivar creation_time: The creation time of the Job.
    :vartype creation_time: ~datetime.datetime
    :ivar state: The state of the Job. Known values are: "active", "disabling", "disabled",
     "enabling", "terminating", "completed", "deleting".
    :vartype state: str or ~azure-batch.models.JobState
    :ivar state_transition_time: The time at which the Job entered its current state.
    :vartype state_transition_time: ~datetime.datetime
    :ivar previous_state: This property is not set if the Job is in its initial Active state. Known
     values are: "active", "disabling", "disabled", "enabling", "terminating", "completed",
     "deleting".
    :vartype previous_state: str or ~azure-batch.models.JobState
    :ivar previous_state_transition_time: This property is not set if the Job is in its initial
     Active state.
    :vartype previous_state_transition_time: ~datetime.datetime
    :ivar priority: Priority values can range from -1000 to 1000, with -1000 being the lowest
     priority and 1000 being the highest priority. The default value is 0.
    :vartype priority: int
    :ivar allow_task_preemption: If the value is set to True, other high priority jobs submitted to
     the system will take precedence and will be able requeue tasks from this job. You can update a
     job's allowTaskPreemption after it has been created using the update job API.
    :vartype allow_task_preemption: bool
    :ivar max_parallel_tasks: The value of maxParallelTasks must be -1 or greater than 0 if
     specified. If not specified, the default value is -1, which means there's no limit to the
     number of tasks that can be run at once. You can update a job's maxParallelTasks after it has
     been created using the update job API.
    :vartype max_parallel_tasks: int
    :ivar constraints: The execution constraints for a Job.
    :vartype constraints: ~azure-batch.models.JobConstraints
    :ivar job_manager_task: The Job Manager Task is automatically started when the Job is created.
     The Batch service tries to schedule the Job Manager Task before any other Tasks in the Job.
     When shrinking a Pool, the Batch service tries to preserve Nodes where Job Manager Tasks are
     running for as long as possible (that is, Compute Nodes running 'normal' Tasks are removed
     before Compute Nodes running Job Manager Tasks). When a Job Manager Task fails and needs to be
     restarted, the system tries to schedule it at the highest priority. If there are no idle
     Compute Nodes available, the system may terminate one of the running Tasks in the Pool and
     return it to the queue in order to make room for the Job Manager Task to restart. Note that a
     Job Manager Task in one Job does not have priority over Tasks in other Jobs. Across Jobs, only
     Job level priorities are observed. For example, if a Job Manager in a priority 0 Job needs to
     be restarted, it will not displace Tasks of a priority 1 Job. Batch will retry Tasks when a
     recovery operation is triggered on a Node. Examples of recovery operations include (but are not
     limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host
     failure. Retries due to recovery operations are independent of and are not counted against the
     maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery
     operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to
     tolerate being interrupted and restarted without causing any corruption or duplicate data. The
     best practice for long running Tasks is to use some form of checkpointing.
    :vartype job_manager_task: ~azure-batch.models.JobManagerTask
    :ivar job_preparation_task: The Job Preparation Task is a special Task run on each Compute Node
     before any other Task of the Job.
    :vartype job_preparation_task: ~azure-batch.models.JobPreparationTask
    :ivar job_release_task: The Job Release Task is a special Task run at the end of the Job on
     each Compute Node that has run any other Task of the Job.
    :vartype job_release_task: ~azure-batch.models.JobReleaseTask
    :ivar common_environment_settings: Individual Tasks can override an environment setting
     specified here by specifying the same setting name with a different value.
    :vartype common_environment_settings: list[~azure-batch.models.EnvironmentSetting]
    :ivar pool_info: Specifies how a Job should be assigned to a Pool.
    :vartype pool_info: ~azure-batch.models.PoolInformation
    :ivar on_all_tasks_complete: The default is noaction. Known values are: "noaction",
     "terminatejob".
    :vartype on_all_tasks_complete: str or ~azure-batch.models.OnAllTasksComplete
    :ivar on_task_failure: A Task is considered to have failed if has a failureInfo. A failureInfo
     is set if the Task completes with a non-zero exit code after exhausting its retry count, or if
     there was an error starting the Task, for example due to a resource file download error. The
     default is noaction. Known values are: "noaction", "performexitoptionsjobaction".
    :vartype on_task_failure: str or ~azure-batch.models.OnTaskFailure
    :ivar network_configuration: The network configuration for the Job.
    :vartype network_configuration: ~azure-batch.models.JobNetworkConfiguration
    :ivar metadata: The Batch service does not assign any meaning to metadata; it is solely for the
     use of user code.
    :vartype metadata: list[~azure-batch.models.MetadataItem]
    :ivar execution_info: Contains information about the execution of a Job in the Azure Batch
     service.
    :vartype execution_info: ~azure-batch.models.JobExecutionInformation
    :ivar stats: This property is populated only if the CloudJob was retrieved with an expand
     clause including the 'stats' attribute; otherwise it is null. The statistics may not be
     immediately available. The Batch service performs periodic roll-up of statistics. The typical
     delay is about 30 minutes.
    :vartype stats: ~azure-batch.models.JobStatistics
    """

    _validation = {
        'url': {'readonly': True},
        'e_tag': {'readonly': True},
        'last_modified': {'readonly': True},
        'creation_time': {'readonly': True},
        'state': {'readonly': True},
        'state_transition_time': {'readonly': True},
        'previous_state': {'readonly': True},
        'previous_state_transition_time': {'readonly': True},
        'execution_info': {'readonly': True},
        'stats': {'readonly': True},
    }

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'display_name': {'key': 'displayName', 'type': 'str'},
        'uses_task_dependencies': {'key': 'usesTaskDependencies', 'type': 'bool'},
        'url': {'key': 'url', 'type': 'str'},
        'e_tag': {'key': 'eTag', 'type': 'str'},
        'last_modified': {'key': 'lastModified', 'type': 'iso-8601'},
        'creation_time': {'key': 'creationTime', 'type': 'iso-8601'},
        'state': {'key': 'state', 'type': 'str'},
        'state_transition_time': {'key': 'stateTransitionTime', 'type': 'iso-8601'},
        'previous_state': {'key': 'previousState', 'type': 'str'},
        'previous_state_transition_time': {'key': 'previousStateTransitionTime', 'type': 'iso-8601'},
        'priority': {'key': 'priority', 'type': 'int'},
        'allow_task_preemption': {'key': 'allowTaskPreemption', 'type': 'bool'},
        'max_parallel_tasks': {'key': 'maxParallelTasks', 'type': 'int'},
        'constraints': {'key': 'constraints', 'type': 'JobConstraints'},
        'job_manager_task': {'key': 'jobManagerTask', 'type': 'JobManagerTask'},
        'job_preparation_task': {'key': 'jobPreparationTask', 'type': 'JobPreparationTask'},
        'job_release_task': {'key': 'jobReleaseTask', 'type': 'JobReleaseTask'},
        'common_environment_settings': {'key': 'commonEnvironmentSettings', 'type': '[EnvironmentSetting]'},
        'pool_info': {'key': 'poolInfo', 'type': 'PoolInformation'},
        'on_all_tasks_complete': {'key': 'onAllTasksComplete', 'type': 'str'},
        'on_task_failure': {'key': 'onTaskFailure', 'type': 'str'},
        'network_configuration': {'key': 'networkConfiguration', 'type': 'JobNetworkConfiguration'},
        'metadata': {'key': 'metadata', 'type': '[MetadataItem]'},
        'execution_info': {'key': 'executionInfo', 'type': 'JobExecutionInformation'},
        'stats': {'key': 'stats', 'type': 'JobStatistics'},
    }

    def __init__(
        self,
        *,
        id: Optional[str] = None,
        display_name: Optional[str] = None,
        uses_task_dependencies: Optional[bool] = None,
        priority: Optional[int] = None,
        allow_task_preemption: Optional[bool] = None,
        max_parallel_tasks: Optional[int] = -1,
        constraints: Optional["_models.JobConstraints"] = None,
        job_manager_task: Optional["_models.JobManagerTask"] = None,
        job_preparation_task: Optional["_models.JobPreparationTask"] = None,
        job_release_task: Optional["_models.JobReleaseTask"] = None,
        common_environment_settings: Optional[List["_models.EnvironmentSetting"]] = None,
        pool_info: Optional["_models.PoolInformation"] = None,
        on_all_tasks_complete: Optional[Union[str, "_models.OnAllTasksComplete"]] = None,
        on_task_failure: Optional[Union[str, "_models.OnTaskFailure"]] = None,
        network_configuration: Optional["_models.JobNetworkConfiguration"] = None,
        metadata: Optional[List["_models.MetadataItem"]] = None,
        **kwargs
    ):
        """
        :keyword id: The ID is case-preserving and case-insensitive (that is, you may not have two IDs
         within an Account that differ only by case).
        :paramtype id: str
        :keyword display_name: The display name for the Job.
        :paramtype display_name: str
        :keyword uses_task_dependencies: Whether Tasks in the Job can define dependencies on each
         other. The default is false.
        :paramtype uses_task_dependencies: bool
        :keyword priority: Priority values can range from -1000 to 1000, with -1000 being the lowest
         priority and 1000 being the highest priority. The default value is 0.
        :paramtype priority: int
        :keyword allow_task_preemption: If the value is set to True, other high priority jobs submitted
         to the system will take precedence and will be able requeue tasks from this job. You can update
         a job's allowTaskPreemption after it has been created using the update job API.
        :paramtype allow_task_preemption: bool
        :keyword max_parallel_tasks: The value of maxParallelTasks must be -1 or greater than 0 if
         specified. If not specified, the default value is -1, which means there's no limit to the
         number of tasks that can be run at once. You can update a job's maxParallelTasks after it has
         been created using the update job API.
        :paramtype max_parallel_tasks: int
        :keyword constraints: The execution constraints for a Job.
        :paramtype constraints: ~azure-batch.models.JobConstraints
        :keyword job_manager_task: The Job Manager Task is automatically started when the Job is
         created. The Batch service tries to schedule the Job Manager Task before any other Tasks in the
         Job. When shrinking a Pool, the Batch service tries to preserve Nodes where Job Manager Tasks
         are running for as long as possible (that is, Compute Nodes running 'normal' Tasks are removed
         before Compute Nodes running Job Manager Tasks). When a Job Manager Task fails and needs to be
         restarted, the system tries to schedule it at the highest priority. If there are no idle
         Compute Nodes available, the system may terminate one of the running Tasks in the Pool and
         return it to the queue in order to make room for the Job Manager Task to restart. Note that a
         Job Manager Task in one Job does not have priority over Tasks in other Jobs. Across Jobs, only
         Job level priorities are observed. For example, if a Job Manager in a priority 0 Job needs to
         be restarted, it will not displace Tasks of a priority 1 Job. Batch will retry Tasks when a
         recovery operation is triggered on a Node. Examples of recovery operations include (but are not
         limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host
         failure. Retries due to recovery operations are independent of and are not counted against the
         maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery
         operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to
         tolerate being interrupted and restarted without causing any corruption or duplicate data. The
         best practice for long running Tasks is to use some form of checkpointing.
        :paramtype job_manager_task: ~azure-batch.models.JobManagerTask
        :keyword job_preparation_task: The Job Preparation Task is a special Task run on each Compute
         Node before any other Task of the Job.
        :paramtype job_preparation_task: ~azure-batch.models.JobPreparationTask
        :keyword job_release_task: The Job Release Task is a special Task run at the end of the Job on
         each Compute Node that has run any other Task of the Job.
        :paramtype job_release_task: ~azure-batch.models.JobReleaseTask
        :keyword common_environment_settings: Individual Tasks can override an environment setting
         specified here by specifying the same setting name with a different value.
        :paramtype common_environment_settings: list[~azure-batch.models.EnvironmentSetting]
        :keyword pool_info: Specifies how a Job should be assigned to a Pool.
        :paramtype pool_info: ~azure-batch.models.PoolInformation
        :keyword on_all_tasks_complete: The default is noaction. Known values are: "noaction",
         "terminatejob".
        :paramtype on_all_tasks_complete: str or ~azure-batch.models.OnAllTasksComplete
        :keyword on_task_failure: A Task is considered to have failed if has a failureInfo. A
         failureInfo is set if the Task completes with a non-zero exit code after exhausting its retry
         count, or if there was an error starting the Task, for example due to a resource file download
         error. The default is noaction. Known values are: "noaction", "performexitoptionsjobaction".
        :paramtype on_task_failure: str or ~azure-batch.models.OnTaskFailure
        :keyword network_configuration: The network configuration for the Job.
        :paramtype network_configuration: ~azure-batch.models.JobNetworkConfiguration
        :keyword metadata: The Batch service does not assign any meaning to metadata; it is solely for
         the use of user code.
        :paramtype metadata: list[~azure-batch.models.MetadataItem]
        """
        super(BatchJob, self).__init__(**kwargs)
        self.id = id
        self.display_name = display_name
        self.uses_task_dependencies = uses_task_dependencies
        self.url = None
        self.e_tag = None
        self.last_modified = None
        self.creation_time = None
        self.state = None
        self.state_transition_time = None
        self.previous_state = None
        self.previous_state_transition_time = None
        self.priority = priority
        self.allow_task_preemption = allow_task_preemption
        self.max_parallel_tasks = max_parallel_tasks
        self.constraints = constraints
        self.job_manager_task = job_manager_task
        self.job_preparation_task = job_preparation_task
        self.job_release_task = job_release_task
        self.common_environment_settings = common_environment_settings
        self.pool_info = pool_info
        self.on_all_tasks_complete = on_all_tasks_complete
        self.on_task_failure = on_task_failure
        self.network_configuration = network_configuration
        self.metadata = metadata
        self.execution_info = None
        self.stats = None


class BatchJobDisableParameters(msrest.serialization.Model):
    """Options when disabling a Job.

    All required parameters must be populated in order to send to Azure.

    :ivar disable_tasks: Required. What to do with active Tasks associated with the Job. Known
     values are: "requeue", "terminate", "wait".
    :vartype disable_tasks: str or ~azure-batch.models.DisableJobOption
    """

    _validation = {
        'disable_tasks': {'required': True},
    }

    _attribute_map = {
        'disable_tasks': {'key': 'disableTasks', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        disable_tasks: Union[str, "_models.DisableJobOption"],
        **kwargs
    ):
        """
        :keyword disable_tasks: Required. What to do with active Tasks associated with the Job. Known
         values are: "requeue", "terminate", "wait".
        :paramtype disable_tasks: str or ~azure-batch.models.DisableJobOption
        """
        super(BatchJobDisableParameters, self).__init__(**kwargs)
        self.disable_tasks = disable_tasks


class BatchJobListPreparationAndReleaseTaskStatusResult(msrest.serialization.Model):
    """The result of listing the status of the Job Preparation and Job Release Tasks for a Job.

    :ivar value: A list of Job Preparation and Job Release Task execution information.
    :vartype value: list[~azure-batch.models.JobPreparationAndReleaseTaskExecutionInformation]
    :ivar odata_next_link: The URL to get the next set of results.
    :vartype odata_next_link: str
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[JobPreparationAndReleaseTaskExecutionInformation]'},
        'odata_next_link': {'key': 'odata\\.nextLink', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["_models.JobPreparationAndReleaseTaskExecutionInformation"]] = None,
        odata_next_link: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword value: A list of Job Preparation and Job Release Task execution information.
        :paramtype value: list[~azure-batch.models.JobPreparationAndReleaseTaskExecutionInformation]
        :keyword odata_next_link: The URL to get the next set of results.
        :paramtype odata_next_link: str
        """
        super(BatchJobListPreparationAndReleaseTaskStatusResult, self).__init__(**kwargs)
        self.value = value
        self.odata_next_link = odata_next_link


class BatchJobListResult(msrest.serialization.Model):
    """The result of listing the Jobs in an Account.

    :ivar value: The list of Jobs.
    :vartype value: list[~azure-batch.models.BatchJob]
    :ivar odata_next_link: The URL to get the next set of results.
    :vartype odata_next_link: str
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[BatchJob]'},
        'odata_next_link': {'key': 'odata\\.nextLink', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["_models.BatchJob"]] = None,
        odata_next_link: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword value: The list of Jobs.
        :paramtype value: list[~azure-batch.models.BatchJob]
        :keyword odata_next_link: The URL to get the next set of results.
        :paramtype odata_next_link: str
        """
        super(BatchJobListResult, self).__init__(**kwargs)
        self.value = value
        self.odata_next_link = odata_next_link


class BatchJobSchedule(msrest.serialization.Model):
    """A Job Schedule that allows recurring Jobs by specifying when to run Jobs and a specification used to create each Job.

    Variables are only populated by the server, and will be ignored when sending a request.

    :ivar id: A string that uniquely identifies the schedule within the Account.
    :vartype id: str
    :ivar display_name: The display name for the schedule.
    :vartype display_name: str
    :ivar url: The URL of the Job Schedule.
    :vartype url: str
    :ivar e_tag: This is an opaque string. You can use it to detect whether the Job Schedule has
     changed between requests. In particular, you can be pass the ETag with an Update Job Schedule
     request to specify that your changes should take effect only if nobody else has modified the
     schedule in the meantime.
    :vartype e_tag: str
    :ivar last_modified: This is the last time at which the schedule level data, such as the Job
     specification or recurrence information, changed. It does not factor in job-level changes such
     as new Jobs being created or Jobs changing state.
    :vartype last_modified: ~datetime.datetime
    :ivar creation_time: The creation time of the Job Schedule.
    :vartype creation_time: ~datetime.datetime
    :ivar state: The state of the Job Schedule. Known values are: "active", "completed",
     "disabled", "terminating", "deleting".
    :vartype state: str or ~azure-batch.models.JobScheduleState
    :ivar state_transition_time: The time at which the Job Schedule entered the current state.
    :vartype state_transition_time: ~datetime.datetime
    :ivar previous_state: This property is not present if the Job Schedule is in its initial active
     state. Known values are: "active", "completed", "disabled", "terminating", "deleting".
    :vartype previous_state: str or ~azure-batch.models.JobScheduleState
    :ivar previous_state_transition_time: This property is not present if the Job Schedule is in
     its initial active state.
    :vartype previous_state_transition_time: ~datetime.datetime
    :ivar schedule: All times are fixed respective to UTC and are not impacted by daylight saving
     time.
    :vartype schedule: ~azure-batch.models.Schedule
    :ivar job_specification: Specifies details of the Jobs to be created on a schedule.
    :vartype job_specification: ~azure-batch.models.JobSpecification
    :ivar execution_info: Contains information about Jobs that have been and will be run under a
     Job Schedule.
    :vartype execution_info: ~azure-batch.models.JobScheduleExecutionInformation
    :ivar metadata: The Batch service does not assign any meaning to metadata; it is solely for the
     use of user code.
    :vartype metadata: list[~azure-batch.models.MetadataItem]
    :ivar stats: Resource usage statistics for a Job Schedule.
    :vartype stats: ~azure-batch.models.JobScheduleStatistics
    """

    _validation = {
        'url': {'readonly': True},
        'e_tag': {'readonly': True},
        'last_modified': {'readonly': True},
        'creation_time': {'readonly': True},
        'state': {'readonly': True},
        'state_transition_time': {'readonly': True},
        'previous_state': {'readonly': True},
        'previous_state_transition_time': {'readonly': True},
        'execution_info': {'readonly': True},
        'stats': {'readonly': True},
    }

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'display_name': {'key': 'displayName', 'type': 'str'},
        'url': {'key': 'url', 'type': 'str'},
        'e_tag': {'key': 'eTag', 'type': 'str'},
        'last_modified': {'key': 'lastModified', 'type': 'iso-8601'},
        'creation_time': {'key': 'creationTime', 'type': 'iso-8601'},
        'state': {'key': 'state', 'type': 'str'},
        'state_transition_time': {'key': 'stateTransitionTime', 'type': 'iso-8601'},
        'previous_state': {'key': 'previousState', 'type': 'str'},
        'previous_state_transition_time': {'key': 'previousStateTransitionTime', 'type': 'iso-8601'},
        'schedule': {'key': 'schedule', 'type': 'Schedule'},
        'job_specification': {'key': 'jobSpecification', 'type': 'JobSpecification'},
        'execution_info': {'key': 'executionInfo', 'type': 'JobScheduleExecutionInformation'},
        'metadata': {'key': 'metadata', 'type': '[MetadataItem]'},
        'stats': {'key': 'stats', 'type': 'JobScheduleStatistics'},
    }

    def __init__(
        self,
        *,
        id: Optional[str] = None,
        display_name: Optional[str] = None,
        schedule: Optional["_models.Schedule"] = None,
        job_specification: Optional["_models.JobSpecification"] = None,
        metadata: Optional[List["_models.MetadataItem"]] = None,
        **kwargs
    ):
        """
        :keyword id: A string that uniquely identifies the schedule within the Account.
        :paramtype id: str
        :keyword display_name: The display name for the schedule.
        :paramtype display_name: str
        :keyword schedule: All times are fixed respective to UTC and are not impacted by daylight
         saving time.
        :paramtype schedule: ~azure-batch.models.Schedule
        :keyword job_specification: Specifies details of the Jobs to be created on a schedule.
        :paramtype job_specification: ~azure-batch.models.JobSpecification
        :keyword metadata: The Batch service does not assign any meaning to metadata; it is solely for
         the use of user code.
        :paramtype metadata: list[~azure-batch.models.MetadataItem]
        """
        super(BatchJobSchedule, self).__init__(**kwargs)
        self.id = id
        self.display_name = display_name
        self.url = None
        self.e_tag = None
        self.last_modified = None
        self.creation_time = None
        self.state = None
        self.state_transition_time = None
        self.previous_state = None
        self.previous_state_transition_time = None
        self.schedule = schedule
        self.job_specification = job_specification
        self.execution_info = None
        self.metadata = metadata
        self.stats = None


class BatchJobScheduleListResult(msrest.serialization.Model):
    """The result of listing the Job Schedules in an Account.

    :ivar value: The list of Job Schedules.
    :vartype value: list[~azure-batch.models.BatchJobSchedule]
    :ivar odata_next_link: The URL to get the next set of results.
    :vartype odata_next_link: str
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[BatchJobSchedule]'},
        'odata_next_link': {'key': 'odata\\.nextLink', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["_models.BatchJobSchedule"]] = None,
        odata_next_link: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword value: The list of Job Schedules.
        :paramtype value: list[~azure-batch.models.BatchJobSchedule]
        :keyword odata_next_link: The URL to get the next set of results.
        :paramtype odata_next_link: str
        """
        super(BatchJobScheduleListResult, self).__init__(**kwargs)
        self.value = value
        self.odata_next_link = odata_next_link


class BatchJobScheduleUpdate(msrest.serialization.Model):
    """The set of changes to be made to a Job Schedule.

    :ivar schedule: All times are fixed respective to UTC and are not impacted by daylight saving
     time. If you do not specify this element, the existing schedule is left unchanged.
    :vartype schedule: ~azure-batch.models.Schedule
    :ivar job_specification: Updates affect only Jobs that are started after the update has taken
     place. Any currently active Job continues with the older specification.
    :vartype job_specification: ~azure-batch.models.JobSpecification
    :ivar metadata: If you do not specify this element, existing metadata is left unchanged.
    :vartype metadata: list[~azure-batch.models.MetadataItem]
    """

    _attribute_map = {
        'schedule': {'key': 'schedule', 'type': 'Schedule'},
        'job_specification': {'key': 'jobSpecification', 'type': 'JobSpecification'},
        'metadata': {'key': 'metadata', 'type': '[MetadataItem]'},
    }

    def __init__(
        self,
        *,
        schedule: Optional["_models.Schedule"] = None,
        job_specification: Optional["_models.JobSpecification"] = None,
        metadata: Optional[List["_models.MetadataItem"]] = None,
        **kwargs
    ):
        """
        :keyword schedule: All times are fixed respective to UTC and are not impacted by daylight
         saving time. If you do not specify this element, the existing schedule is left unchanged.
        :paramtype schedule: ~azure-batch.models.Schedule
        :keyword job_specification: Updates affect only Jobs that are started after the update has
         taken place. Any currently active Job continues with the older specification.
        :paramtype job_specification: ~azure-batch.models.JobSpecification
        :keyword metadata: If you do not specify this element, existing metadata is left unchanged.
        :paramtype metadata: list[~azure-batch.models.MetadataItem]
        """
        super(BatchJobScheduleUpdate, self).__init__(**kwargs)
        self.schedule = schedule
        self.job_specification = job_specification
        self.metadata = metadata


class BatchJobTerminateParameters(msrest.serialization.Model):
    """Options when terminating a Job.

    :ivar terminate_reason: The text you want to appear as the Job's TerminateReason. The default
     is 'UserTerminate'.
    :vartype terminate_reason: str
    """

    _attribute_map = {
        'terminate_reason': {'key': 'terminateReason', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        terminate_reason: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword terminate_reason: The text you want to appear as the Job's TerminateReason. The
         default is 'UserTerminate'.
        :paramtype terminate_reason: str
        """
        super(BatchJobTerminateParameters, self).__init__(**kwargs)
        self.terminate_reason = terminate_reason


class BatchJobUpdate(msrest.serialization.Model):
    """The set of changes to be made to a Job.

    :ivar priority: Priority values can range from -1000 to 1000, with -1000 being the lowest
     priority and 1000 being the highest priority. If omitted, the priority of the Job is left
     unchanged.
    :vartype priority: int
    :ivar max_parallel_tasks: The value of maxParallelTasks must be -1 or greater than 0 if
     specified. If not specified, the default value is -1, which means there's no limit to the
     number of tasks that can be run at once. You can update a job's maxParallelTasks after it has
     been created using the update job API.
    :vartype max_parallel_tasks: int
    :ivar allow_task_preemption: If the value is set to True, other high priority jobs submitted to
     the system will take precedence and will be able requeue tasks from this job. You can update a
     job's allowTaskPreemption after it has been created using the update job API.
    :vartype allow_task_preemption: bool
    :ivar on_all_tasks_complete: If omitted, the completion behavior is left unchanged. You may not
     change the value from terminatejob to noaction - that is, once you have engaged automatic Job
     termination, you cannot turn it off again. If you try to do this, the request fails with an
     'invalid property value' error response; if you are calling the REST API directly, the HTTP
     status code is 400 (Bad Request). Known values are: "noaction", "terminatejob".
    :vartype on_all_tasks_complete: str or ~azure-batch.models.OnAllTasksComplete
    :ivar constraints: If omitted, the existing execution constraints are left unchanged.
    :vartype constraints: ~azure-batch.models.JobConstraints
    :ivar pool_info: You may change the Pool for a Job only when the Job is disabled. The Patch Job
     call will fail if you include the poolInfo element and the Job is not disabled. If you specify
     an autoPoolSpecification in the poolInfo, only the keepAlive property of the
     autoPoolSpecification can be updated, and then only if the autoPoolSpecification has a
     poolLifetimeOption of Job (other job properties can be updated as normal). If omitted, the Job
     continues to run on its current Pool.
    :vartype pool_info: ~azure-batch.models.PoolInformation
    :ivar metadata: If omitted, the existing Job metadata is left unchanged.
    :vartype metadata: list[~azure-batch.models.MetadataItem]
    """

    _attribute_map = {
        'priority': {'key': 'priority', 'type': 'int'},
        'max_parallel_tasks': {'key': 'maxParallelTasks', 'type': 'int'},
        'allow_task_preemption': {'key': 'allowTaskPreemption', 'type': 'bool'},
        'on_all_tasks_complete': {'key': 'onAllTasksComplete', 'type': 'str'},
        'constraints': {'key': 'constraints', 'type': 'JobConstraints'},
        'pool_info': {'key': 'poolInfo', 'type': 'PoolInformation'},
        'metadata': {'key': 'metadata', 'type': '[MetadataItem]'},
    }

    def __init__(
        self,
        *,
        priority: Optional[int] = None,
        max_parallel_tasks: Optional[int] = None,
        allow_task_preemption: Optional[bool] = None,
        on_all_tasks_complete: Optional[Union[str, "_models.OnAllTasksComplete"]] = None,
        constraints: Optional["_models.JobConstraints"] = None,
        pool_info: Optional["_models.PoolInformation"] = None,
        metadata: Optional[List["_models.MetadataItem"]] = None,
        **kwargs
    ):
        """
        :keyword priority: Priority values can range from -1000 to 1000, with -1000 being the lowest
         priority and 1000 being the highest priority. If omitted, the priority of the Job is left
         unchanged.
        :paramtype priority: int
        :keyword max_parallel_tasks: The value of maxParallelTasks must be -1 or greater than 0 if
         specified. If not specified, the default value is -1, which means there's no limit to the
         number of tasks that can be run at once. You can update a job's maxParallelTasks after it has
         been created using the update job API.
        :paramtype max_parallel_tasks: int
        :keyword allow_task_preemption: If the value is set to True, other high priority jobs submitted
         to the system will take precedence and will be able requeue tasks from this job. You can update
         a job's allowTaskPreemption after it has been created using the update job API.
        :paramtype allow_task_preemption: bool
        :keyword on_all_tasks_complete: If omitted, the completion behavior is left unchanged. You may
         not change the value from terminatejob to noaction - that is, once you have engaged automatic
         Job termination, you cannot turn it off again. If you try to do this, the request fails with an
         'invalid property value' error response; if you are calling the REST API directly, the HTTP
         status code is 400 (Bad Request). Known values are: "noaction", "terminatejob".
        :paramtype on_all_tasks_complete: str or ~azure-batch.models.OnAllTasksComplete
        :keyword constraints: If omitted, the existing execution constraints are left unchanged.
        :paramtype constraints: ~azure-batch.models.JobConstraints
        :keyword pool_info: You may change the Pool for a Job only when the Job is disabled. The Patch
         Job call will fail if you include the poolInfo element and the Job is not disabled. If you
         specify an autoPoolSpecification in the poolInfo, only the keepAlive property of the
         autoPoolSpecification can be updated, and then only if the autoPoolSpecification has a
         poolLifetimeOption of Job (other job properties can be updated as normal). If omitted, the Job
         continues to run on its current Pool.
        :paramtype pool_info: ~azure-batch.models.PoolInformation
        :keyword metadata: If omitted, the existing Job metadata is left unchanged.
        :paramtype metadata: list[~azure-batch.models.MetadataItem]
        """
        super(BatchJobUpdate, self).__init__(**kwargs)
        self.priority = priority
        self.max_parallel_tasks = max_parallel_tasks
        self.allow_task_preemption = allow_task_preemption
        self.on_all_tasks_complete = on_all_tasks_complete
        self.constraints = constraints
        self.pool_info = pool_info
        self.metadata = metadata


class BatchPool(msrest.serialization.Model):
    """A Pool in the Azure Batch service.

    Variables are only populated by the server, and will be ignored when sending a request.

    :ivar id: The ID can contain any combination of alphanumeric characters including hyphens and
     underscores, and cannot contain more than 64 characters. The ID is case-preserving and
     case-insensitive (that is, you may not have two IDs within an Account that differ only by
     case).
    :vartype id: str
    :ivar display_name: The display name need not be unique and can contain any Unicode characters
     up to a maximum length of 1024.
    :vartype display_name: str
    :ivar url: The URL of the Pool.
    :vartype url: str
    :ivar e_tag: This is an opaque string. You can use it to detect whether the Pool has changed
     between requests. In particular, you can be pass the ETag when updating a Pool to specify that
     your changes should take effect only if nobody else has modified the Pool in the meantime.
    :vartype e_tag: str
    :ivar last_modified: This is the last time at which the Pool level data, such as the
     targetDedicatedNodes or enableAutoscale settings, changed. It does not factor in node-level
     changes such as a Compute Node changing state.
    :vartype last_modified: ~datetime.datetime
    :ivar creation_time: The creation time of the Pool.
    :vartype creation_time: ~datetime.datetime
    :ivar state: The current state of the Pool. Known values are: "active", "deleting".
    :vartype state: str or ~azure-batch.models.PoolState
    :ivar state_transition_time: The time at which the Pool entered its current state.
    :vartype state_transition_time: ~datetime.datetime
    :ivar allocation_state: Whether the Pool is resizing. Known values are: "steady", "resizing",
     "stopping".
    :vartype allocation_state: str or ~azure-batch.models.AllocationState
    :ivar allocation_state_transition_time: The time at which the Pool entered its current
     allocation state.
    :vartype allocation_state_transition_time: ~datetime.datetime
    :ivar vm_size: For information about available sizes of virtual machines in Pools, see Choose a
     VM size for Compute Nodes in an Azure Batch Pool
     (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
    :vartype vm_size: str
    :ivar cloud_service_configuration: This property and virtualMachineConfiguration are mutually
     exclusive and one of the properties must be specified. This property cannot be specified if the
     Batch Account was created with its poolAllocationMode property set to 'UserSubscription'.
    :vartype cloud_service_configuration: ~azure-batch.models.CloudServiceConfiguration
    :ivar virtual_machine_configuration: This property and cloudServiceConfiguration are mutually
     exclusive and one of the properties must be specified.
    :vartype virtual_machine_configuration: ~azure-batch.models.VirtualMachineConfiguration
    :ivar resize_timeout: This is the timeout for the most recent resize operation. (The initial
     sizing when the Pool is created counts as a resize.) The default value is 15 minutes.
    :vartype resize_timeout: ~datetime.timedelta
    :ivar resize_errors: This property is set only if one or more errors occurred during the last
     Pool resize, and only when the Pool allocationState is Steady.
    :vartype resize_errors: list[~azure-batch.models.ResizeError]
    :ivar current_dedicated_nodes: The number of dedicated Compute Nodes currently in the Pool.
    :vartype current_dedicated_nodes: int
    :ivar current_low_priority_nodes: Spot/Low-priority Compute Nodes which have been preempted are
     included in this count.
    :vartype current_low_priority_nodes: int
    :ivar target_dedicated_nodes: The desired number of dedicated Compute Nodes in the Pool.
    :vartype target_dedicated_nodes: int
    :ivar target_low_priority_nodes: The desired number of Spot/Low-priority Compute Nodes in the
     Pool.
    :vartype target_low_priority_nodes: int
    :ivar enable_auto_scale: If false, at least one of targetDedicatedNodes and
     targetLowPriorityNodes must be specified. If true, the autoScaleFormula property is required
     and the Pool automatically resizes according to the formula. The default value is false.
    :vartype enable_auto_scale: bool
    :ivar auto_scale_formula: This property is set only if the Pool automatically scales, i.e.
     enableAutoScale is true.
    :vartype auto_scale_formula: str
    :ivar auto_scale_evaluation_interval: This property is set only if the Pool automatically
     scales, i.e. enableAutoScale is true.
    :vartype auto_scale_evaluation_interval: ~datetime.timedelta
    :ivar auto_scale_run: This property is set only if the Pool automatically scales, i.e.
     enableAutoScale is true.
    :vartype auto_scale_run: ~azure-batch.models.AutoScaleRun
    :ivar enable_inter_node_communication: This imposes restrictions on which Compute Nodes can be
     assigned to the Pool. Specifying this value can reduce the chance of the requested number of
     Compute Nodes to be allocated in the Pool.
    :vartype enable_inter_node_communication: bool
    :ivar network_configuration: The network configuration for a Pool.
    :vartype network_configuration: ~azure-batch.models.NetworkConfiguration
    :ivar start_task: Batch will retry Tasks when a recovery operation is triggered on a Node.
     Examples of recovery operations include (but are not limited to) when an unhealthy Node is
     rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations
     are independent of and are not counted against the maxTaskRetryCount. Even if the
     maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of
     this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and
     restarted without causing any corruption or duplicate data. The best practice for long running
     Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even
     though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks
     which create breakaway process or install/launch services from the StartTask working directory,
     as this will block Batch from being able to re-run the StartTask.
    :vartype start_task: ~azure-batch.models.StartTask
    :ivar certificate_references: For Windows Nodes, the Batch service installs the Certificates to
     the specified Certificate store and location. For Linux Compute Nodes, the Certificates are
     stored in a directory inside the Task working directory and an environment variable
     AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates
     with visibility of 'remoteUser', a 'certs' directory is created in the user's home directory
     (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
    :vartype certificate_references: list[~azure-batch.models.CertificateReference]
    :ivar application_package_references: Changes to Package references affect all new Nodes
     joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are
     rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
    :vartype application_package_references: list[~azure-batch.models.ApplicationPackageReference]
    :ivar application_licenses: The list of application licenses must be a subset of available
     Batch service application licenses. If a license is requested which is not supported, Pool
     creation will fail.
    :vartype application_licenses: list[str]
    :ivar task_slots_per_node: The default value is 1. The maximum value is the smaller of 4 times
     the number of cores of the vmSize of the pool or 256.
    :vartype task_slots_per_node: int
    :ivar task_scheduling_policy: If not specified, the default is spread.
    :vartype task_scheduling_policy: ~azure-batch.models.TaskSchedulingPolicy
    :ivar user_accounts: The list of user Accounts to be created on each Compute Node in the Pool.
    :vartype user_accounts: list[~azure-batch.models.UserAccount]
    :ivar metadata: A list of name-value pairs associated with the Pool as metadata.
    :vartype metadata: list[~azure-batch.models.MetadataItem]
    :ivar stats: This property is populated only if the CloudPool was retrieved with an expand
     clause including the 'stats' attribute; otherwise it is null. The statistics may not be
     immediately available. The Batch service performs periodic roll-up of statistics. The typical
     delay is about 30 minutes.
    :vartype stats: ~azure-batch.models.PoolStatistics
    :ivar mount_configuration: This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
    :vartype mount_configuration: list[~azure-batch.models.MountConfiguration]
    :ivar identity: The list of user identities associated with the Batch pool. The user identity
     dictionary key references will be ARM resource ids in the form:
     '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/{identityName}'.
    :vartype identity: ~azure-batch.models.BatchPoolIdentity
    """

    _validation = {
        'url': {'readonly': True},
        'e_tag': {'readonly': True},
        'last_modified': {'readonly': True},
        'creation_time': {'readonly': True},
        'state': {'readonly': True},
        'state_transition_time': {'readonly': True},
        'allocation_state': {'readonly': True},
        'allocation_state_transition_time': {'readonly': True},
        'resize_errors': {'readonly': True},
        'current_dedicated_nodes': {'readonly': True},
        'current_low_priority_nodes': {'readonly': True},
        'auto_scale_run': {'readonly': True},
        'stats': {'readonly': True},
        'identity': {'readonly': True},
    }

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'display_name': {'key': 'displayName', 'type': 'str'},
        'url': {'key': 'url', 'type': 'str'},
        'e_tag': {'key': 'eTag', 'type': 'str'},
        'last_modified': {'key': 'lastModified', 'type': 'iso-8601'},
        'creation_time': {'key': 'creationTime', 'type': 'iso-8601'},
        'state': {'key': 'state', 'type': 'str'},
        'state_transition_time': {'key': 'stateTransitionTime', 'type': 'iso-8601'},
        'allocation_state': {'key': 'allocationState', 'type': 'str'},
        'allocation_state_transition_time': {'key': 'allocationStateTransitionTime', 'type': 'iso-8601'},
        'vm_size': {'key': 'vmSize', 'type': 'str'},
        'cloud_service_configuration': {'key': 'cloudServiceConfiguration', 'type': 'CloudServiceConfiguration'},
        'virtual_machine_configuration': {'key': 'virtualMachineConfiguration', 'type': 'VirtualMachineConfiguration'},
        'resize_timeout': {'key': 'resizeTimeout', 'type': 'duration'},
        'resize_errors': {'key': 'resizeErrors', 'type': '[ResizeError]'},
        'current_dedicated_nodes': {'key': 'currentDedicatedNodes', 'type': 'int'},
        'current_low_priority_nodes': {'key': 'currentLowPriorityNodes', 'type': 'int'},
        'target_dedicated_nodes': {'key': 'targetDedicatedNodes', 'type': 'int'},
        'target_low_priority_nodes': {'key': 'targetLowPriorityNodes', 'type': 'int'},
        'enable_auto_scale': {'key': 'enableAutoScale', 'type': 'bool'},
        'auto_scale_formula': {'key': 'autoScaleFormula', 'type': 'str'},
        'auto_scale_evaluation_interval': {'key': 'autoScaleEvaluationInterval', 'type': 'duration'},
        'auto_scale_run': {'key': 'autoScaleRun', 'type': 'AutoScaleRun'},
        'enable_inter_node_communication': {'key': 'enableInterNodeCommunication', 'type': 'bool'},
        'network_configuration': {'key': 'networkConfiguration', 'type': 'NetworkConfiguration'},
        'start_task': {'key': 'startTask', 'type': 'StartTask'},
        'certificate_references': {'key': 'certificateReferences', 'type': '[CertificateReference]'},
        'application_package_references': {'key': 'applicationPackageReferences', 'type': '[ApplicationPackageReference]'},
        'application_licenses': {'key': 'applicationLicenses', 'type': '[str]'},
        'task_slots_per_node': {'key': 'taskSlotsPerNode', 'type': 'int'},
        'task_scheduling_policy': {'key': 'taskSchedulingPolicy', 'type': 'TaskSchedulingPolicy'},
        'user_accounts': {'key': 'userAccounts', 'type': '[UserAccount]'},
        'metadata': {'key': 'metadata', 'type': '[MetadataItem]'},
        'stats': {'key': 'stats', 'type': 'PoolStatistics'},
        'mount_configuration': {'key': 'mountConfiguration', 'type': '[MountConfiguration]'},
        'identity': {'key': 'identity', 'type': 'BatchPoolIdentity'},
    }

    def __init__(
        self,
        *,
        id: Optional[str] = None,
        display_name: Optional[str] = None,
        vm_size: Optional[str] = None,
        cloud_service_configuration: Optional["_models.CloudServiceConfiguration"] = None,
        virtual_machine_configuration: Optional["_models.VirtualMachineConfiguration"] = None,
        resize_timeout: Optional[datetime.timedelta] = None,
        target_dedicated_nodes: Optional[int] = None,
        target_low_priority_nodes: Optional[int] = None,
        enable_auto_scale: Optional[bool] = None,
        auto_scale_formula: Optional[str] = None,
        auto_scale_evaluation_interval: Optional[datetime.timedelta] = None,
        enable_inter_node_communication: Optional[bool] = None,
        network_configuration: Optional["_models.NetworkConfiguration"] = None,
        start_task: Optional["_models.StartTask"] = None,
        certificate_references: Optional[List["_models.CertificateReference"]] = None,
        application_package_references: Optional[List["_models.ApplicationPackageReference"]] = None,
        application_licenses: Optional[List[str]] = None,
        task_slots_per_node: Optional[int] = None,
        task_scheduling_policy: Optional["_models.TaskSchedulingPolicy"] = None,
        user_accounts: Optional[List["_models.UserAccount"]] = None,
        metadata: Optional[List["_models.MetadataItem"]] = None,
        mount_configuration: Optional[List["_models.MountConfiguration"]] = None,
        **kwargs
    ):
        """
        :keyword id: The ID can contain any combination of alphanumeric characters including hyphens
         and underscores, and cannot contain more than 64 characters. The ID is case-preserving and
         case-insensitive (that is, you may not have two IDs within an Account that differ only by
         case).
        :paramtype id: str
        :keyword display_name: The display name need not be unique and can contain any Unicode
         characters up to a maximum length of 1024.
        :paramtype display_name: str
        :keyword vm_size: For information about available sizes of virtual machines in Pools, see
         Choose a VM size for Compute Nodes in an Azure Batch Pool
         (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        :paramtype vm_size: str
        :keyword cloud_service_configuration: This property and virtualMachineConfiguration are
         mutually exclusive and one of the properties must be specified. This property cannot be
         specified if the Batch Account was created with its poolAllocationMode property set to
         'UserSubscription'.
        :paramtype cloud_service_configuration: ~azure-batch.models.CloudServiceConfiguration
        :keyword virtual_machine_configuration: This property and cloudServiceConfiguration are
         mutually exclusive and one of the properties must be specified.
        :paramtype virtual_machine_configuration: ~azure-batch.models.VirtualMachineConfiguration
        :keyword resize_timeout: This is the timeout for the most recent resize operation. (The initial
         sizing when the Pool is created counts as a resize.) The default value is 15 minutes.
        :paramtype resize_timeout: ~datetime.timedelta
        :keyword target_dedicated_nodes: The desired number of dedicated Compute Nodes in the Pool.
        :paramtype target_dedicated_nodes: int
        :keyword target_low_priority_nodes: The desired number of Spot/Low-priority Compute Nodes in
         the Pool.
        :paramtype target_low_priority_nodes: int
        :keyword enable_auto_scale: If false, at least one of targetDedicatedNodes and
         targetLowPriorityNodes must be specified. If true, the autoScaleFormula property is required
         and the Pool automatically resizes according to the formula. The default value is false.
        :paramtype enable_auto_scale: bool
        :keyword auto_scale_formula: This property is set only if the Pool automatically scales, i.e.
         enableAutoScale is true.
        :paramtype auto_scale_formula: str
        :keyword auto_scale_evaluation_interval: This property is set only if the Pool automatically
         scales, i.e. enableAutoScale is true.
        :paramtype auto_scale_evaluation_interval: ~datetime.timedelta
        :keyword enable_inter_node_communication: This imposes restrictions on which Compute Nodes can
         be assigned to the Pool. Specifying this value can reduce the chance of the requested number of
         Compute Nodes to be allocated in the Pool.
        :paramtype enable_inter_node_communication: bool
        :keyword network_configuration: The network configuration for a Pool.
        :paramtype network_configuration: ~azure-batch.models.NetworkConfiguration
        :keyword start_task: Batch will retry Tasks when a recovery operation is triggered on a Node.
         Examples of recovery operations include (but are not limited to) when an unhealthy Node is
         rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations
         are independent of and are not counted against the maxTaskRetryCount. Even if the
         maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of
         this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and
         restarted without causing any corruption or duplicate data. The best practice for long running
         Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even
         though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks
         which create breakaway process or install/launch services from the StartTask working directory,
         as this will block Batch from being able to re-run the StartTask.
        :paramtype start_task: ~azure-batch.models.StartTask
        :keyword certificate_references: For Windows Nodes, the Batch service installs the Certificates
         to the specified Certificate store and location. For Linux Compute Nodes, the Certificates are
         stored in a directory inside the Task working directory and an environment variable
         AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates
         with visibility of 'remoteUser', a 'certs' directory is created in the user's home directory
         (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        :paramtype certificate_references: list[~azure-batch.models.CertificateReference]
        :keyword application_package_references: Changes to Package references affect all new Nodes
         joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are
         rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
        :paramtype application_package_references:
         list[~azure-batch.models.ApplicationPackageReference]
        :keyword application_licenses: The list of application licenses must be a subset of available
         Batch service application licenses. If a license is requested which is not supported, Pool
         creation will fail.
        :paramtype application_licenses: list[str]
        :keyword task_slots_per_node: The default value is 1. The maximum value is the smaller of 4
         times the number of cores of the vmSize of the pool or 256.
        :paramtype task_slots_per_node: int
        :keyword task_scheduling_policy: If not specified, the default is spread.
        :paramtype task_scheduling_policy: ~azure-batch.models.TaskSchedulingPolicy
        :keyword user_accounts: The list of user Accounts to be created on each Compute Node in the
         Pool.
        :paramtype user_accounts: list[~azure-batch.models.UserAccount]
        :keyword metadata: A list of name-value pairs associated with the Pool as metadata.
        :paramtype metadata: list[~azure-batch.models.MetadataItem]
        :keyword mount_configuration: This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
        :paramtype mount_configuration: list[~azure-batch.models.MountConfiguration]
        """
        super(BatchPool, self).__init__(**kwargs)
        self.id = id
        self.display_name = display_name
        self.url = None
        self.e_tag = None
        self.last_modified = None
        self.creation_time = None
        self.state = None
        self.state_transition_time = None
        self.allocation_state = None
        self.allocation_state_transition_time = None
        self.vm_size = vm_size
        self.cloud_service_configuration = cloud_service_configuration
        self.virtual_machine_configuration = virtual_machine_configuration
        self.resize_timeout = resize_timeout
        self.resize_errors = None
        self.current_dedicated_nodes = None
        self.current_low_priority_nodes = None
        self.target_dedicated_nodes = target_dedicated_nodes
        self.target_low_priority_nodes = target_low_priority_nodes
        self.enable_auto_scale = enable_auto_scale
        self.auto_scale_formula = auto_scale_formula
        self.auto_scale_evaluation_interval = auto_scale_evaluation_interval
        self.auto_scale_run = None
        self.enable_inter_node_communication = enable_inter_node_communication
        self.network_configuration = network_configuration
        self.start_task = start_task
        self.certificate_references = certificate_references
        self.application_package_references = application_package_references
        self.application_licenses = application_licenses
        self.task_slots_per_node = task_slots_per_node
        self.task_scheduling_policy = task_scheduling_policy
        self.user_accounts = user_accounts
        self.metadata = metadata
        self.stats = None
        self.mount_configuration = mount_configuration
        self.identity = None


class BatchPoolEnableAutoScaleParameters(msrest.serialization.Model):
    """Options for enabling automatic scaling on a Pool.

    :ivar auto_scale_formula: The formula is checked for validity before it is applied to the Pool.
     If the formula is not valid, the Batch service rejects the request with detailed error
     information. For more information about specifying this formula, see Automatically scale
     Compute Nodes in an Azure Batch Pool
     (https://azure.microsoft.com/en-us/documentation/articles/batch-automatic-scaling).
    :vartype auto_scale_formula: str
    :ivar auto_scale_evaluation_interval: The default value is 15 minutes. The minimum and maximum
     value are 5 minutes and 168 hours respectively. If you specify a value less than 5 minutes or
     greater than 168 hours, the Batch service rejects the request with an invalid property value
     error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request). If
     you specify a new interval, then the existing autoscale evaluation schedule will be stopped and
     a new autoscale evaluation schedule will be started, with its starting time being the time when
     this request was issued.
    :vartype auto_scale_evaluation_interval: ~datetime.timedelta
    """

    _attribute_map = {
        'auto_scale_formula': {'key': 'autoScaleFormula', 'type': 'str'},
        'auto_scale_evaluation_interval': {'key': 'autoScaleEvaluationInterval', 'type': 'duration'},
    }

    def __init__(
        self,
        *,
        auto_scale_formula: Optional[str] = None,
        auto_scale_evaluation_interval: Optional[datetime.timedelta] = None,
        **kwargs
    ):
        """
        :keyword auto_scale_formula: The formula is checked for validity before it is applied to the
         Pool. If the formula is not valid, the Batch service rejects the request with detailed error
         information. For more information about specifying this formula, see Automatically scale
         Compute Nodes in an Azure Batch Pool
         (https://azure.microsoft.com/en-us/documentation/articles/batch-automatic-scaling).
        :paramtype auto_scale_formula: str
        :keyword auto_scale_evaluation_interval: The default value is 15 minutes. The minimum and
         maximum value are 5 minutes and 168 hours respectively. If you specify a value less than 5
         minutes or greater than 168 hours, the Batch service rejects the request with an invalid
         property value error; if you are calling the REST API directly, the HTTP status code is 400
         (Bad Request). If you specify a new interval, then the existing autoscale evaluation schedule
         will be stopped and a new autoscale evaluation schedule will be started, with its starting time
         being the time when this request was issued.
        :paramtype auto_scale_evaluation_interval: ~datetime.timedelta
        """
        super(BatchPoolEnableAutoScaleParameters, self).__init__(**kwargs)
        self.auto_scale_formula = auto_scale_formula
        self.auto_scale_evaluation_interval = auto_scale_evaluation_interval


class BatchPoolEvaluateAutoScaleParameters(msrest.serialization.Model):
    """Options for evaluating an automatic scaling formula on a Pool.

    All required parameters must be populated in order to send to Azure.

    :ivar auto_scale_formula: Required. The formula is validated and its results calculated, but it
     is not applied to the Pool. To apply the formula to the Pool, 'Enable automatic scaling on a
     Pool'. For more information about specifying this formula, see Automatically scale Compute
     Nodes in an Azure Batch Pool
     (https://azure.microsoft.com/en-us/documentation/articles/batch-automatic-scaling).
    :vartype auto_scale_formula: str
    """

    _validation = {
        'auto_scale_formula': {'required': True},
    }

    _attribute_map = {
        'auto_scale_formula': {'key': 'autoScaleFormula', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        auto_scale_formula: str,
        **kwargs
    ):
        """
        :keyword auto_scale_formula: Required. The formula is validated and its results calculated, but
         it is not applied to the Pool. To apply the formula to the Pool, 'Enable automatic scaling on a
         Pool'. For more information about specifying this formula, see Automatically scale Compute
         Nodes in an Azure Batch Pool
         (https://azure.microsoft.com/en-us/documentation/articles/batch-automatic-scaling).
        :paramtype auto_scale_formula: str
        """
        super(BatchPoolEvaluateAutoScaleParameters, self).__init__(**kwargs)
        self.auto_scale_formula = auto_scale_formula


class BatchPoolIdentity(msrest.serialization.Model):
    """The identity of the Batch pool, if configured.

    All required parameters must be populated in order to send to Azure.

    :ivar type: Required. The list of user identities associated with the Batch pool. The user
     identity dictionary key references will be ARM resource ids in the form:
     '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/{identityName}'.
     Known values are: "UserAssigned", "None".
    :vartype type: str or ~azure-batch.models.PoolIdentityType
    :ivar user_assigned_identities: The user identity dictionary key references will be ARM
     resource ids in the form:
     '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/{identityName}'.
    :vartype user_assigned_identities: list[~azure-batch.models.UserAssignedIdentity]
    """

    _validation = {
        'type': {'required': True},
    }

    _attribute_map = {
        'type': {'key': 'type', 'type': 'str'},
        'user_assigned_identities': {'key': 'userAssignedIdentities', 'type': '[UserAssignedIdentity]'},
    }

    def __init__(
        self,
        *,
        type: Union[str, "_models.PoolIdentityType"],
        user_assigned_identities: Optional[List["_models.UserAssignedIdentity"]] = None,
        **kwargs
    ):
        """
        :keyword type: Required. The list of user identities associated with the Batch pool. The user
         identity dictionary key references will be ARM resource ids in the form:
         '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/{identityName}'.
         Known values are: "UserAssigned", "None".
        :paramtype type: str or ~azure-batch.models.PoolIdentityType
        :keyword user_assigned_identities: The user identity dictionary key references will be ARM
         resource ids in the form:
         '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/{identityName}'.
        :paramtype user_assigned_identities: list[~azure-batch.models.UserAssignedIdentity]
        """
        super(BatchPoolIdentity, self).__init__(**kwargs)
        self.type = type
        self.user_assigned_identities = user_assigned_identities


class BatchPoolListResult(msrest.serialization.Model):
    """The result of listing the Pools in an Account.

    :ivar value: The list of Pools.
    :vartype value: list[~azure-batch.models.BatchPool]
    :ivar odata_next_link: The URL to get the next set of results.
    :vartype odata_next_link: str
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[BatchPool]'},
        'odata_next_link': {'key': 'odata\\.nextLink', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["_models.BatchPool"]] = None,
        odata_next_link: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword value: The list of Pools.
        :paramtype value: list[~azure-batch.models.BatchPool]
        :keyword odata_next_link: The URL to get the next set of results.
        :paramtype odata_next_link: str
        """
        super(BatchPoolListResult, self).__init__(**kwargs)
        self.value = value
        self.odata_next_link = odata_next_link


class BatchPoolResizeParameters(msrest.serialization.Model):
    """Options for changing the size of a Pool.

    :ivar target_dedicated_nodes: The desired number of dedicated Compute Nodes in the Pool.
    :vartype target_dedicated_nodes: int
    :ivar target_low_priority_nodes: The desired number of Spot/Low-priority Compute Nodes in the
     Pool.
    :vartype target_low_priority_nodes: int
    :ivar resize_timeout: The default value is 15 minutes. The minimum value is 5 minutes. If you
     specify a value less than 5 minutes, the Batch service returns an error; if you are calling the
     REST API directly, the HTTP status code is 400 (Bad Request).
    :vartype resize_timeout: ~datetime.timedelta
    :ivar node_deallocation_option: The default value is requeue. Known values are: "requeue",
     "terminate", "taskcompletion", "retaineddata".
    :vartype node_deallocation_option: str or ~azure-batch.models.ComputeNodeDeallocationOption
    """

    _attribute_map = {
        'target_dedicated_nodes': {'key': 'targetDedicatedNodes', 'type': 'int'},
        'target_low_priority_nodes': {'key': 'targetLowPriorityNodes', 'type': 'int'},
        'resize_timeout': {'key': 'resizeTimeout', 'type': 'duration'},
        'node_deallocation_option': {'key': 'nodeDeallocationOption', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        target_dedicated_nodes: Optional[int] = None,
        target_low_priority_nodes: Optional[int] = None,
        resize_timeout: Optional[datetime.timedelta] = None,
        node_deallocation_option: Optional[Union[str, "_models.ComputeNodeDeallocationOption"]] = None,
        **kwargs
    ):
        """
        :keyword target_dedicated_nodes: The desired number of dedicated Compute Nodes in the Pool.
        :paramtype target_dedicated_nodes: int
        :keyword target_low_priority_nodes: The desired number of Spot/Low-priority Compute Nodes in
         the Pool.
        :paramtype target_low_priority_nodes: int
        :keyword resize_timeout: The default value is 15 minutes. The minimum value is 5 minutes. If
         you specify a value less than 5 minutes, the Batch service returns an error; if you are calling
         the REST API directly, the HTTP status code is 400 (Bad Request).
        :paramtype resize_timeout: ~datetime.timedelta
        :keyword node_deallocation_option: The default value is requeue. Known values are: "requeue",
         "terminate", "taskcompletion", "retaineddata".
        :paramtype node_deallocation_option: str or ~azure-batch.models.ComputeNodeDeallocationOption
        """
        super(BatchPoolResizeParameters, self).__init__(**kwargs)
        self.target_dedicated_nodes = target_dedicated_nodes
        self.target_low_priority_nodes = target_low_priority_nodes
        self.resize_timeout = resize_timeout
        self.node_deallocation_option = node_deallocation_option


class BatchPoolUpdate(msrest.serialization.Model):
    """The set of changes to be made to a Pool.

    :ivar start_task: If this element is present, it overwrites any existing StartTask. If omitted,
     any existing StartTask is left unchanged.
    :vartype start_task: ~azure-batch.models.StartTask
    :ivar certificate_references: If this element is present, it replaces any existing Certificate
     references configured on the Pool. If omitted, any existing Certificate references are left
     unchanged. For Windows Nodes, the Batch service installs the Certificates to the specified
     Certificate store and location. For Linux Compute Nodes, the Certificates are stored in a
     directory inside the Task working directory and an environment variable
     AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates
     with visibility of 'remoteUser', a 'certs' directory is created in the user's home directory
     (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
    :vartype certificate_references: list[~azure-batch.models.CertificateReference]
    :ivar application_package_references: Changes to Package references affect all new Nodes
     joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are
     rebooted or reimaged. If this element is present, it replaces any existing Package references.
     If you specify an empty collection, then all Package references are removed from the Pool. If
     omitted, any existing Package references are left unchanged.
    :vartype application_package_references: list[~azure-batch.models.ApplicationPackageReference]
    :ivar metadata: If this element is present, it replaces any existing metadata configured on the
     Pool. If you specify an empty collection, any metadata is removed from the Pool. If omitted,
     any existing metadata is left unchanged.
    :vartype metadata: list[~azure-batch.models.MetadataItem]
    """

    _attribute_map = {
        'start_task': {'key': 'startTask', 'type': 'StartTask'},
        'certificate_references': {'key': 'certificateReferences', 'type': '[CertificateReference]'},
        'application_package_references': {'key': 'applicationPackageReferences', 'type': '[ApplicationPackageReference]'},
        'metadata': {'key': 'metadata', 'type': '[MetadataItem]'},
    }

    def __init__(
        self,
        *,
        start_task: Optional["_models.StartTask"] = None,
        certificate_references: Optional[List["_models.CertificateReference"]] = None,
        application_package_references: Optional[List["_models.ApplicationPackageReference"]] = None,
        metadata: Optional[List["_models.MetadataItem"]] = None,
        **kwargs
    ):
        """
        :keyword start_task: If this element is present, it overwrites any existing StartTask. If
         omitted, any existing StartTask is left unchanged.
        :paramtype start_task: ~azure-batch.models.StartTask
        :keyword certificate_references: If this element is present, it replaces any existing
         Certificate references configured on the Pool. If omitted, any existing Certificate references
         are left unchanged. For Windows Nodes, the Batch service installs the Certificates to the
         specified Certificate store and location. For Linux Compute Nodes, the Certificates are stored
         in a directory inside the Task working directory and an environment variable
         AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates
         with visibility of 'remoteUser', a 'certs' directory is created in the user's home directory
         (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        :paramtype certificate_references: list[~azure-batch.models.CertificateReference]
        :keyword application_package_references: Changes to Package references affect all new Nodes
         joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are
         rebooted or reimaged. If this element is present, it replaces any existing Package references.
         If you specify an empty collection, then all Package references are removed from the Pool. If
         omitted, any existing Package references are left unchanged.
        :paramtype application_package_references:
         list[~azure-batch.models.ApplicationPackageReference]
        :keyword metadata: If this element is present, it replaces any existing metadata configured on
         the Pool. If you specify an empty collection, any metadata is removed from the Pool. If
         omitted, any existing metadata is left unchanged.
        :paramtype metadata: list[~azure-batch.models.MetadataItem]
        """
        super(BatchPoolUpdate, self).__init__(**kwargs)
        self.start_task = start_task
        self.certificate_references = certificate_references
        self.application_package_references = application_package_references
        self.metadata = metadata


class BatchTask(msrest.serialization.Model):
    """Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing.

    Variables are only populated by the server, and will be ignored when sending a request.

    :ivar id: The ID can contain any combination of alphanumeric characters including hyphens and
     underscores, and cannot contain more than 64 characters.
    :vartype id: str
    :ivar display_name: The display name need not be unique and can contain any Unicode characters
     up to a maximum length of 1024.
    :vartype display_name: str
    :ivar url: The URL of the Task.
    :vartype url: str
    :ivar e_tag: This is an opaque string. You can use it to detect whether the Task has changed
     between requests. In particular, you can be pass the ETag when updating a Task to specify that
     your changes should take effect only if nobody else has modified the Task in the meantime.
    :vartype e_tag: str
    :ivar last_modified: The last modified time of the Task.
    :vartype last_modified: ~datetime.datetime
    :ivar creation_time: The creation time of the Task.
    :vartype creation_time: ~datetime.datetime
    :ivar exit_conditions: How the Batch service should respond when the Task completes.
    :vartype exit_conditions: ~azure-batch.models.ExitConditions
    :ivar state: The state of the Task. Known values are: "active", "preparing", "running",
     "completed".
    :vartype state: str or ~azure-batch.models.TaskState
    :ivar state_transition_time: The time at which the Task entered its current state.
    :vartype state_transition_time: ~datetime.datetime
    :ivar previous_state: This property is not set if the Task is in its initial Active state.
     Known values are: "active", "preparing", "running", "completed".
    :vartype previous_state: str or ~azure-batch.models.TaskState
    :ivar previous_state_transition_time: This property is not set if the Task is in its initial
     Active state.
    :vartype previous_state_transition_time: ~datetime.datetime
    :ivar command_line: For multi-instance Tasks, the command line is executed as the primary Task,
     after the primary Task and all subtasks have finished executing the coordination command line.
     The command line does not run under a shell, and therefore cannot take advantage of shell
     features such as environment variable expansion. If you want to take advantage of such
     features, you should invoke the shell in the command line, for example using "cmd /c MyCommand"
     in Windows or "/bin/sh -c MyCommand" in Linux. If the command line refers to file paths, it
     should use a relative path (relative to the Task working directory), or use the Batch provided
     environment variable
     (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
    :vartype command_line: str
    :ivar container_settings: If the Pool that will run this Task has containerConfiguration set,
     this must be set as well. If the Pool that will run this Task doesn't have
     containerConfiguration set, this must not be set. When this is specified, all directories
     recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node)
     are mapped into the container, all Task environment variables are mapped into the container,
     and the Task command line is executed in the container. Files produced in the container outside
     of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs
     will not be able to access those files.
    :vartype container_settings: ~azure-batch.models.TaskContainerSettings
    :ivar resource_files: For multi-instance Tasks, the resource files will only be downloaded to
     the Compute Node on which the primary Task is executed. There is a maximum size for the list of
     resource files.  When the max size is exceeded, the request will fail and the response error
     code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
     reduced in size. This can be achieved using .zip files, Application Packages, or Docker
     Containers.
    :vartype resource_files: list[~azure-batch.models.ResourceFile]
    :ivar output_files: For multi-instance Tasks, the files will only be uploaded from the Compute
     Node on which the primary Task is executed.
    :vartype output_files: list[~azure-batch.models.OutputFile]
    :ivar environment_settings: A list of environment variable settings for the Task.
    :vartype environment_settings: list[~azure-batch.models.EnvironmentSetting]
    :ivar affinity_info: A locality hint that can be used by the Batch service to select a Compute
     Node on which to start a Task.
    :vartype affinity_info: ~azure-batch.models.AffinityInformation
    :ivar constraints: Execution constraints to apply to a Task.
    :vartype constraints: ~azure-batch.models.TaskConstraints
    :ivar required_slots: The default is 1. A Task can only be scheduled to run on a compute node
     if the node has enough free scheduling slots available. For multi-instance Tasks, this must be
     1.
    :vartype required_slots: int
    :ivar user_identity: If omitted, the Task runs as a non-administrative user unique to the Task.
    :vartype user_identity: ~azure-batch.models.UserIdentity
    :ivar execution_info: Information about the execution of a Task.
    :vartype execution_info: ~azure-batch.models.TaskExecutionInformation
    :ivar node_info: Information about the Compute Node on which a Task ran.
    :vartype node_info: ~azure-batch.models.ComputeNodeInformation
    :ivar multi_instance_settings: Multi-instance Tasks are commonly used to support MPI Tasks. In
     the MPI case, if any of the subtasks fail (for example due to exiting with a non-zero exit
     code) the entire multi-instance Task fails. The multi-instance Task is then terminated and
     retried, up to its retry limit.
    :vartype multi_instance_settings: ~azure-batch.models.MultiInstanceSettings
    :ivar stats: Resource usage statistics for a Task.
    :vartype stats: ~azure-batch.models.TaskStatistics
    :ivar depends_on: This Task will not be scheduled until all Tasks that it depends on have
     completed successfully. If any of those Tasks fail and exhaust their retry counts, this Task
     will never be scheduled.
    :vartype depends_on: ~azure-batch.models.TaskDependencies
    :ivar application_package_references: Application packages are downloaded and deployed to a
     shared directory, not the Task working directory. Therefore, if a referenced package is already
     on the Node, and is up to date, then it is not re-downloaded; the existing copy on the Compute
     Node is used. If a referenced Package cannot be installed, for example because the package has
     been deleted or because download failed, the Task fails.
    :vartype application_package_references: list[~azure-batch.models.ApplicationPackageReference]
    :ivar authentication_token_settings: If this property is set, the Batch service provides the
     Task with an authentication token which can be used to authenticate Batch service operations
     without requiring an Account access key. The token is provided via the
     AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the Task can carry out
     using the token depend on the settings. For example, a Task can request Job permissions in
     order to add other Tasks to the Job, or check the status of the Job or of other Tasks under the
     Job.
    :vartype authentication_token_settings: ~azure-batch.models.AuthenticationTokenSettings
    """

    _validation = {
        'url': {'readonly': True},
        'e_tag': {'readonly': True},
        'last_modified': {'readonly': True},
        'creation_time': {'readonly': True},
        'state': {'readonly': True},
        'state_transition_time': {'readonly': True},
        'previous_state': {'readonly': True},
        'previous_state_transition_time': {'readonly': True},
        'execution_info': {'readonly': True},
        'node_info': {'readonly': True},
        'stats': {'readonly': True},
    }

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'display_name': {'key': 'displayName', 'type': 'str'},
        'url': {'key': 'url', 'type': 'str'},
        'e_tag': {'key': 'eTag', 'type': 'str'},
        'last_modified': {'key': 'lastModified', 'type': 'iso-8601'},
        'creation_time': {'key': 'creationTime', 'type': 'iso-8601'},
        'exit_conditions': {'key': 'exitConditions', 'type': 'ExitConditions'},
        'state': {'key': 'state', 'type': 'str'},
        'state_transition_time': {'key': 'stateTransitionTime', 'type': 'iso-8601'},
        'previous_state': {'key': 'previousState', 'type': 'str'},
        'previous_state_transition_time': {'key': 'previousStateTransitionTime', 'type': 'iso-8601'},
        'command_line': {'key': 'commandLine', 'type': 'str'},
        'container_settings': {'key': 'containerSettings', 'type': 'TaskContainerSettings'},
        'resource_files': {'key': 'resourceFiles', 'type': '[ResourceFile]'},
        'output_files': {'key': 'outputFiles', 'type': '[OutputFile]'},
        'environment_settings': {'key': 'environmentSettings', 'type': '[EnvironmentSetting]'},
        'affinity_info': {'key': 'affinityInfo', 'type': 'AffinityInformation'},
        'constraints': {'key': 'constraints', 'type': 'TaskConstraints'},
        'required_slots': {'key': 'requiredSlots', 'type': 'int'},
        'user_identity': {'key': 'userIdentity', 'type': 'UserIdentity'},
        'execution_info': {'key': 'executionInfo', 'type': 'TaskExecutionInformation'},
        'node_info': {'key': 'nodeInfo', 'type': 'ComputeNodeInformation'},
        'multi_instance_settings': {'key': 'multiInstanceSettings', 'type': 'MultiInstanceSettings'},
        'stats': {'key': 'stats', 'type': 'TaskStatistics'},
        'depends_on': {'key': 'dependsOn', 'type': 'TaskDependencies'},
        'application_package_references': {'key': 'applicationPackageReferences', 'type': '[ApplicationPackageReference]'},
        'authentication_token_settings': {'key': 'authenticationTokenSettings', 'type': 'AuthenticationTokenSettings'},
    }

    def __init__(
        self,
        *,
        id: Optional[str] = None,
        display_name: Optional[str] = None,
        exit_conditions: Optional["_models.ExitConditions"] = None,
        command_line: Optional[str] = None,
        container_settings: Optional["_models.TaskContainerSettings"] = None,
        resource_files: Optional[List["_models.ResourceFile"]] = None,
        output_files: Optional[List["_models.OutputFile"]] = None,
        environment_settings: Optional[List["_models.EnvironmentSetting"]] = None,
        affinity_info: Optional["_models.AffinityInformation"] = None,
        constraints: Optional["_models.TaskConstraints"] = None,
        required_slots: Optional[int] = None,
        user_identity: Optional["_models.UserIdentity"] = None,
        multi_instance_settings: Optional["_models.MultiInstanceSettings"] = None,
        depends_on: Optional["_models.TaskDependencies"] = None,
        application_package_references: Optional[List["_models.ApplicationPackageReference"]] = None,
        authentication_token_settings: Optional["_models.AuthenticationTokenSettings"] = None,
        **kwargs
    ):
        """
        :keyword id: The ID can contain any combination of alphanumeric characters including hyphens
         and underscores, and cannot contain more than 64 characters.
        :paramtype id: str
        :keyword display_name: The display name need not be unique and can contain any Unicode
         characters up to a maximum length of 1024.
        :paramtype display_name: str
        :keyword exit_conditions: How the Batch service should respond when the Task completes.
        :paramtype exit_conditions: ~azure-batch.models.ExitConditions
        :keyword command_line: For multi-instance Tasks, the command line is executed as the primary
         Task, after the primary Task and all subtasks have finished executing the coordination command
         line. The command line does not run under a shell, and therefore cannot take advantage of shell
         features such as environment variable expansion. If you want to take advantage of such
         features, you should invoke the shell in the command line, for example using "cmd /c MyCommand"
         in Windows or "/bin/sh -c MyCommand" in Linux. If the command line refers to file paths, it
         should use a relative path (relative to the Task working directory), or use the Batch provided
         environment variable
         (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        :paramtype command_line: str
        :keyword container_settings: If the Pool that will run this Task has containerConfiguration
         set, this must be set as well. If the Pool that will run this Task doesn't have
         containerConfiguration set, this must not be set. When this is specified, all directories
         recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node)
         are mapped into the container, all Task environment variables are mapped into the container,
         and the Task command line is executed in the container. Files produced in the container outside
         of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs
         will not be able to access those files.
        :paramtype container_settings: ~azure-batch.models.TaskContainerSettings
        :keyword resource_files: For multi-instance Tasks, the resource files will only be downloaded
         to the Compute Node on which the primary Task is executed. There is a maximum size for the list
         of resource files.  When the max size is exceeded, the request will fail and the response error
         code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
         reduced in size. This can be achieved using .zip files, Application Packages, or Docker
         Containers.
        :paramtype resource_files: list[~azure-batch.models.ResourceFile]
        :keyword output_files: For multi-instance Tasks, the files will only be uploaded from the
         Compute Node on which the primary Task is executed.
        :paramtype output_files: list[~azure-batch.models.OutputFile]
        :keyword environment_settings: A list of environment variable settings for the Task.
        :paramtype environment_settings: list[~azure-batch.models.EnvironmentSetting]
        :keyword affinity_info: A locality hint that can be used by the Batch service to select a
         Compute Node on which to start a Task.
        :paramtype affinity_info: ~azure-batch.models.AffinityInformation
        :keyword constraints: Execution constraints to apply to a Task.
        :paramtype constraints: ~azure-batch.models.TaskConstraints
        :keyword required_slots: The default is 1. A Task can only be scheduled to run on a compute
         node if the node has enough free scheduling slots available. For multi-instance Tasks, this
         must be 1.
        :paramtype required_slots: int
        :keyword user_identity: If omitted, the Task runs as a non-administrative user unique to the
         Task.
        :paramtype user_identity: ~azure-batch.models.UserIdentity
        :keyword multi_instance_settings: Multi-instance Tasks are commonly used to support MPI Tasks.
         In the MPI case, if any of the subtasks fail (for example due to exiting with a non-zero exit
         code) the entire multi-instance Task fails. The multi-instance Task is then terminated and
         retried, up to its retry limit.
        :paramtype multi_instance_settings: ~azure-batch.models.MultiInstanceSettings
        :keyword depends_on: This Task will not be scheduled until all Tasks that it depends on have
         completed successfully. If any of those Tasks fail and exhaust their retry counts, this Task
         will never be scheduled.
        :paramtype depends_on: ~azure-batch.models.TaskDependencies
        :keyword application_package_references: Application packages are downloaded and deployed to a
         shared directory, not the Task working directory. Therefore, if a referenced package is already
         on the Node, and is up to date, then it is not re-downloaded; the existing copy on the Compute
         Node is used. If a referenced Package cannot be installed, for example because the package has
         been deleted or because download failed, the Task fails.
        :paramtype application_package_references:
         list[~azure-batch.models.ApplicationPackageReference]
        :keyword authentication_token_settings: If this property is set, the Batch service provides the
         Task with an authentication token which can be used to authenticate Batch service operations
         without requiring an Account access key. The token is provided via the
         AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the Task can carry out
         using the token depend on the settings. For example, a Task can request Job permissions in
         order to add other Tasks to the Job, or check the status of the Job or of other Tasks under the
         Job.
        :paramtype authentication_token_settings: ~azure-batch.models.AuthenticationTokenSettings
        """
        super(BatchTask, self).__init__(**kwargs)
        self.id = id
        self.display_name = display_name
        self.url = None
        self.e_tag = None
        self.last_modified = None
        self.creation_time = None
        self.exit_conditions = exit_conditions
        self.state = None
        self.state_transition_time = None
        self.previous_state = None
        self.previous_state_transition_time = None
        self.command_line = command_line
        self.container_settings = container_settings
        self.resource_files = resource_files
        self.output_files = output_files
        self.environment_settings = environment_settings
        self.affinity_info = affinity_info
        self.constraints = constraints
        self.required_slots = required_slots
        self.user_identity = user_identity
        self.execution_info = None
        self.node_info = None
        self.multi_instance_settings = multi_instance_settings
        self.stats = None
        self.depends_on = depends_on
        self.application_package_references = application_package_references
        self.authentication_token_settings = authentication_token_settings


class BatchTaskCollection(msrest.serialization.Model):
    """A collection of Azure Batch Tasks to add.

    All required parameters must be populated in order to send to Azure.

    :ivar value: Required. The total serialized size of this collection must be less than 1MB. If
     it is greater than 1MB (for example if each Task has 100's of resource files or environment
     variables), the request will fail with code 'RequestBodyTooLarge' and should be retried again
     with fewer Tasks.
    :vartype value: list[~azure-batch.models.BatchTask]
    """

    _validation = {
        'value': {'required': True, 'max_items': 100, 'min_items': 0},
    }

    _attribute_map = {
        'value': {'key': 'value', 'type': '[BatchTask]'},
    }

    def __init__(
        self,
        *,
        value: List["_models.BatchTask"],
        **kwargs
    ):
        """
        :keyword value: Required. The total serialized size of this collection must be less than 1MB.
         If it is greater than 1MB (for example if each Task has 100's of resource files or environment
         variables), the request will fail with code 'RequestBodyTooLarge' and should be retried again
         with fewer Tasks.
        :paramtype value: list[~azure-batch.models.BatchTask]
        """
        super(BatchTaskCollection, self).__init__(**kwargs)
        self.value = value


class BatchTaskListResult(msrest.serialization.Model):
    """The result of listing the Tasks in a Job.

    :ivar value: The list of Tasks.
    :vartype value: list[~azure-batch.models.BatchTask]
    :ivar odata_next_link: The URL to get the next set of results.
    :vartype odata_next_link: str
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[BatchTask]'},
        'odata_next_link': {'key': 'odata\\.nextLink', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["_models.BatchTask"]] = None,
        odata_next_link: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword value: The list of Tasks.
        :paramtype value: list[~azure-batch.models.BatchTask]
        :keyword odata_next_link: The URL to get the next set of results.
        :paramtype odata_next_link: str
        """
        super(BatchTaskListResult, self).__init__(**kwargs)
        self.value = value
        self.odata_next_link = odata_next_link


class BatchTaskListSubtasksResult(msrest.serialization.Model):
    """The result of listing the subtasks of a Task.

    :ivar value: The list of subtasks.
    :vartype value: list[~azure-batch.models.SubtaskInformation]
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[SubtaskInformation]'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["_models.SubtaskInformation"]] = None,
        **kwargs
    ):
        """
        :keyword value: The list of subtasks.
        :paramtype value: list[~azure-batch.models.SubtaskInformation]
        """
        super(BatchTaskListSubtasksResult, self).__init__(**kwargs)
        self.value = value


class Certificate(msrest.serialization.Model):
    """A Certificate that can be installed on Compute Nodes and can be used to authenticate operations on the machine.

    Variables are only populated by the server, and will be ignored when sending a request.

    :ivar thumbprint: The X.509 thumbprint of the Certificate. This is a sequence of up to 40 hex
     digits.
    :vartype thumbprint: str
    :ivar thumbprint_algorithm: The algorithm used to derive the thumbprint.
    :vartype thumbprint_algorithm: str
    :ivar url: The URL of the Certificate.
    :vartype url: str
    :ivar state: The state of the Certificate. Known values are: "active", "deleting",
     "deletefailed".
    :vartype state: str or ~azure-batch.models.CertificateState
    :ivar state_transition_time: The time at which the Certificate entered its current state.
    :vartype state_transition_time: ~datetime.datetime
    :ivar previous_state: This property is not set if the Certificate is in its initial active
     state. Known values are: "active", "deleting", "deletefailed".
    :vartype previous_state: str or ~azure-batch.models.CertificateState
    :ivar previous_state_transition_time: This property is not set if the Certificate is in its
     initial Active state.
    :vartype previous_state_transition_time: ~datetime.datetime
    :ivar public_data: The public part of the Certificate as a base-64 encoded .cer file.
    :vartype public_data: str
    :ivar delete_certificate_error: This property is set only if the Certificate is in the
     DeleteFailed state.
    :vartype delete_certificate_error: ~azure-batch.models.DeleteCertificateError
    :ivar data: The base64-encoded contents of the Certificate. The maximum size is 10KB.
    :vartype data: str
    :ivar certificate_format: The format of the Certificate data. Known values are: "pfx", "cer".
    :vartype certificate_format: str or ~azure-batch.models.CertificateFormat
    :ivar password: This must be omitted if the Certificate format is cer.
    :vartype password: str
    """

    _validation = {
        'url': {'readonly': True},
        'state': {'readonly': True},
        'state_transition_time': {'readonly': True},
        'previous_state': {'readonly': True},
        'previous_state_transition_time': {'readonly': True},
        'public_data': {'readonly': True},
        'delete_certificate_error': {'readonly': True},
    }

    _attribute_map = {
        'thumbprint': {'key': 'thumbprint', 'type': 'str'},
        'thumbprint_algorithm': {'key': 'thumbprintAlgorithm', 'type': 'str'},
        'url': {'key': 'url', 'type': 'str'},
        'state': {'key': 'state', 'type': 'str'},
        'state_transition_time': {'key': 'stateTransitionTime', 'type': 'iso-8601'},
        'previous_state': {'key': 'previousState', 'type': 'str'},
        'previous_state_transition_time': {'key': 'previousStateTransitionTime', 'type': 'iso-8601'},
        'public_data': {'key': 'publicData', 'type': 'str'},
        'delete_certificate_error': {'key': 'deleteCertificateError', 'type': 'DeleteCertificateError'},
        'data': {'key': 'data', 'type': 'str'},
        'certificate_format': {'key': 'certificateFormat', 'type': 'str'},
        'password': {'key': 'password', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        thumbprint: Optional[str] = None,
        thumbprint_algorithm: Optional[str] = None,
        data: Optional[str] = None,
        certificate_format: Optional[Union[str, "_models.CertificateFormat"]] = None,
        password: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword thumbprint: The X.509 thumbprint of the Certificate. This is a sequence of up to 40
         hex digits.
        :paramtype thumbprint: str
        :keyword thumbprint_algorithm: The algorithm used to derive the thumbprint.
        :paramtype thumbprint_algorithm: str
        :keyword data: The base64-encoded contents of the Certificate. The maximum size is 10KB.
        :paramtype data: str
        :keyword certificate_format: The format of the Certificate data. Known values are: "pfx",
         "cer".
        :paramtype certificate_format: str or ~azure-batch.models.CertificateFormat
        :keyword password: This must be omitted if the Certificate format is cer.
        :paramtype password: str
        """
        super(Certificate, self).__init__(**kwargs)
        self.thumbprint = thumbprint
        self.thumbprint_algorithm = thumbprint_algorithm
        self.url = None
        self.state = None
        self.state_transition_time = None
        self.previous_state = None
        self.previous_state_transition_time = None
        self.public_data = None
        self.delete_certificate_error = None
        self.data = data
        self.certificate_format = certificate_format
        self.password = password


class CertificateListResult(msrest.serialization.Model):
    """The result of listing the Certificates in the Account.

    :ivar value: The list of Certificates.
    :vartype value: list[~azure-batch.models.Certificate]
    :ivar odata_next_link: The URL to get the next set of results.
    :vartype odata_next_link: str
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[Certificate]'},
        'odata_next_link': {'key': 'odata\\.nextLink', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["_models.Certificate"]] = None,
        odata_next_link: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword value: The list of Certificates.
        :paramtype value: list[~azure-batch.models.Certificate]
        :keyword odata_next_link: The URL to get the next set of results.
        :paramtype odata_next_link: str
        """
        super(CertificateListResult, self).__init__(**kwargs)
        self.value = value
        self.odata_next_link = odata_next_link


class CertificateReference(msrest.serialization.Model):
    """A reference to a Certificate to be installed on Compute Nodes in a Pool.

    All required parameters must be populated in order to send to Azure.

    :ivar thumbprint: Required. The thumbprint of the Certificate.
    :vartype thumbprint: str
    :ivar thumbprint_algorithm: Required. The algorithm with which the thumbprint is associated.
     This must be sha1.
    :vartype thumbprint_algorithm: str
    :ivar store_location: The default value is currentuser. This property is applicable only for
     Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration,
     or with virtualMachineConfiguration using a Windows Image reference). For Linux Compute Nodes,
     the Certificates are stored in a directory inside the Task working directory and an environment
     variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For
     Certificates with visibility of 'remoteUser', a 'certs' directory is created in the user's home
     directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory. Known
     values are: "currentuser", "localmachine".
    :vartype store_location: str or ~azure-batch.models.CertificateStoreLocation
    :ivar store_name: This property is applicable only for Pools configured with Windows Compute
     Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration
     using a Windows Image reference). Common store names include: My, Root, CA, Trust, Disallowed,
     TrustedPeople, TrustedPublisher, AuthRoot, AddressBook, but any custom store name can also be
     used. The default value is My.
    :vartype store_name: str
    :ivar visibility: You can specify more than one visibility in this collection. The default is
     all Accounts.
    :vartype visibility: list[str or ~azure-batch.models.CertificateVisibility]
    """

    _validation = {
        'thumbprint': {'required': True},
        'thumbprint_algorithm': {'required': True},
    }

    _attribute_map = {
        'thumbprint': {'key': 'thumbprint', 'type': 'str'},
        'thumbprint_algorithm': {'key': 'thumbprintAlgorithm', 'type': 'str'},
        'store_location': {'key': 'storeLocation', 'type': 'str'},
        'store_name': {'key': 'storeName', 'type': 'str'},
        'visibility': {'key': 'visibility', 'type': '[str]'},
    }

    def __init__(
        self,
        *,
        thumbprint: str,
        thumbprint_algorithm: str,
        store_location: Optional[Union[str, "_models.CertificateStoreLocation"]] = None,
        store_name: Optional[str] = None,
        visibility: Optional[List[Union[str, "_models.CertificateVisibility"]]] = None,
        **kwargs
    ):
        """
        :keyword thumbprint: Required. The thumbprint of the Certificate.
        :paramtype thumbprint: str
        :keyword thumbprint_algorithm: Required. The algorithm with which the thumbprint is associated.
         This must be sha1.
        :paramtype thumbprint_algorithm: str
        :keyword store_location: The default value is currentuser. This property is applicable only for
         Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration,
         or with virtualMachineConfiguration using a Windows Image reference). For Linux Compute Nodes,
         the Certificates are stored in a directory inside the Task working directory and an environment
         variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For
         Certificates with visibility of 'remoteUser', a 'certs' directory is created in the user's home
         directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory. Known
         values are: "currentuser", "localmachine".
        :paramtype store_location: str or ~azure-batch.models.CertificateStoreLocation
        :keyword store_name: This property is applicable only for Pools configured with Windows Compute
         Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration
         using a Windows Image reference). Common store names include: My, Root, CA, Trust, Disallowed,
         TrustedPeople, TrustedPublisher, AuthRoot, AddressBook, but any custom store name can also be
         used. The default value is My.
        :paramtype store_name: str
        :keyword visibility: You can specify more than one visibility in this collection. The default
         is all Accounts.
        :paramtype visibility: list[str or ~azure-batch.models.CertificateVisibility]
        """
        super(CertificateReference, self).__init__(**kwargs)
        self.thumbprint = thumbprint
        self.thumbprint_algorithm = thumbprint_algorithm
        self.store_location = store_location
        self.store_name = store_name
        self.visibility = visibility


class CIFSMountConfiguration(msrest.serialization.Model):
    """Information used to connect to a CIFS file system.

    All required parameters must be populated in order to send to Azure.

    :ivar username: Required. The user to use for authentication against the CIFS file system.
    :vartype username: str
    :ivar source: Required. The URI of the file system to mount.
    :vartype source: str
    :ivar relative_mount_path: Required. All file systems are mounted relative to the Batch mounts
     directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
    :vartype relative_mount_path: str
    :ivar mount_options: These are 'net use' options in Windows and 'mount' options in Linux.
    :vartype mount_options: str
    :ivar password: Required. The password to use for authentication against the CIFS file system.
    :vartype password: str
    """

    _validation = {
        'username': {'required': True},
        'source': {'required': True},
        'relative_mount_path': {'required': True},
        'password': {'required': True},
    }

    _attribute_map = {
        'username': {'key': 'username', 'type': 'str'},
        'source': {'key': 'source', 'type': 'str'},
        'relative_mount_path': {'key': 'relativeMountPath', 'type': 'str'},
        'mount_options': {'key': 'mountOptions', 'type': 'str'},
        'password': {'key': 'password', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        username: str,
        source: str,
        relative_mount_path: str,
        password: str,
        mount_options: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword username: Required. The user to use for authentication against the CIFS file system.
        :paramtype username: str
        :keyword source: Required. The URI of the file system to mount.
        :paramtype source: str
        :keyword relative_mount_path: Required. All file systems are mounted relative to the Batch
         mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        :paramtype relative_mount_path: str
        :keyword mount_options: These are 'net use' options in Windows and 'mount' options in Linux.
        :paramtype mount_options: str
        :keyword password: Required. The password to use for authentication against the CIFS file
         system.
        :paramtype password: str
        """
        super(CIFSMountConfiguration, self).__init__(**kwargs)
        self.username = username
        self.source = source
        self.relative_mount_path = relative_mount_path
        self.mount_options = mount_options
        self.password = password


class CloudServiceConfiguration(msrest.serialization.Model):
    """The configuration for Compute Nodes in a Pool based on the Azure Cloud Services platform.

    All required parameters must be populated in order to send to Azure.

    :ivar os_family: Required. Possible values are:
     2 - OS Family 2, equivalent to Windows Server 2008 R2 SP1.
     3 - OS Family 3, equivalent to Windows Server 2012.
     4 - OS Family 4, equivalent to Windows Server 2012 R2.
     5 - OS Family 5, equivalent to Windows Server 2016.
     6 - OS Family 6, equivalent to Windows Server 2019. For more information, see Azure Guest OS
     Releases
     (https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
    :vartype os_family: str
    :ivar os_version: The default value is * which specifies the latest operating system version
     for the specified OS family.
    :vartype os_version: str
    """

    _validation = {
        'os_family': {'required': True},
    }

    _attribute_map = {
        'os_family': {'key': 'osFamily', 'type': 'str'},
        'os_version': {'key': 'osVersion', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        os_family: str,
        os_version: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword os_family: Required. Possible values are:
         2 - OS Family 2, equivalent to Windows Server 2008 R2 SP1.
         3 - OS Family 3, equivalent to Windows Server 2012.
         4 - OS Family 4, equivalent to Windows Server 2012 R2.
         5 - OS Family 5, equivalent to Windows Server 2016.
         6 - OS Family 6, equivalent to Windows Server 2019. For more information, see Azure Guest OS
         Releases
         (https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
        :paramtype os_family: str
        :keyword os_version: The default value is * which specifies the latest operating system version
         for the specified OS family.
        :paramtype os_version: str
        """
        super(CloudServiceConfiguration, self).__init__(**kwargs)
        self.os_family = os_family
        self.os_version = os_version


class ComputeNode(msrest.serialization.Model):
    """A Compute Node in the Batch service.

    :ivar id: Every Compute Node that is added to a Pool is assigned a unique ID. Whenever a
     Compute Node is removed from a Pool, all of its local files are deleted, and the ID is
     reclaimed and could be reused for new Compute Nodes.
    :vartype id: str
    :ivar url: The URL of the Compute Node.
    :vartype url: str
    :ivar state: The Spot/Low-priority Compute Node has been preempted. Tasks which were running on
     the Compute Node when it was preempted will be rescheduled when another Compute Node becomes
     available. Known values are: "idle", "rebooting", "reimaging", "running", "unusable",
     "creating", "starting", "waitingforstarttask", "starttaskfailed", "unknown", "leavingpool",
     "offline", "preempted".
    :vartype state: str or ~azure-batch.models.ComputeNodeState
    :ivar scheduling_state: Whether the Compute Node is available for Task scheduling. Known values
     are: "enabled", "disabled".
    :vartype scheduling_state: str or ~azure-batch.models.SchedulingState
    :ivar state_transition_time: The time at which the Compute Node entered its current state.
    :vartype state_transition_time: ~datetime.datetime
    :ivar last_boot_time: This property may not be present if the Compute Node state is unusable.
    :vartype last_boot_time: ~datetime.datetime
    :ivar allocation_time: This is the time when the Compute Node was initially allocated and
     doesn't change once set. It is not updated when the Compute Node is service healed or
     preempted.
    :vartype allocation_time: ~datetime.datetime
    :ivar ip_address: Every Compute Node that is added to a Pool is assigned a unique IP address.
     Whenever a Compute Node is removed from a Pool, all of its local files are deleted, and the IP
     address is reclaimed and could be reused for new Compute Nodes.
    :vartype ip_address: str
    :ivar affinity_id: Note that this is just a soft affinity. If the target Compute Node is busy
     or unavailable at the time the Task is scheduled, then the Task will be scheduled elsewhere.
    :vartype affinity_id: str
    :ivar vm_size: For information about available sizes of virtual machines in Pools, see Choose a
     VM size for Compute Nodes in an Azure Batch Pool
     (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
    :vartype vm_size: str
    :ivar total_tasks_run: The total number of Job Tasks completed on the Compute Node. This
     includes Job Manager Tasks and normal Tasks, but not Job Preparation, Job Release or Start
     Tasks.
    :vartype total_tasks_run: int
    :ivar running_tasks_count: The total number of currently running Job Tasks on the Compute Node.
     This includes Job Manager Tasks and normal Tasks, but not Job Preparation, Job Release or Start
     Tasks.
    :vartype running_tasks_count: int
    :ivar running_task_slots_count: The total number of scheduling slots used by currently running
     Job Tasks on the Compute Node. This includes Job Manager Tasks and normal Tasks, but not Job
     Preparation, Job Release or Start Tasks.
    :vartype running_task_slots_count: int
    :ivar total_tasks_succeeded: The total number of Job Tasks which completed successfully (with
     exitCode 0) on the Compute Node. This includes Job Manager Tasks and normal Tasks, but not Job
     Preparation, Job Release or Start Tasks.
    :vartype total_tasks_succeeded: int
    :ivar recent_tasks: This property is present only if at least one Task has run on this Compute
     Node since it was assigned to the Pool.
    :vartype recent_tasks: list[~azure-batch.models.TaskInformation]
    :ivar start_task: Batch will retry Tasks when a recovery operation is triggered on a Node.
     Examples of recovery operations include (but are not limited to) when an unhealthy Node is
     rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations
     are independent of and are not counted against the maxTaskRetryCount. Even if the
     maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of
     this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and
     restarted without causing any corruption or duplicate data. The best practice for long running
     Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even
     though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks
     which create breakaway process or install/launch services from the StartTask working directory,
     as this will block Batch from being able to re-run the StartTask.
    :vartype start_task: ~azure-batch.models.StartTask
    :ivar start_task_info: Information about a StartTask running on a Compute Node.
    :vartype start_task_info: ~azure-batch.models.StartTaskInformation
    :ivar certificate_references: For Windows Nodes, the Batch service installs the Certificates to
     the specified Certificate store and location. For Linux Compute Nodes, the Certificates are
     stored in a directory inside the Task working directory and an environment variable
     AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates
     with visibility of 'remoteUser', a 'certs' directory is created in the user's home directory
     (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
    :vartype certificate_references: list[~azure-batch.models.CertificateReference]
    :ivar errors: The list of errors that are currently being encountered by the Compute Node.
    :vartype errors: list[~azure-batch.models.ComputeNodeError]
    :ivar is_dedicated: Whether this Compute Node is a dedicated Compute Node. If false, the
     Compute Node is a Spot/Low-priority Compute Node.
    :vartype is_dedicated: bool
    :ivar endpoint_configuration: The endpoint configuration for the Compute Node.
    :vartype endpoint_configuration: ~azure-batch.models.ComputeNodeEndpointConfiguration
    :ivar node_agent_info: The Batch Compute Node agent is a program that runs on each Compute Node
     in the Pool and provides Batch capability on the Compute Node.
    :vartype node_agent_info: ~azure-batch.models.NodeAgentInformation
    :ivar virtual_machine_info: Info about the current state of the virtual machine.
    :vartype virtual_machine_info: ~azure-batch.models.VirtualMachineInfo
    """

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'url': {'key': 'url', 'type': 'str'},
        'state': {'key': 'state', 'type': 'str'},
        'scheduling_state': {'key': 'schedulingState', 'type': 'str'},
        'state_transition_time': {'key': 'stateTransitionTime', 'type': 'iso-8601'},
        'last_boot_time': {'key': 'lastBootTime', 'type': 'iso-8601'},
        'allocation_time': {'key': 'allocationTime', 'type': 'iso-8601'},
        'ip_address': {'key': 'ipAddress', 'type': 'str'},
        'affinity_id': {'key': 'affinityId', 'type': 'str'},
        'vm_size': {'key': 'vmSize', 'type': 'str'},
        'total_tasks_run': {'key': 'totalTasksRun', 'type': 'int'},
        'running_tasks_count': {'key': 'runningTasksCount', 'type': 'int'},
        'running_task_slots_count': {'key': 'runningTaskSlotsCount', 'type': 'int'},
        'total_tasks_succeeded': {'key': 'totalTasksSucceeded', 'type': 'int'},
        'recent_tasks': {'key': 'recentTasks', 'type': '[TaskInformation]'},
        'start_task': {'key': 'startTask', 'type': 'StartTask'},
        'start_task_info': {'key': 'startTaskInfo', 'type': 'StartTaskInformation'},
        'certificate_references': {'key': 'certificateReferences', 'type': '[CertificateReference]'},
        'errors': {'key': 'errors', 'type': '[ComputeNodeError]'},
        'is_dedicated': {'key': 'isDedicated', 'type': 'bool'},
        'endpoint_configuration': {'key': 'endpointConfiguration', 'type': 'ComputeNodeEndpointConfiguration'},
        'node_agent_info': {'key': 'nodeAgentInfo', 'type': 'NodeAgentInformation'},
        'virtual_machine_info': {'key': 'virtualMachineInfo', 'type': 'VirtualMachineInfo'},
    }

    def __init__(
        self,
        *,
        id: Optional[str] = None,
        url: Optional[str] = None,
        state: Optional[Union[str, "_models.ComputeNodeState"]] = None,
        scheduling_state: Optional[Union[str, "_models.SchedulingState"]] = None,
        state_transition_time: Optional[datetime.datetime] = None,
        last_boot_time: Optional[datetime.datetime] = None,
        allocation_time: Optional[datetime.datetime] = None,
        ip_address: Optional[str] = None,
        affinity_id: Optional[str] = None,
        vm_size: Optional[str] = None,
        total_tasks_run: Optional[int] = None,
        running_tasks_count: Optional[int] = None,
        running_task_slots_count: Optional[int] = None,
        total_tasks_succeeded: Optional[int] = None,
        recent_tasks: Optional[List["_models.TaskInformation"]] = None,
        start_task: Optional["_models.StartTask"] = None,
        start_task_info: Optional["_models.StartTaskInformation"] = None,
        certificate_references: Optional[List["_models.CertificateReference"]] = None,
        errors: Optional[List["_models.ComputeNodeError"]] = None,
        is_dedicated: Optional[bool] = None,
        endpoint_configuration: Optional["_models.ComputeNodeEndpointConfiguration"] = None,
        node_agent_info: Optional["_models.NodeAgentInformation"] = None,
        virtual_machine_info: Optional["_models.VirtualMachineInfo"] = None,
        **kwargs
    ):
        """
        :keyword id: Every Compute Node that is added to a Pool is assigned a unique ID. Whenever a
         Compute Node is removed from a Pool, all of its local files are deleted, and the ID is
         reclaimed and could be reused for new Compute Nodes.
        :paramtype id: str
        :keyword url: The URL of the Compute Node.
        :paramtype url: str
        :keyword state: The Spot/Low-priority Compute Node has been preempted. Tasks which were running
         on the Compute Node when it was preempted will be rescheduled when another Compute Node becomes
         available. Known values are: "idle", "rebooting", "reimaging", "running", "unusable",
         "creating", "starting", "waitingforstarttask", "starttaskfailed", "unknown", "leavingpool",
         "offline", "preempted".
        :paramtype state: str or ~azure-batch.models.ComputeNodeState
        :keyword scheduling_state: Whether the Compute Node is available for Task scheduling. Known
         values are: "enabled", "disabled".
        :paramtype scheduling_state: str or ~azure-batch.models.SchedulingState
        :keyword state_transition_time: The time at which the Compute Node entered its current state.
        :paramtype state_transition_time: ~datetime.datetime
        :keyword last_boot_time: This property may not be present if the Compute Node state is
         unusable.
        :paramtype last_boot_time: ~datetime.datetime
        :keyword allocation_time: This is the time when the Compute Node was initially allocated and
         doesn't change once set. It is not updated when the Compute Node is service healed or
         preempted.
        :paramtype allocation_time: ~datetime.datetime
        :keyword ip_address: Every Compute Node that is added to a Pool is assigned a unique IP
         address. Whenever a Compute Node is removed from a Pool, all of its local files are deleted,
         and the IP address is reclaimed and could be reused for new Compute Nodes.
        :paramtype ip_address: str
        :keyword affinity_id: Note that this is just a soft affinity. If the target Compute Node is
         busy or unavailable at the time the Task is scheduled, then the Task will be scheduled
         elsewhere.
        :paramtype affinity_id: str
        :keyword vm_size: For information about available sizes of virtual machines in Pools, see
         Choose a VM size for Compute Nodes in an Azure Batch Pool
         (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        :paramtype vm_size: str
        :keyword total_tasks_run: The total number of Job Tasks completed on the Compute Node. This
         includes Job Manager Tasks and normal Tasks, but not Job Preparation, Job Release or Start
         Tasks.
        :paramtype total_tasks_run: int
        :keyword running_tasks_count: The total number of currently running Job Tasks on the Compute
         Node. This includes Job Manager Tasks and normal Tasks, but not Job Preparation, Job Release or
         Start Tasks.
        :paramtype running_tasks_count: int
        :keyword running_task_slots_count: The total number of scheduling slots used by currently
         running Job Tasks on the Compute Node. This includes Job Manager Tasks and normal Tasks, but
         not Job Preparation, Job Release or Start Tasks.
        :paramtype running_task_slots_count: int
        :keyword total_tasks_succeeded: The total number of Job Tasks which completed successfully
         (with exitCode 0) on the Compute Node. This includes Job Manager Tasks and normal Tasks, but
         not Job Preparation, Job Release or Start Tasks.
        :paramtype total_tasks_succeeded: int
        :keyword recent_tasks: This property is present only if at least one Task has run on this
         Compute Node since it was assigned to the Pool.
        :paramtype recent_tasks: list[~azure-batch.models.TaskInformation]
        :keyword start_task: Batch will retry Tasks when a recovery operation is triggered on a Node.
         Examples of recovery operations include (but are not limited to) when an unhealthy Node is
         rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations
         are independent of and are not counted against the maxTaskRetryCount. Even if the
         maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of
         this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and
         restarted without causing any corruption or duplicate data. The best practice for long running
         Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even
         though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks
         which create breakaway process or install/launch services from the StartTask working directory,
         as this will block Batch from being able to re-run the StartTask.
        :paramtype start_task: ~azure-batch.models.StartTask
        :keyword start_task_info: Information about a StartTask running on a Compute Node.
        :paramtype start_task_info: ~azure-batch.models.StartTaskInformation
        :keyword certificate_references: For Windows Nodes, the Batch service installs the Certificates
         to the specified Certificate store and location. For Linux Compute Nodes, the Certificates are
         stored in a directory inside the Task working directory and an environment variable
         AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates
         with visibility of 'remoteUser', a 'certs' directory is created in the user's home directory
         (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        :paramtype certificate_references: list[~azure-batch.models.CertificateReference]
        :keyword errors: The list of errors that are currently being encountered by the Compute Node.
        :paramtype errors: list[~azure-batch.models.ComputeNodeError]
        :keyword is_dedicated: Whether this Compute Node is a dedicated Compute Node. If false, the
         Compute Node is a Spot/Low-priority Compute Node.
        :paramtype is_dedicated: bool
        :keyword endpoint_configuration: The endpoint configuration for the Compute Node.
        :paramtype endpoint_configuration: ~azure-batch.models.ComputeNodeEndpointConfiguration
        :keyword node_agent_info: The Batch Compute Node agent is a program that runs on each Compute
         Node in the Pool and provides Batch capability on the Compute Node.
        :paramtype node_agent_info: ~azure-batch.models.NodeAgentInformation
        :keyword virtual_machine_info: Info about the current state of the virtual machine.
        :paramtype virtual_machine_info: ~azure-batch.models.VirtualMachineInfo
        """
        super(ComputeNode, self).__init__(**kwargs)
        self.id = id
        self.url = url
        self.state = state
        self.scheduling_state = scheduling_state
        self.state_transition_time = state_transition_time
        self.last_boot_time = last_boot_time
        self.allocation_time = allocation_time
        self.ip_address = ip_address
        self.affinity_id = affinity_id
        self.vm_size = vm_size
        self.total_tasks_run = total_tasks_run
        self.running_tasks_count = running_tasks_count
        self.running_task_slots_count = running_task_slots_count
        self.total_tasks_succeeded = total_tasks_succeeded
        self.recent_tasks = recent_tasks
        self.start_task = start_task
        self.start_task_info = start_task_info
        self.certificate_references = certificate_references
        self.errors = errors
        self.is_dedicated = is_dedicated
        self.endpoint_configuration = endpoint_configuration
        self.node_agent_info = node_agent_info
        self.virtual_machine_info = virtual_machine_info


class ComputeNodeEndpointConfiguration(msrest.serialization.Model):
    """The endpoint configuration for the Compute Node.

    All required parameters must be populated in order to send to Azure.

    :ivar inbound_endpoints: Required. The list of inbound endpoints that are accessible on the
     Compute Node.
    :vartype inbound_endpoints: list[~azure-batch.models.InboundEndpoint]
    """

    _validation = {
        'inbound_endpoints': {'required': True},
    }

    _attribute_map = {
        'inbound_endpoints': {'key': 'inboundEndpoints', 'type': '[InboundEndpoint]'},
    }

    def __init__(
        self,
        *,
        inbound_endpoints: List["_models.InboundEndpoint"],
        **kwargs
    ):
        """
        :keyword inbound_endpoints: Required. The list of inbound endpoints that are accessible on the
         Compute Node.
        :paramtype inbound_endpoints: list[~azure-batch.models.InboundEndpoint]
        """
        super(ComputeNodeEndpointConfiguration, self).__init__(**kwargs)
        self.inbound_endpoints = inbound_endpoints


class ComputeNodeError(msrest.serialization.Model):
    """An error encountered by a Compute Node.

    :ivar code: An identifier for the Compute Node error. Codes are invariant and are intended to
     be consumed programmatically.
    :vartype code: str
    :ivar message: A message describing the Compute Node error, intended to be suitable for display
     in a user interface.
    :vartype message: str
    :ivar error_details: The list of additional error details related to the Compute Node error.
    :vartype error_details: list[~azure-batch.models.NameValuePair]
    """

    _attribute_map = {
        'code': {'key': 'code', 'type': 'str'},
        'message': {'key': 'message', 'type': 'str'},
        'error_details': {'key': 'errorDetails', 'type': '[NameValuePair]'},
    }

    def __init__(
        self,
        *,
        code: Optional[str] = None,
        message: Optional[str] = None,
        error_details: Optional[List["_models.NameValuePair"]] = None,
        **kwargs
    ):
        """
        :keyword code: An identifier for the Compute Node error. Codes are invariant and are intended
         to be consumed programmatically.
        :paramtype code: str
        :keyword message: A message describing the Compute Node error, intended to be suitable for
         display in a user interface.
        :paramtype message: str
        :keyword error_details: The list of additional error details related to the Compute Node error.
        :paramtype error_details: list[~azure-batch.models.NameValuePair]
        """
        super(ComputeNodeError, self).__init__(**kwargs)
        self.code = code
        self.message = message
        self.error_details = error_details


class ComputeNodeGetRemoteLoginSettingsResult(msrest.serialization.Model):
    """The remote login settings for a Compute Node.

    All required parameters must be populated in order to send to Azure.

    :ivar remote_login_ip_address: Required. The IP address used for remote login to the Compute
     Node.
    :vartype remote_login_ip_address: str
    :ivar remote_login_port: Required. The port used for remote login to the Compute Node.
    :vartype remote_login_port: int
    """

    _validation = {
        'remote_login_ip_address': {'required': True},
        'remote_login_port': {'required': True},
    }

    _attribute_map = {
        'remote_login_ip_address': {'key': 'remoteLoginIPAddress', 'type': 'str'},
        'remote_login_port': {'key': 'remoteLoginPort', 'type': 'int'},
    }

    def __init__(
        self,
        *,
        remote_login_ip_address: str,
        remote_login_port: int,
        **kwargs
    ):
        """
        :keyword remote_login_ip_address: Required. The IP address used for remote login to the Compute
         Node.
        :paramtype remote_login_ip_address: str
        :keyword remote_login_port: Required. The port used for remote login to the Compute Node.
        :paramtype remote_login_port: int
        """
        super(ComputeNodeGetRemoteLoginSettingsResult, self).__init__(**kwargs)
        self.remote_login_ip_address = remote_login_ip_address
        self.remote_login_port = remote_login_port


class ComputeNodeIdentityReference(msrest.serialization.Model):
    """The reference to a user assigned identity associated with the Batch pool which a compute node will use.

    :ivar resource_id: The ARM resource id of the user assigned identity.
    :vartype resource_id: str
    """

    _attribute_map = {
        'resource_id': {'key': 'resourceId', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        resource_id: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword resource_id: The ARM resource id of the user assigned identity.
        :paramtype resource_id: str
        """
        super(ComputeNodeIdentityReference, self).__init__(**kwargs)
        self.resource_id = resource_id


class ComputeNodeInformation(msrest.serialization.Model):
    """Information about the Compute Node on which a Task ran.

    :ivar affinity_id: An identifier for the Node on which the Task ran, which can be passed when
     adding a Task to request that the Task be scheduled on this Compute Node.
    :vartype affinity_id: str
    :ivar node_url: The URL of the Compute Node on which the Task ran.
    :vartype node_url: str
    :ivar pool_id: The ID of the Pool on which the Task ran.
    :vartype pool_id: str
    :ivar node_id: The ID of the Compute Node on which the Task ran.
    :vartype node_id: str
    :ivar task_root_directory: The root directory of the Task on the Compute Node.
    :vartype task_root_directory: str
    :ivar task_root_directory_url: The URL to the root directory of the Task on the Compute Node.
    :vartype task_root_directory_url: str
    """

    _attribute_map = {
        'affinity_id': {'key': 'affinityId', 'type': 'str'},
        'node_url': {'key': 'nodeUrl', 'type': 'str'},
        'pool_id': {'key': 'poolId', 'type': 'str'},
        'node_id': {'key': 'nodeId', 'type': 'str'},
        'task_root_directory': {'key': 'taskRootDirectory', 'type': 'str'},
        'task_root_directory_url': {'key': 'taskRootDirectoryUrl', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        affinity_id: Optional[str] = None,
        node_url: Optional[str] = None,
        pool_id: Optional[str] = None,
        node_id: Optional[str] = None,
        task_root_directory: Optional[str] = None,
        task_root_directory_url: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword affinity_id: An identifier for the Node on which the Task ran, which can be passed
         when adding a Task to request that the Task be scheduled on this Compute Node.
        :paramtype affinity_id: str
        :keyword node_url: The URL of the Compute Node on which the Task ran.
        :paramtype node_url: str
        :keyword pool_id: The ID of the Pool on which the Task ran.
        :paramtype pool_id: str
        :keyword node_id: The ID of the Compute Node on which the Task ran.
        :paramtype node_id: str
        :keyword task_root_directory: The root directory of the Task on the Compute Node.
        :paramtype task_root_directory: str
        :keyword task_root_directory_url: The URL to the root directory of the Task on the Compute
         Node.
        :paramtype task_root_directory_url: str
        """
        super(ComputeNodeInformation, self).__init__(**kwargs)
        self.affinity_id = affinity_id
        self.node_url = node_url
        self.pool_id = pool_id
        self.node_id = node_id
        self.task_root_directory = task_root_directory
        self.task_root_directory_url = task_root_directory_url


class ComputeNodeListResult(msrest.serialization.Model):
    """The result of listing the Compute Nodes in a Pool.

    :ivar value: The list of Compute Nodes.
    :vartype value: list[~azure-batch.models.ComputeNode]
    :ivar odata_next_link: The URL to get the next set of results.
    :vartype odata_next_link: str
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[ComputeNode]'},
        'odata_next_link': {'key': 'odata\\.nextLink', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["_models.ComputeNode"]] = None,
        odata_next_link: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword value: The list of Compute Nodes.
        :paramtype value: list[~azure-batch.models.ComputeNode]
        :keyword odata_next_link: The URL to get the next set of results.
        :paramtype odata_next_link: str
        """
        super(ComputeNodeListResult, self).__init__(**kwargs)
        self.value = value
        self.odata_next_link = odata_next_link


class ComputeNodeUser(msrest.serialization.Model):
    """A user Account for RDP or SSH access on a Compute Node.

    All required parameters must be populated in order to send to Azure.

    :ivar name: Required. The user name of the Account.
    :vartype name: str
    :ivar is_admin: The default value is false.
    :vartype is_admin: bool
    :ivar expiry_time: If omitted, the default is 1 day from the current time. For Linux Compute
     Nodes, the expiryTime has a precision up to a day.
    :vartype expiry_time: ~datetime.datetime
    :ivar password: The password is required for Windows Compute Nodes (those created with
     'cloudServiceConfiguration', or created with 'virtualMachineConfiguration' using a Windows
     Image reference). For Linux Compute Nodes, the password can optionally be specified along with
     the sshPublicKey property.
    :vartype password: str
    :ivar ssh_public_key: The public key should be compatible with OpenSSH encoding and should be
     base 64 encoded. This property can be specified only for Linux Compute Nodes. If this is
     specified for a Windows Compute Node, then the Batch service rejects the request; if you are
     calling the REST API directly, the HTTP status code is 400 (Bad Request).
    :vartype ssh_public_key: str
    """

    _validation = {
        'name': {'required': True},
    }

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'is_admin': {'key': 'isAdmin', 'type': 'bool'},
        'expiry_time': {'key': 'expiryTime', 'type': 'iso-8601'},
        'password': {'key': 'password', 'type': 'str'},
        'ssh_public_key': {'key': 'sshPublicKey', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        is_admin: Optional[bool] = None,
        expiry_time: Optional[datetime.datetime] = None,
        password: Optional[str] = None,
        ssh_public_key: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword name: Required. The user name of the Account.
        :paramtype name: str
        :keyword is_admin: The default value is false.
        :paramtype is_admin: bool
        :keyword expiry_time: If omitted, the default is 1 day from the current time. For Linux Compute
         Nodes, the expiryTime has a precision up to a day.
        :paramtype expiry_time: ~datetime.datetime
        :keyword password: The password is required for Windows Compute Nodes (those created with
         'cloudServiceConfiguration', or created with 'virtualMachineConfiguration' using a Windows
         Image reference). For Linux Compute Nodes, the password can optionally be specified along with
         the sshPublicKey property.
        :paramtype password: str
        :keyword ssh_public_key: The public key should be compatible with OpenSSH encoding and should
         be base 64 encoded. This property can be specified only for Linux Compute Nodes. If this is
         specified for a Windows Compute Node, then the Batch service rejects the request; if you are
         calling the REST API directly, the HTTP status code is 400 (Bad Request).
        :paramtype ssh_public_key: str
        """
        super(ComputeNodeUser, self).__init__(**kwargs)
        self.name = name
        self.is_admin = is_admin
        self.expiry_time = expiry_time
        self.password = password
        self.ssh_public_key = ssh_public_key


class ContainerConfiguration(msrest.serialization.Model):
    """The configuration for container-enabled Pools.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar type: The container technology to be used. Has constant value: "dockerCompatible".
    :vartype type: str
    :ivar container_image_names: This is the full Image reference, as would be specified to "docker
     pull". An Image will be sourced from the default Docker registry unless the Image is fully
     qualified with an alternative registry.
    :vartype container_image_names: list[str]
    :ivar container_registries: If any Images must be downloaded from a private registry which
     requires credentials, then those credentials must be provided here.
    :vartype container_registries: list[~azure-batch.models.ContainerRegistry]
    """

    _validation = {
        'type': {'required': True, 'constant': True},
    }

    _attribute_map = {
        'type': {'key': 'type', 'type': 'str'},
        'container_image_names': {'key': 'containerImageNames', 'type': '[str]'},
        'container_registries': {'key': 'containerRegistries', 'type': '[ContainerRegistry]'},
    }

    type = "dockerCompatible"

    def __init__(
        self,
        *,
        container_image_names: Optional[List[str]] = None,
        container_registries: Optional[List["_models.ContainerRegistry"]] = None,
        **kwargs
    ):
        """
        :keyword container_image_names: This is the full Image reference, as would be specified to
         "docker pull". An Image will be sourced from the default Docker registry unless the Image is
         fully qualified with an alternative registry.
        :paramtype container_image_names: list[str]
        :keyword container_registries: If any Images must be downloaded from a private registry which
         requires credentials, then those credentials must be provided here.
        :paramtype container_registries: list[~azure-batch.models.ContainerRegistry]
        """
        super(ContainerConfiguration, self).__init__(**kwargs)
        self.container_image_names = container_image_names
        self.container_registries = container_registries


class ContainerRegistry(msrest.serialization.Model):
    """A private container registry.

    :ivar user_name: The user name to log into the registry server.
    :vartype user_name: str
    :ivar password: The password to log into the registry server.
    :vartype password: str
    :ivar registry_server: If omitted, the default is "docker.io".
    :vartype registry_server: str
    :ivar identity_reference: The reference to a user assigned identity associated with the Batch
     pool which a compute node will use.
    :vartype identity_reference: ~azure-batch.models.ComputeNodeIdentityReference
    """

    _attribute_map = {
        'user_name': {'key': 'username', 'type': 'str'},
        'password': {'key': 'password', 'type': 'str'},
        'registry_server': {'key': 'registryServer', 'type': 'str'},
        'identity_reference': {'key': 'identityReference', 'type': 'ComputeNodeIdentityReference'},
    }

    def __init__(
        self,
        *,
        user_name: Optional[str] = None,
        password: Optional[str] = None,
        registry_server: Optional[str] = None,
        identity_reference: Optional["_models.ComputeNodeIdentityReference"] = None,
        **kwargs
    ):
        """
        :keyword user_name: The user name to log into the registry server.
        :paramtype user_name: str
        :keyword password: The password to log into the registry server.
        :paramtype password: str
        :keyword registry_server: If omitted, the default is "docker.io".
        :paramtype registry_server: str
        :keyword identity_reference: The reference to a user assigned identity associated with the
         Batch pool which a compute node will use.
        :paramtype identity_reference: ~azure-batch.models.ComputeNodeIdentityReference
        """
        super(ContainerRegistry, self).__init__(**kwargs)
        self.user_name = user_name
        self.password = password
        self.registry_server = registry_server
        self.identity_reference = identity_reference


class DataDisk(msrest.serialization.Model):
    """Settings which will be used by the data disks associated to Compute Nodes in the Pool. When using attached data disks, you need to mount and format the disks from within a VM to use them.

    All required parameters must be populated in order to send to Azure.

    :ivar lun: Required. The lun is used to uniquely identify each data disk. If attaching multiple
     disks, each should have a distinct lun. The value must be between 0 and 63, inclusive.
    :vartype lun: int
    :ivar caching: The default value for caching is readwrite. For information about the caching
     options see:
     https://blogs.msdn.microsoft.com/windowsazurestorage/2012/06/27/exploring-windows-azure-drives-disks-and-images/.
     Known values are: "none", "readonly", "readwrite".
    :vartype caching: str or ~azure-batch.models.CachingType
    :ivar disk_size_gb: Required. The initial disk size in gigabytes.
    :vartype disk_size_gb: int
    :ivar storage_account_type: If omitted, the default is "standard_lrs". Known values are:
     "standard_lrs", "premium_lrs".
    :vartype storage_account_type: str or ~azure-batch.models.StorageAccountType
    """

    _validation = {
        'lun': {'required': True},
        'disk_size_gb': {'required': True},
    }

    _attribute_map = {
        'lun': {'key': 'lun', 'type': 'int'},
        'caching': {'key': 'caching', 'type': 'str'},
        'disk_size_gb': {'key': 'diskSizeGB', 'type': 'int'},
        'storage_account_type': {'key': 'storageAccountType', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        lun: int,
        disk_size_gb: int,
        caching: Optional[Union[str, "_models.CachingType"]] = None,
        storage_account_type: Optional[Union[str, "_models.StorageAccountType"]] = None,
        **kwargs
    ):
        """
        :keyword lun: Required. The lun is used to uniquely identify each data disk. If attaching
         multiple disks, each should have a distinct lun. The value must be between 0 and 63, inclusive.
        :paramtype lun: int
        :keyword caching: The default value for caching is readwrite. For information about the caching
         options see:
         https://blogs.msdn.microsoft.com/windowsazurestorage/2012/06/27/exploring-windows-azure-drives-disks-and-images/.
         Known values are: "none", "readonly", "readwrite".
        :paramtype caching: str or ~azure-batch.models.CachingType
        :keyword disk_size_gb: Required. The initial disk size in gigabytes.
        :paramtype disk_size_gb: int
        :keyword storage_account_type: If omitted, the default is "standard_lrs". Known values are:
         "standard_lrs", "premium_lrs".
        :paramtype storage_account_type: str or ~azure-batch.models.StorageAccountType
        """
        super(DataDisk, self).__init__(**kwargs)
        self.lun = lun
        self.caching = caching
        self.disk_size_gb = disk_size_gb
        self.storage_account_type = storage_account_type


class DeleteCertificateError(msrest.serialization.Model):
    """An error encountered by the Batch service when deleting a Certificate.

    :ivar code: An identifier for the Certificate deletion error. Codes are invariant and are
     intended to be consumed programmatically.
    :vartype code: str
    :ivar message: A message describing the Certificate deletion error, intended to be suitable for
     display in a user interface.
    :vartype message: str
    :ivar values: This list includes details such as the active Pools and Compute Nodes referencing
     this Certificate. However, if a large number of resources reference the Certificate, the list
     contains only about the first hundred.
    :vartype values: list[~azure-batch.models.NameValuePair]
    """

    _attribute_map = {
        'code': {'key': 'code', 'type': 'str'},
        'message': {'key': 'message', 'type': 'str'},
        'values': {'key': 'values', 'type': '[NameValuePair]'},
    }

    def __init__(
        self,
        *,
        code: Optional[str] = None,
        message: Optional[str] = None,
        values: Optional[List["_models.NameValuePair"]] = None,
        **kwargs
    ):
        """
        :keyword code: An identifier for the Certificate deletion error. Codes are invariant and are
         intended to be consumed programmatically.
        :paramtype code: str
        :keyword message: A message describing the Certificate deletion error, intended to be suitable
         for display in a user interface.
        :paramtype message: str
        :keyword values: This list includes details such as the active Pools and Compute Nodes
         referencing this Certificate. However, if a large number of resources reference the
         Certificate, the list contains only about the first hundred.
        :paramtype values: list[~azure-batch.models.NameValuePair]
        """
        super(DeleteCertificateError, self).__init__(**kwargs)
        self.code = code
        self.message = message
        self.values = values


class DiffDiskSettings(msrest.serialization.Model):
    """Specifies the ephemeral Disk Settings for the operating system disk used by the compute node (VM).

    :ivar placement: This property can be used by user in the request to choose the location e.g.,
     cache disk space for Ephemeral OS disk provisioning. For more information on Ephemeral OS disk
     size requirements, please refer to Ephemeral OS disk size requirements for Windows VMs at
     https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements
     and Linux VMs at
     https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
     The only acceptable values to pass in are None and "CacheDisk". The default value is None.
    :vartype placement: str
    """

    _attribute_map = {
        'placement': {'key': 'placement', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        placement: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword placement: This property can be used by user in the request to choose the location
         e.g., cache disk space for Ephemeral OS disk provisioning. For more information on Ephemeral OS
         disk size requirements, please refer to Ephemeral OS disk size requirements for Windows VMs at
         https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements
         and Linux VMs at
         https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
         The only acceptable values to pass in are None and "CacheDisk". The default value is None.
        :paramtype placement: str
        """
        super(DiffDiskSettings, self).__init__(**kwargs)
        self.placement = placement


class DiskEncryptionConfiguration(msrest.serialization.Model):
    """The disk encryption configuration applied on compute nodes in the pool. Disk encryption configuration is not supported on Linux pool created with Shared Image Gallery Image.

    :ivar targets: If omitted, no disks on the compute nodes in the pool will be encrypted. On
     Linux pool, only "TemporaryDisk" is supported; on Windows pool, "OsDisk" and "TemporaryDisk"
     must be specified.
    :vartype targets: list[str or ~azure-batch.models.DiskEncryptionTarget]
    """

    _attribute_map = {
        'targets': {'key': 'targets', 'type': '[str]'},
    }

    def __init__(
        self,
        *,
        targets: Optional[List[Union[str, "_models.DiskEncryptionTarget"]]] = None,
        **kwargs
    ):
        """
        :keyword targets: If omitted, no disks on the compute nodes in the pool will be encrypted. On
         Linux pool, only "TemporaryDisk" is supported; on Windows pool, "OsDisk" and "TemporaryDisk"
         must be specified.
        :paramtype targets: list[str or ~azure-batch.models.DiskEncryptionTarget]
        """
        super(DiskEncryptionConfiguration, self).__init__(**kwargs)
        self.targets = targets


class EnvironmentSetting(msrest.serialization.Model):
    """An environment variable to be set on a Task process.

    All required parameters must be populated in order to send to Azure.

    :ivar name: Required. The name of the environment variable.
    :vartype name: str
    :ivar value: The value of the environment variable.
    :vartype value: str
    """

    _validation = {
        'name': {'required': True},
    }

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'value': {'key': 'value', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        value: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword name: Required. The name of the environment variable.
        :paramtype name: str
        :keyword value: The value of the environment variable.
        :paramtype value: str
        """
        super(EnvironmentSetting, self).__init__(**kwargs)
        self.name = name
        self.value = value


class ErrorMessage(msrest.serialization.Model):
    """An error message received in an Azure Batch error response.

    :ivar lang: The language code of the error message.
    :vartype lang: str
    :ivar value: The text of the message.
    :vartype value: str
    """

    _attribute_map = {
        'lang': {'key': 'lang', 'type': 'str'},
        'value': {'key': 'value', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        lang: Optional[str] = None,
        value: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword lang: The language code of the error message.
        :paramtype lang: str
        :keyword value: The text of the message.
        :paramtype value: str
        """
        super(ErrorMessage, self).__init__(**kwargs)
        self.lang = lang
        self.value = value


class ExitCodeMapping(msrest.serialization.Model):
    """How the Batch service should respond if a Task exits with a particular exit code.

    All required parameters must be populated in order to send to Azure.

    :ivar code: Required. A process exit code.
    :vartype code: int
    :ivar exit_options: Required. Specifies how the Batch service responds to a particular exit
     condition.
    :vartype exit_options: ~azure-batch.models.ExitOptions
    """

    _validation = {
        'code': {'required': True},
        'exit_options': {'required': True},
    }

    _attribute_map = {
        'code': {'key': 'code', 'type': 'int'},
        'exit_options': {'key': 'exitOptions', 'type': 'ExitOptions'},
    }

    def __init__(
        self,
        *,
        code: int,
        exit_options: "_models.ExitOptions",
        **kwargs
    ):
        """
        :keyword code: Required. A process exit code.
        :paramtype code: int
        :keyword exit_options: Required. Specifies how the Batch service responds to a particular exit
         condition.
        :paramtype exit_options: ~azure-batch.models.ExitOptions
        """
        super(ExitCodeMapping, self).__init__(**kwargs)
        self.code = code
        self.exit_options = exit_options


class ExitCodeRangeMapping(msrest.serialization.Model):
    """A range of exit codes and how the Batch service should respond to exit codes within that range.

    All required parameters must be populated in order to send to Azure.

    :ivar start: Required. The first exit code in the range.
    :vartype start: int
    :ivar end: Required. The last exit code in the range.
    :vartype end: int
    :ivar exit_options: Required. Specifies how the Batch service responds to a particular exit
     condition.
    :vartype exit_options: ~azure-batch.models.ExitOptions
    """

    _validation = {
        'start': {'required': True},
        'end': {'required': True},
        'exit_options': {'required': True},
    }

    _attribute_map = {
        'start': {'key': 'start', 'type': 'int'},
        'end': {'key': 'end', 'type': 'int'},
        'exit_options': {'key': 'exitOptions', 'type': 'ExitOptions'},
    }

    def __init__(
        self,
        *,
        start: int,
        end: int,
        exit_options: "_models.ExitOptions",
        **kwargs
    ):
        """
        :keyword start: Required. The first exit code in the range.
        :paramtype start: int
        :keyword end: Required. The last exit code in the range.
        :paramtype end: int
        :keyword exit_options: Required. Specifies how the Batch service responds to a particular exit
         condition.
        :paramtype exit_options: ~azure-batch.models.ExitOptions
        """
        super(ExitCodeRangeMapping, self).__init__(**kwargs)
        self.start = start
        self.end = end
        self.exit_options = exit_options


class ExitConditions(msrest.serialization.Model):
    """Specifies how the Batch service should respond when the Task completes.

    :ivar exit_codes: A list of individual Task exit codes and how the Batch service should respond
     to them.
    :vartype exit_codes: list[~azure-batch.models.ExitCodeMapping]
    :ivar exit_code_ranges: A list of Task exit code ranges and how the Batch service should
     respond to them.
    :vartype exit_code_ranges: list[~azure-batch.models.ExitCodeRangeMapping]
    :ivar pre_processing_error: Specifies how the Batch service responds to a particular exit
     condition.
    :vartype pre_processing_error: ~azure-batch.models.ExitOptions
    :ivar file_upload_error: If the Task exited with an exit code that was specified via exitCodes
     or exitCodeRanges, and then encountered a file upload error, then the action specified by the
     exit code takes precedence.
    :vartype file_upload_error: ~azure-batch.models.ExitOptions
    :ivar default: This value is used if the Task exits with any nonzero exit code not listed in
     the exitCodes or exitCodeRanges collection, with a pre-processing error if the
     preProcessingError property is not present, or with a file upload error if the fileUploadError
     property is not present. If you want non-default behavior on exit code 0, you must list it
     explicitly using the exitCodes or exitCodeRanges collection.
    :vartype default: ~azure-batch.models.ExitOptions
    """

    _attribute_map = {
        'exit_codes': {'key': 'exitCodes', 'type': '[ExitCodeMapping]'},
        'exit_code_ranges': {'key': 'exitCodeRanges', 'type': '[ExitCodeRangeMapping]'},
        'pre_processing_error': {'key': 'preProcessingError', 'type': 'ExitOptions'},
        'file_upload_error': {'key': 'fileUploadError', 'type': 'ExitOptions'},
        'default': {'key': 'default', 'type': 'ExitOptions'},
    }

    def __init__(
        self,
        *,
        exit_codes: Optional[List["_models.ExitCodeMapping"]] = None,
        exit_code_ranges: Optional[List["_models.ExitCodeRangeMapping"]] = None,
        pre_processing_error: Optional["_models.ExitOptions"] = None,
        file_upload_error: Optional["_models.ExitOptions"] = None,
        default: Optional["_models.ExitOptions"] = None,
        **kwargs
    ):
        """
        :keyword exit_codes: A list of individual Task exit codes and how the Batch service should
         respond to them.
        :paramtype exit_codes: list[~azure-batch.models.ExitCodeMapping]
        :keyword exit_code_ranges: A list of Task exit code ranges and how the Batch service should
         respond to them.
        :paramtype exit_code_ranges: list[~azure-batch.models.ExitCodeRangeMapping]
        :keyword pre_processing_error: Specifies how the Batch service responds to a particular exit
         condition.
        :paramtype pre_processing_error: ~azure-batch.models.ExitOptions
        :keyword file_upload_error: If the Task exited with an exit code that was specified via
         exitCodes or exitCodeRanges, and then encountered a file upload error, then the action
         specified by the exit code takes precedence.
        :paramtype file_upload_error: ~azure-batch.models.ExitOptions
        :keyword default: This value is used if the Task exits with any nonzero exit code not listed in
         the exitCodes or exitCodeRanges collection, with a pre-processing error if the
         preProcessingError property is not present, or with a file upload error if the fileUploadError
         property is not present. If you want non-default behavior on exit code 0, you must list it
         explicitly using the exitCodes or exitCodeRanges collection.
        :paramtype default: ~azure-batch.models.ExitOptions
        """
        super(ExitConditions, self).__init__(**kwargs)
        self.exit_codes = exit_codes
        self.exit_code_ranges = exit_code_ranges
        self.pre_processing_error = pre_processing_error
        self.file_upload_error = file_upload_error
        self.default = default


class ExitOptions(msrest.serialization.Model):
    """Specifies how the Batch service responds to a particular exit condition.

    :ivar job_action: The default is none for exit code 0 and terminate for all other exit
     conditions. If the Job's onTaskFailed property is noaction, then specifying this property
     returns an error and the add Task request fails with an invalid property value error; if you
     are calling the REST API directly, the HTTP status code is 400 (Bad Request). Known values are:
     "none", "disable", "terminate".
    :vartype job_action: str or ~azure-batch.models.JobAction
    :ivar dependency_action: Possible values are 'satisfy' (allowing dependent tasks to progress)
     and 'block' (dependent tasks continue to wait). Batch does not yet support cancellation of
     dependent tasks. Known values are: "satisfy", "block".
    :vartype dependency_action: str or ~azure-batch.models.DependencyAction
    """

    _attribute_map = {
        'job_action': {'key': 'jobAction', 'type': 'str'},
        'dependency_action': {'key': 'dependencyAction', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        job_action: Optional[Union[str, "_models.JobAction"]] = None,
        dependency_action: Optional[Union[str, "_models.DependencyAction"]] = None,
        **kwargs
    ):
        """
        :keyword job_action: The default is none for exit code 0 and terminate for all other exit
         conditions. If the Job's onTaskFailed property is noaction, then specifying this property
         returns an error and the add Task request fails with an invalid property value error; if you
         are calling the REST API directly, the HTTP status code is 400 (Bad Request). Known values are:
         "none", "disable", "terminate".
        :paramtype job_action: str or ~azure-batch.models.JobAction
        :keyword dependency_action: Possible values are 'satisfy' (allowing dependent tasks to
         progress) and 'block' (dependent tasks continue to wait). Batch does not yet support
         cancellation of dependent tasks. Known values are: "satisfy", "block".
        :paramtype dependency_action: str or ~azure-batch.models.DependencyAction
        """
        super(ExitOptions, self).__init__(**kwargs)
        self.job_action = job_action
        self.dependency_action = dependency_action


class FileProperties(msrest.serialization.Model):
    """The properties of a file on a Compute Node.

    All required parameters must be populated in order to send to Azure.

    :ivar creation_time: The creation time is not returned for files on Linux Compute Nodes.
    :vartype creation_time: ~datetime.datetime
    :ivar last_modified: Required. The time at which the file was last modified.
    :vartype last_modified: ~datetime.datetime
    :ivar content_length: Required. The length of the file.
    :vartype content_length: long
    :ivar content_type: The content type of the file.
    :vartype content_type: str
    :ivar file_mode: The file mode is returned only for files on Linux Compute Nodes.
    :vartype file_mode: str
    """

    _validation = {
        'last_modified': {'required': True},
        'content_length': {'required': True},
    }

    _attribute_map = {
        'creation_time': {'key': 'creationTime', 'type': 'iso-8601'},
        'last_modified': {'key': 'lastModified', 'type': 'iso-8601'},
        'content_length': {'key': 'contentLength', 'type': 'long'},
        'content_type': {'key': 'contentType', 'type': 'str'},
        'file_mode': {'key': 'fileMode', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        last_modified: datetime.datetime,
        content_length: int,
        creation_time: Optional[datetime.datetime] = None,
        content_type: Optional[str] = None,
        file_mode: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword creation_time: The creation time is not returned for files on Linux Compute Nodes.
        :paramtype creation_time: ~datetime.datetime
        :keyword last_modified: Required. The time at which the file was last modified.
        :paramtype last_modified: ~datetime.datetime
        :keyword content_length: Required. The length of the file.
        :paramtype content_length: long
        :keyword content_type: The content type of the file.
        :paramtype content_type: str
        :keyword file_mode: The file mode is returned only for files on Linux Compute Nodes.
        :paramtype file_mode: str
        """
        super(FileProperties, self).__init__(**kwargs)
        self.creation_time = creation_time
        self.last_modified = last_modified
        self.content_length = content_length
        self.content_type = content_type
        self.file_mode = file_mode


class HttpHeader(msrest.serialization.Model):
    """An HTTP header name-value pair.

    All required parameters must be populated in order to send to Azure.

    :ivar name: Required. The case-insensitive name of the header to be used while uploading output
     files.
    :vartype name: str
    :ivar value: The value of the header to be used while uploading output files.
    :vartype value: str
    """

    _validation = {
        'name': {'required': True},
    }

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'value': {'key': 'value', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        value: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword name: Required. The case-insensitive name of the header to be used while uploading
         output files.
        :paramtype name: str
        :keyword value: The value of the header to be used while uploading output files.
        :paramtype value: str
        """
        super(HttpHeader, self).__init__(**kwargs)
        self.name = name
        self.value = value


class ImageInformation(msrest.serialization.Model):
    """A reference to the Azure Virtual Machines Marketplace Image and additional information about the Image.

    All required parameters must be populated in order to send to Azure.

    :ivar node_agent_sku_id: Required. The ID of the Compute Node agent SKU which the Image
     supports.
    :vartype node_agent_sku_id: str
    :ivar image_reference: Required. A reference to an Azure Virtual Machines Marketplace Image or
     a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references
     verified by Azure Batch, see the 'List Supported Images' operation.
    :vartype image_reference: ~azure-batch.models.ImageReference
    :ivar os_type: Required. The type of operating system (e.g. Windows or Linux) of the Image.
     Known values are: "linux", "windows".
    :vartype os_type: str or ~azure-batch.models.OSType
    :ivar capabilities: Not every capability of the Image is listed. Capabilities in this list are
     considered of special interest and are generally related to integration with other features in
     the Azure Batch service.
    :vartype capabilities: list[str]
    :ivar batch_support_end_of_life: The time when the Azure Batch service will stop accepting
     create Pool requests for the Image.
    :vartype batch_support_end_of_life: ~datetime.datetime
    :ivar verification_type: Required. Whether the Azure Batch service actively verifies that the
     Image is compatible with the associated Compute Node agent SKU. Known values are: "verified",
     "unverified".
    :vartype verification_type: str or ~azure-batch.models.VerificationType
    """

    _validation = {
        'node_agent_sku_id': {'required': True},
        'image_reference': {'required': True},
        'os_type': {'required': True},
        'verification_type': {'required': True},
    }

    _attribute_map = {
        'node_agent_sku_id': {'key': 'nodeAgentSKUId', 'type': 'str'},
        'image_reference': {'key': 'imageReference', 'type': 'ImageReference'},
        'os_type': {'key': 'osType', 'type': 'str'},
        'capabilities': {'key': 'capabilities', 'type': '[str]'},
        'batch_support_end_of_life': {'key': 'batchSupportEndOfLife', 'type': 'iso-8601'},
        'verification_type': {'key': 'verificationType', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        node_agent_sku_id: str,
        image_reference: "_models.ImageReference",
        os_type: Union[str, "_models.OSType"],
        verification_type: Union[str, "_models.VerificationType"],
        capabilities: Optional[List[str]] = None,
        batch_support_end_of_life: Optional[datetime.datetime] = None,
        **kwargs
    ):
        """
        :keyword node_agent_sku_id: Required. The ID of the Compute Node agent SKU which the Image
         supports.
        :paramtype node_agent_sku_id: str
        :keyword image_reference: Required. A reference to an Azure Virtual Machines Marketplace Image
         or a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references
         verified by Azure Batch, see the 'List Supported Images' operation.
        :paramtype image_reference: ~azure-batch.models.ImageReference
        :keyword os_type: Required. The type of operating system (e.g. Windows or Linux) of the Image.
         Known values are: "linux", "windows".
        :paramtype os_type: str or ~azure-batch.models.OSType
        :keyword capabilities: Not every capability of the Image is listed. Capabilities in this list
         are considered of special interest and are generally related to integration with other features
         in the Azure Batch service.
        :paramtype capabilities: list[str]
        :keyword batch_support_end_of_life: The time when the Azure Batch service will stop accepting
         create Pool requests for the Image.
        :paramtype batch_support_end_of_life: ~datetime.datetime
        :keyword verification_type: Required. Whether the Azure Batch service actively verifies that
         the Image is compatible with the associated Compute Node agent SKU. Known values are:
         "verified", "unverified".
        :paramtype verification_type: str or ~azure-batch.models.VerificationType
        """
        super(ImageInformation, self).__init__(**kwargs)
        self.node_agent_sku_id = node_agent_sku_id
        self.image_reference = image_reference
        self.os_type = os_type
        self.capabilities = capabilities
        self.batch_support_end_of_life = batch_support_end_of_life
        self.verification_type = verification_type


class ImageReference(msrest.serialization.Model):
    """A reference to an Azure Virtual Machines Marketplace Image or a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references verified by Azure Batch, see the 'List Supported Images' operation.

    Variables are only populated by the server, and will be ignored when sending a request.

    :ivar publisher: For example, Canonical or MicrosoftWindowsServer.
    :vartype publisher: str
    :ivar offer: For example, UbuntuServer or WindowsServer.
    :vartype offer: str
    :ivar sku: For example, 18.04-LTS or 2019-Datacenter.
    :vartype sku: str
    :ivar version: A value of 'latest' can be specified to select the latest version of an Image.
     If omitted, the default is 'latest'.
    :vartype version: str
    :ivar virtual_machine_image_id: This property is mutually exclusive with other ImageReference
     properties. The Shared Image Gallery Image must have replicas in the same region and must be in
     the same subscription as the Azure Batch account. If the image version is not specified in the
     imageId, the latest version will be used. For information about the firewall settings for the
     Batch Compute Node agent to communicate with the Batch service see
     https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
    :vartype virtual_machine_image_id: str
    :ivar exact_version: The specific version of the platform image or marketplace image used to
     create the node. This read-only field differs from 'version' only if the value specified for
     'version' when the pool was created was 'latest'.
    :vartype exact_version: str
    """

    _validation = {
        'exact_version': {'readonly': True},
    }

    _attribute_map = {
        'publisher': {'key': 'publisher', 'type': 'str'},
        'offer': {'key': 'offer', 'type': 'str'},
        'sku': {'key': 'sku', 'type': 'str'},
        'version': {'key': 'version', 'type': 'str'},
        'virtual_machine_image_id': {'key': 'virtualMachineImageId', 'type': 'str'},
        'exact_version': {'key': 'exactVersion', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        publisher: Optional[str] = None,
        offer: Optional[str] = None,
        sku: Optional[str] = None,
        version: Optional[str] = None,
        virtual_machine_image_id: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword publisher: For example, Canonical or MicrosoftWindowsServer.
        :paramtype publisher: str
        :keyword offer: For example, UbuntuServer or WindowsServer.
        :paramtype offer: str
        :keyword sku: For example, 18.04-LTS or 2019-Datacenter.
        :paramtype sku: str
        :keyword version: A value of 'latest' can be specified to select the latest version of an
         Image. If omitted, the default is 'latest'.
        :paramtype version: str
        :keyword virtual_machine_image_id: This property is mutually exclusive with other
         ImageReference properties. The Shared Image Gallery Image must have replicas in the same region
         and must be in the same subscription as the Azure Batch account. If the image version is not
         specified in the imageId, the latest version will be used. For information about the firewall
         settings for the Batch Compute Node agent to communicate with the Batch service see
         https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
        :paramtype virtual_machine_image_id: str
        """
        super(ImageReference, self).__init__(**kwargs)
        self.publisher = publisher
        self.offer = offer
        self.sku = sku
        self.version = version
        self.virtual_machine_image_id = virtual_machine_image_id
        self.exact_version = None


class InboundEndpoint(msrest.serialization.Model):
    """An inbound endpoint on a Compute Node.

    All required parameters must be populated in order to send to Azure.

    :ivar name: Required. The name of the endpoint.
    :vartype name: str
    :ivar protocol: Required. The protocol of the endpoint. Known values are: "tcp", "udp".
    :vartype protocol: str or ~azure-batch.models.InboundEndpointProtocol
    :ivar public_ip_address: Required. The public IP address of the Compute Node.
    :vartype public_ip_address: str
    :ivar public_fqdn: Required. The public fully qualified domain name for the Compute Node.
    :vartype public_fqdn: str
    :ivar frontend_port: Required. The public port number of the endpoint.
    :vartype frontend_port: int
    :ivar backend_port: Required. The backend port number of the endpoint.
    :vartype backend_port: int
    """

    _validation = {
        'name': {'required': True},
        'protocol': {'required': True},
        'public_ip_address': {'required': True},
        'public_fqdn': {'required': True},
        'frontend_port': {'required': True},
        'backend_port': {'required': True},
    }

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'protocol': {'key': 'protocol', 'type': 'str'},
        'public_ip_address': {'key': 'publicIPAddress', 'type': 'str'},
        'public_fqdn': {'key': 'publicFQDN', 'type': 'str'},
        'frontend_port': {'key': 'frontendPort', 'type': 'int'},
        'backend_port': {'key': 'backendPort', 'type': 'int'},
    }

    def __init__(
        self,
        *,
        name: str,
        protocol: Union[str, "_models.InboundEndpointProtocol"],
        public_ip_address: str,
        public_fqdn: str,
        frontend_port: int,
        backend_port: int,
        **kwargs
    ):
        """
        :keyword name: Required. The name of the endpoint.
        :paramtype name: str
        :keyword protocol: Required. The protocol of the endpoint. Known values are: "tcp", "udp".
        :paramtype protocol: str or ~azure-batch.models.InboundEndpointProtocol
        :keyword public_ip_address: Required. The public IP address of the Compute Node.
        :paramtype public_ip_address: str
        :keyword public_fqdn: Required. The public fully qualified domain name for the Compute Node.
        :paramtype public_fqdn: str
        :keyword frontend_port: Required. The public port number of the endpoint.
        :paramtype frontend_port: int
        :keyword backend_port: Required. The backend port number of the endpoint.
        :paramtype backend_port: int
        """
        super(InboundEndpoint, self).__init__(**kwargs)
        self.name = name
        self.protocol = protocol
        self.public_ip_address = public_ip_address
        self.public_fqdn = public_fqdn
        self.frontend_port = frontend_port
        self.backend_port = backend_port


class InboundNATPool(msrest.serialization.Model):
    """A inbound NAT Pool that can be used to address specific ports on Compute Nodes in a Batch Pool externally.

    All required parameters must be populated in order to send to Azure.

    :ivar name: Required. The name must be unique within a Batch Pool, can contain letters,
     numbers, underscores, periods, and hyphens. Names must start with a letter or number, must end
     with a letter, number, or underscore, and cannot exceed 77 characters.  If any invalid values
     are provided the request fails with HTTP status code 400.
    :vartype name: str
    :ivar protocol: Required. The protocol of the endpoint. Known values are: "tcp", "udp".
    :vartype protocol: str or ~azure-batch.models.InboundEndpointProtocol
    :ivar backend_port: Required. This must be unique within a Batch Pool. Acceptable values are
     between 1 and 65535 except for 22, 3389, 29876 and 29877 as these are reserved. If any reserved
     values are provided the request fails with HTTP status code 400.
    :vartype backend_port: int
    :ivar frontend_port_range_start: Required. Acceptable values range between 1 and 65534 except
     ports from 50000 to 55000 which are reserved. All ranges within a Pool must be distinct and
     cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping
     values are provided the request fails with HTTP status code 400.
    :vartype frontend_port_range_start: int
    :ivar frontend_port_range_end: Required. Acceptable values range between 1 and 65534 except
     ports from 50000 to 55000 which are reserved by the Batch service. All ranges within a Pool
     must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved
     or overlapping values are provided the request fails with HTTP status code 400.
    :vartype frontend_port_range_end: int
    :ivar network_security_group_rules: The maximum number of rules that can be specified across
     all the endpoints on a Batch Pool is 25. If no network security group rules are specified, a
     default rule will be created to allow inbound access to the specified backendPort. If the
     maximum number of network security group rules is exceeded the request fails with HTTP status
     code 400.
    :vartype network_security_group_rules: list[~azure-batch.models.NetworkSecurityGroupRule]
    """

    _validation = {
        'name': {'required': True},
        'protocol': {'required': True},
        'backend_port': {'required': True},
        'frontend_port_range_start': {'required': True},
        'frontend_port_range_end': {'required': True},
    }

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'protocol': {'key': 'protocol', 'type': 'str'},
        'backend_port': {'key': 'backendPort', 'type': 'int'},
        'frontend_port_range_start': {'key': 'frontendPortRangeStart', 'type': 'int'},
        'frontend_port_range_end': {'key': 'frontendPortRangeEnd', 'type': 'int'},
        'network_security_group_rules': {'key': 'networkSecurityGroupRules', 'type': '[NetworkSecurityGroupRule]'},
    }

    def __init__(
        self,
        *,
        name: str,
        protocol: Union[str, "_models.InboundEndpointProtocol"],
        backend_port: int,
        frontend_port_range_start: int,
        frontend_port_range_end: int,
        network_security_group_rules: Optional[List["_models.NetworkSecurityGroupRule"]] = None,
        **kwargs
    ):
        """
        :keyword name: Required. The name must be unique within a Batch Pool, can contain letters,
         numbers, underscores, periods, and hyphens. Names must start with a letter or number, must end
         with a letter, number, or underscore, and cannot exceed 77 characters.  If any invalid values
         are provided the request fails with HTTP status code 400.
        :paramtype name: str
        :keyword protocol: Required. The protocol of the endpoint. Known values are: "tcp", "udp".
        :paramtype protocol: str or ~azure-batch.models.InboundEndpointProtocol
        :keyword backend_port: Required. This must be unique within a Batch Pool. Acceptable values are
         between 1 and 65535 except for 22, 3389, 29876 and 29877 as these are reserved. If any reserved
         values are provided the request fails with HTTP status code 400.
        :paramtype backend_port: int
        :keyword frontend_port_range_start: Required. Acceptable values range between 1 and 65534
         except ports from 50000 to 55000 which are reserved. All ranges within a Pool must be distinct
         and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping
         values are provided the request fails with HTTP status code 400.
        :paramtype frontend_port_range_start: int
        :keyword frontend_port_range_end: Required. Acceptable values range between 1 and 65534 except
         ports from 50000 to 55000 which are reserved by the Batch service. All ranges within a Pool
         must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved
         or overlapping values are provided the request fails with HTTP status code 400.
        :paramtype frontend_port_range_end: int
        :keyword network_security_group_rules: The maximum number of rules that can be specified across
         all the endpoints on a Batch Pool is 25. If no network security group rules are specified, a
         default rule will be created to allow inbound access to the specified backendPort. If the
         maximum number of network security group rules is exceeded the request fails with HTTP status
         code 400.
        :paramtype network_security_group_rules: list[~azure-batch.models.NetworkSecurityGroupRule]
        """
        super(InboundNATPool, self).__init__(**kwargs)
        self.name = name
        self.protocol = protocol
        self.backend_port = backend_port
        self.frontend_port_range_start = frontend_port_range_start
        self.frontend_port_range_end = frontend_port_range_end
        self.network_security_group_rules = network_security_group_rules


class InstanceViewStatus(msrest.serialization.Model):
    """The instance view status.

    :ivar code: The status code.
    :vartype code: str
    :ivar display_status: The localized label for the status.
    :vartype display_status: str
    :ivar level: Level code. Known values are: "Error", "Info", "Warning".
    :vartype level: str or ~azure-batch.models.StatusLevelTypes
    :ivar message: The detailed status message.
    :vartype message: str
    :ivar time: The time of the status.
    :vartype time: str
    """

    _attribute_map = {
        'code': {'key': 'code', 'type': 'str'},
        'display_status': {'key': 'displayStatus', 'type': 'str'},
        'level': {'key': 'level', 'type': 'str'},
        'message': {'key': 'message', 'type': 'str'},
        'time': {'key': 'time', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        code: Optional[str] = None,
        display_status: Optional[str] = None,
        level: Optional[Union[str, "_models.StatusLevelTypes"]] = None,
        message: Optional[str] = None,
        time: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword code: The status code.
        :paramtype code: str
        :keyword display_status: The localized label for the status.
        :paramtype display_status: str
        :keyword level: Level code. Known values are: "Error", "Info", "Warning".
        :paramtype level: str or ~azure-batch.models.StatusLevelTypes
        :keyword message: The detailed status message.
        :paramtype message: str
        :keyword time: The time of the status.
        :paramtype time: str
        """
        super(InstanceViewStatus, self).__init__(**kwargs)
        self.code = code
        self.display_status = display_status
        self.level = level
        self.message = message
        self.time = time


class JobConstraints(msrest.serialization.Model):
    """The execution constraints for a Job.

    :ivar max_wall_clock_time: If the Job does not complete within the time limit, the Batch
     service terminates it and any Tasks that are still running. In this case, the termination
     reason will be MaxWallClockTimeExpiry. If this property is not specified, there is no time
     limit on how long the Job may run.
    :vartype max_wall_clock_time: ~datetime.timedelta
    :ivar max_task_retry_count: Note that this value specifically controls the number of retries.
     The Batch service will try each Task once, and may then retry up to this limit. For example, if
     the maximum retry count is 3, Batch tries a Task up to 4 times (one initial try and 3 retries).
     If the maximum retry count is 0, the Batch service does not retry Tasks. If the maximum retry
     count is -1, the Batch service retries the Task without limit, however this is not recommended
     for a start task or any task. The default value is 0 (no retries).
    :vartype max_task_retry_count: int
    """

    _attribute_map = {
        'max_wall_clock_time': {'key': 'maxWallClockTime', 'type': 'duration'},
        'max_task_retry_count': {'key': 'maxTaskRetryCount', 'type': 'int'},
    }

    def __init__(
        self,
        *,
        max_wall_clock_time: Optional[datetime.timedelta] = None,
        max_task_retry_count: Optional[int] = None,
        **kwargs
    ):
        """
        :keyword max_wall_clock_time: If the Job does not complete within the time limit, the Batch
         service terminates it and any Tasks that are still running. In this case, the termination
         reason will be MaxWallClockTimeExpiry. If this property is not specified, there is no time
         limit on how long the Job may run.
        :paramtype max_wall_clock_time: ~datetime.timedelta
        :keyword max_task_retry_count: Note that this value specifically controls the number of
         retries. The Batch service will try each Task once, and may then retry up to this limit. For
         example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one initial try and
         3 retries). If the maximum retry count is 0, the Batch service does not retry Tasks. If the
         maximum retry count is -1, the Batch service retries the Task without limit, however this is
         not recommended for a start task or any task. The default value is 0 (no retries).
        :paramtype max_task_retry_count: int
        """
        super(JobConstraints, self).__init__(**kwargs)
        self.max_wall_clock_time = max_wall_clock_time
        self.max_task_retry_count = max_task_retry_count


class JobExecutionInformation(msrest.serialization.Model):
    """Contains information about the execution of a Job in the Azure Batch service.

    All required parameters must be populated in order to send to Azure.

    :ivar start_time: Required. This is the time at which the Job was created.
    :vartype start_time: ~datetime.datetime
    :ivar end_time: This property is set only if the Job is in the completed state.
    :vartype end_time: ~datetime.datetime
    :ivar pool_id: This element contains the actual Pool where the Job is assigned. When you get
     Job details from the service, they also contain a poolInfo element, which contains the Pool
     configuration data from when the Job was added or updated. That poolInfo element may also
     contain a poolId element. If it does, the two IDs are the same. If it does not, it means the
     Job ran on an auto Pool, and this property contains the ID of that auto Pool.
    :vartype pool_id: str
    :ivar scheduling_error: This property is not set if there was no error starting the Job.
    :vartype scheduling_error: ~azure-batch.models.JobSchedulingError
    :ivar terminate_reason: This property is set only if the Job is in the completed state. If the
     Batch service terminates the Job, it sets the reason as follows: JMComplete - the Job Manager
     Task completed, and killJobOnCompletion was set to true. MaxWallClockTimeExpiry - the Job
     reached its maxWallClockTime constraint. TerminateJobSchedule - the Job ran as part of a
     schedule, and the schedule terminated. AllTasksComplete - the Job's onAllTasksComplete
     attribute is set to terminatejob, and all Tasks in the Job are complete. TaskFailed - the Job's
     onTaskFailure attribute is set to performExitOptionsJobAction, and a Task in the Job failed
     with an exit condition that specified a jobAction of terminatejob. Any other string is a
     user-defined reason specified in a call to the 'Terminate a Job' operation.
    :vartype terminate_reason: str
    """

    _validation = {
        'start_time': {'required': True},
    }

    _attribute_map = {
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'end_time': {'key': 'endTime', 'type': 'iso-8601'},
        'pool_id': {'key': 'poolId', 'type': 'str'},
        'scheduling_error': {'key': 'schedulingError', 'type': 'JobSchedulingError'},
        'terminate_reason': {'key': 'terminateReason', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        start_time: datetime.datetime,
        end_time: Optional[datetime.datetime] = None,
        pool_id: Optional[str] = None,
        scheduling_error: Optional["_models.JobSchedulingError"] = None,
        terminate_reason: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword start_time: Required. This is the time at which the Job was created.
        :paramtype start_time: ~datetime.datetime
        :keyword end_time: This property is set only if the Job is in the completed state.
        :paramtype end_time: ~datetime.datetime
        :keyword pool_id: This element contains the actual Pool where the Job is assigned. When you get
         Job details from the service, they also contain a poolInfo element, which contains the Pool
         configuration data from when the Job was added or updated. That poolInfo element may also
         contain a poolId element. If it does, the two IDs are the same. If it does not, it means the
         Job ran on an auto Pool, and this property contains the ID of that auto Pool.
        :paramtype pool_id: str
        :keyword scheduling_error: This property is not set if there was no error starting the Job.
        :paramtype scheduling_error: ~azure-batch.models.JobSchedulingError
        :keyword terminate_reason: This property is set only if the Job is in the completed state. If
         the Batch service terminates the Job, it sets the reason as follows: JMComplete - the Job
         Manager Task completed, and killJobOnCompletion was set to true. MaxWallClockTimeExpiry - the
         Job reached its maxWallClockTime constraint. TerminateJobSchedule - the Job ran as part of a
         schedule, and the schedule terminated. AllTasksComplete - the Job's onAllTasksComplete
         attribute is set to terminatejob, and all Tasks in the Job are complete. TaskFailed - the Job's
         onTaskFailure attribute is set to performExitOptionsJobAction, and a Task in the Job failed
         with an exit condition that specified a jobAction of terminatejob. Any other string is a
         user-defined reason specified in a call to the 'Terminate a Job' operation.
        :paramtype terminate_reason: str
        """
        super(JobExecutionInformation, self).__init__(**kwargs)
        self.start_time = start_time
        self.end_time = end_time
        self.pool_id = pool_id
        self.scheduling_error = scheduling_error
        self.terminate_reason = terminate_reason


class JobManagerTask(msrest.serialization.Model):
    """The Job Manager Task is automatically started when the Job is created. The Batch service tries to schedule the Job Manager Task before any other Tasks in the Job. When shrinking a Pool, the Batch service tries to preserve Nodes where Job Manager Tasks are running for as long as possible (that is, Compute Nodes running 'normal' Tasks are removed before Compute Nodes running Job Manager Tasks). When a Job Manager Task fails and needs to be restarted, the system tries to schedule it at the highest priority. If there are no idle Compute Nodes available, the system may terminate one of the running Tasks in the Pool and return it to the queue in order to make room for the Job Manager Task to restart. Note that a Job Manager Task in one Job does not have priority over Tasks in other Jobs. Across Jobs, only Job level priorities are observed. For example, if a Job Manager in a priority 0 Job needs to be restarted, it will not displace Tasks of a priority 1 Job. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing.

    All required parameters must be populated in order to send to Azure.

    :ivar id: Required. The ID can contain any combination of alphanumeric characters including
     hyphens and underscores and cannot contain more than 64 characters.
    :vartype id: str
    :ivar display_name: It need not be unique and can contain any Unicode characters up to a
     maximum length of 1024.
    :vartype display_name: str
    :ivar command_line: Required. The command line does not run under a shell, and therefore cannot
     take advantage of shell features such as environment variable expansion. If you want to take
     advantage of such features, you should invoke the shell in the command line, for example using
     "cmd /c MyCommand" in Windows or "/bin/sh -c MyCommand" in Linux. If the command line refers to
     file paths, it should use a relative path (relative to the Task working directory), or use the
     Batch provided environment variable
     (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
    :vartype command_line: str
    :ivar container_settings: If the Pool that will run this Task has containerConfiguration set,
     this must be set as well. If the Pool that will run this Task doesn't have
     containerConfiguration set, this must not be set. When this is specified, all directories
     recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node)
     are mapped into the container, all Task environment variables are mapped into the container,
     and the Task command line is executed in the container. Files produced in the container outside
     of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs
     will not be able to access those files.
    :vartype container_settings: ~azure-batch.models.TaskContainerSettings
    :ivar resource_files: Files listed under this element are located in the Task's working
     directory. There is a maximum size for the list of resource files.  When the max size is
     exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If
     this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved
     using .zip files, Application Packages, or Docker Containers.
    :vartype resource_files: list[~azure-batch.models.ResourceFile]
    :ivar output_files: For multi-instance Tasks, the files will only be uploaded from the Compute
     Node on which the primary Task is executed.
    :vartype output_files: list[~azure-batch.models.OutputFile]
    :ivar environment_settings: A list of environment variable settings for the Job Manager Task.
    :vartype environment_settings: list[~azure-batch.models.EnvironmentSetting]
    :ivar constraints: Execution constraints to apply to a Task.
    :vartype constraints: ~azure-batch.models.TaskConstraints
    :ivar required_slots: The default is 1. A Task can only be scheduled to run on a compute node
     if the node has enough free scheduling slots available. For multi-instance Tasks, this property
     is not supported and must not be specified.
    :vartype required_slots: int
    :ivar kill_job_on_completion: If true, when the Job Manager Task completes, the Batch service
     marks the Job as complete. If any Tasks are still running at this time (other than Job
     Release), those Tasks are terminated. If false, the completion of the Job Manager Task does not
     affect the Job status. In this case, you should either use the onAllTasksComplete attribute to
     terminate the Job, or have a client or user terminate the Job explicitly. An example of this is
     if the Job Manager creates a set of Tasks but then takes no further role in their execution.
     The default value is true. If you are using the onAllTasksComplete and onTaskFailure attributes
     to control Job lifetime, and using the Job Manager Task only to create the Tasks for the Job
     (not to monitor progress), then it is important to set killJobOnCompletion to false.
    :vartype kill_job_on_completion: bool
    :ivar user_identity: If omitted, the Task runs as a non-administrative user unique to the Task.
    :vartype user_identity: ~azure-batch.models.UserIdentity
    :ivar run_exclusive: If true, no other Tasks will run on the same Node for as long as the Job
     Manager is running. If false, other Tasks can run simultaneously with the Job Manager on a
     Compute Node. The Job Manager Task counts normally against the Compute Node's concurrent Task
     limit, so this is only relevant if the Compute Node allows multiple concurrent Tasks. The
     default value is true.
    :vartype run_exclusive: bool
    :ivar application_package_references: Application Packages are downloaded and deployed to a
     shared directory, not the Task working directory. Therefore, if a referenced Application
     Package is already on the Compute Node, and is up to date, then it is not re-downloaded; the
     existing copy on the Compute Node is used. If a referenced Application Package cannot be
     installed, for example because the package has been deleted or because download failed, the
     Task fails.
    :vartype application_package_references: list[~azure-batch.models.ApplicationPackageReference]
    :ivar authentication_token_settings: If this property is set, the Batch service provides the
     Task with an authentication token which can be used to authenticate Batch service operations
     without requiring an Account access key. The token is provided via the
     AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the Task can carry out
     using the token depend on the settings. For example, a Task can request Job permissions in
     order to add other Tasks to the Job, or check the status of the Job or of other Tasks under the
     Job.
    :vartype authentication_token_settings: ~azure-batch.models.AuthenticationTokenSettings
    :ivar allow_low_priority_node: The default value is true.
    :vartype allow_low_priority_node: bool
    """

    _validation = {
        'id': {'required': True},
        'command_line': {'required': True},
    }

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'display_name': {'key': 'displayName', 'type': 'str'},
        'command_line': {'key': 'commandLine', 'type': 'str'},
        'container_settings': {'key': 'containerSettings', 'type': 'TaskContainerSettings'},
        'resource_files': {'key': 'resourceFiles', 'type': '[ResourceFile]'},
        'output_files': {'key': 'outputFiles', 'type': '[OutputFile]'},
        'environment_settings': {'key': 'environmentSettings', 'type': '[EnvironmentSetting]'},
        'constraints': {'key': 'constraints', 'type': 'TaskConstraints'},
        'required_slots': {'key': 'requiredSlots', 'type': 'int'},
        'kill_job_on_completion': {'key': 'killJobOnCompletion', 'type': 'bool'},
        'user_identity': {'key': 'userIdentity', 'type': 'UserIdentity'},
        'run_exclusive': {'key': 'runExclusive', 'type': 'bool'},
        'application_package_references': {'key': 'applicationPackageReferences', 'type': '[ApplicationPackageReference]'},
        'authentication_token_settings': {'key': 'authenticationTokenSettings', 'type': 'AuthenticationTokenSettings'},
        'allow_low_priority_node': {'key': 'allowLowPriorityNode', 'type': 'bool'},
    }

    def __init__(
        self,
        *,
        id: str,
        command_line: str,
        display_name: Optional[str] = None,
        container_settings: Optional["_models.TaskContainerSettings"] = None,
        resource_files: Optional[List["_models.ResourceFile"]] = None,
        output_files: Optional[List["_models.OutputFile"]] = None,
        environment_settings: Optional[List["_models.EnvironmentSetting"]] = None,
        constraints: Optional["_models.TaskConstraints"] = None,
        required_slots: Optional[int] = None,
        kill_job_on_completion: Optional[bool] = None,
        user_identity: Optional["_models.UserIdentity"] = None,
        run_exclusive: Optional[bool] = None,
        application_package_references: Optional[List["_models.ApplicationPackageReference"]] = None,
        authentication_token_settings: Optional["_models.AuthenticationTokenSettings"] = None,
        allow_low_priority_node: Optional[bool] = None,
        **kwargs
    ):
        """
        :keyword id: Required. The ID can contain any combination of alphanumeric characters including
         hyphens and underscores and cannot contain more than 64 characters.
        :paramtype id: str
        :keyword display_name: It need not be unique and can contain any Unicode characters up to a
         maximum length of 1024.
        :paramtype display_name: str
        :keyword command_line: Required. The command line does not run under a shell, and therefore
         cannot take advantage of shell features such as environment variable expansion. If you want to
         take advantage of such features, you should invoke the shell in the command line, for example
         using "cmd /c MyCommand" in Windows or "/bin/sh -c MyCommand" in Linux. If the command line
         refers to file paths, it should use a relative path (relative to the Task working directory),
         or use the Batch provided environment variable
         (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        :paramtype command_line: str
        :keyword container_settings: If the Pool that will run this Task has containerConfiguration
         set, this must be set as well. If the Pool that will run this Task doesn't have
         containerConfiguration set, this must not be set. When this is specified, all directories
         recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node)
         are mapped into the container, all Task environment variables are mapped into the container,
         and the Task command line is executed in the container. Files produced in the container outside
         of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs
         will not be able to access those files.
        :paramtype container_settings: ~azure-batch.models.TaskContainerSettings
        :keyword resource_files: Files listed under this element are located in the Task's working
         directory. There is a maximum size for the list of resource files.  When the max size is
         exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If
         this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved
         using .zip files, Application Packages, or Docker Containers.
        :paramtype resource_files: list[~azure-batch.models.ResourceFile]
        :keyword output_files: For multi-instance Tasks, the files will only be uploaded from the
         Compute Node on which the primary Task is executed.
        :paramtype output_files: list[~azure-batch.models.OutputFile]
        :keyword environment_settings: A list of environment variable settings for the Job Manager
         Task.
        :paramtype environment_settings: list[~azure-batch.models.EnvironmentSetting]
        :keyword constraints: Execution constraints to apply to a Task.
        :paramtype constraints: ~azure-batch.models.TaskConstraints
        :keyword required_slots: The default is 1. A Task can only be scheduled to run on a compute
         node if the node has enough free scheduling slots available. For multi-instance Tasks, this
         property is not supported and must not be specified.
        :paramtype required_slots: int
        :keyword kill_job_on_completion: If true, when the Job Manager Task completes, the Batch
         service marks the Job as complete. If any Tasks are still running at this time (other than Job
         Release), those Tasks are terminated. If false, the completion of the Job Manager Task does not
         affect the Job status. In this case, you should either use the onAllTasksComplete attribute to
         terminate the Job, or have a client or user terminate the Job explicitly. An example of this is
         if the Job Manager creates a set of Tasks but then takes no further role in their execution.
         The default value is true. If you are using the onAllTasksComplete and onTaskFailure attributes
         to control Job lifetime, and using the Job Manager Task only to create the Tasks for the Job
         (not to monitor progress), then it is important to set killJobOnCompletion to false.
        :paramtype kill_job_on_completion: bool
        :keyword user_identity: If omitted, the Task runs as a non-administrative user unique to the
         Task.
        :paramtype user_identity: ~azure-batch.models.UserIdentity
        :keyword run_exclusive: If true, no other Tasks will run on the same Node for as long as the
         Job Manager is running. If false, other Tasks can run simultaneously with the Job Manager on a
         Compute Node. The Job Manager Task counts normally against the Compute Node's concurrent Task
         limit, so this is only relevant if the Compute Node allows multiple concurrent Tasks. The
         default value is true.
        :paramtype run_exclusive: bool
        :keyword application_package_references: Application Packages are downloaded and deployed to a
         shared directory, not the Task working directory. Therefore, if a referenced Application
         Package is already on the Compute Node, and is up to date, then it is not re-downloaded; the
         existing copy on the Compute Node is used. If a referenced Application Package cannot be
         installed, for example because the package has been deleted or because download failed, the
         Task fails.
        :paramtype application_package_references:
         list[~azure-batch.models.ApplicationPackageReference]
        :keyword authentication_token_settings: If this property is set, the Batch service provides the
         Task with an authentication token which can be used to authenticate Batch service operations
         without requiring an Account access key. The token is provided via the
         AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the Task can carry out
         using the token depend on the settings. For example, a Task can request Job permissions in
         order to add other Tasks to the Job, or check the status of the Job or of other Tasks under the
         Job.
        :paramtype authentication_token_settings: ~azure-batch.models.AuthenticationTokenSettings
        :keyword allow_low_priority_node: The default value is true.
        :paramtype allow_low_priority_node: bool
        """
        super(JobManagerTask, self).__init__(**kwargs)
        self.id = id
        self.display_name = display_name
        self.command_line = command_line
        self.container_settings = container_settings
        self.resource_files = resource_files
        self.output_files = output_files
        self.environment_settings = environment_settings
        self.constraints = constraints
        self.required_slots = required_slots
        self.kill_job_on_completion = kill_job_on_completion
        self.user_identity = user_identity
        self.run_exclusive = run_exclusive
        self.application_package_references = application_package_references
        self.authentication_token_settings = authentication_token_settings
        self.allow_low_priority_node = allow_low_priority_node


class JobNetworkConfiguration(msrest.serialization.Model):
    """The network configuration for the Job.

    All required parameters must be populated in order to send to Azure.

    :ivar subnet_id: Required. The virtual network must be in the same region and subscription as
     the Azure Batch Account. The specified subnet should have enough free IP addresses to
     accommodate the number of Compute Nodes which will run Tasks from the Job. This can be up to
     the number of Compute Nodes in the Pool. The 'MicrosoftAzureBatch' service principal must have
     the 'Classic Virtual Machine Contributor' Role-Based Access Control (RBAC) role for the
     specified VNet so that Azure Batch service can schedule Tasks on the Nodes. This can be
     verified by checking if the specified VNet has any associated Network Security Groups (NSG). If
     communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service
     will set the state of the Compute Nodes to unusable. This is of the form
     /subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}.
     If the specified VNet has any associated Network Security Groups (NSG), then a few reserved
     system ports must be enabled for inbound communication from the Azure Batch service. For Pools
     created with a Virtual Machine configuration, enable ports 29876 and 29877, as well as port 22
     for Linux and port 3389 for Windows. Port 443 is also required to be open for outbound
     connections for communications to Azure Storage. For more details see:
     https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
    :vartype subnet_id: str
    """

    _validation = {
        'subnet_id': {'required': True},
    }

    _attribute_map = {
        'subnet_id': {'key': 'subnetId', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        subnet_id: str,
        **kwargs
    ):
        """
        :keyword subnet_id: Required. The virtual network must be in the same region and subscription
         as the Azure Batch Account. The specified subnet should have enough free IP addresses to
         accommodate the number of Compute Nodes which will run Tasks from the Job. This can be up to
         the number of Compute Nodes in the Pool. The 'MicrosoftAzureBatch' service principal must have
         the 'Classic Virtual Machine Contributor' Role-Based Access Control (RBAC) role for the
         specified VNet so that Azure Batch service can schedule Tasks on the Nodes. This can be
         verified by checking if the specified VNet has any associated Network Security Groups (NSG). If
         communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service
         will set the state of the Compute Nodes to unusable. This is of the form
         /subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}.
         If the specified VNet has any associated Network Security Groups (NSG), then a few reserved
         system ports must be enabled for inbound communication from the Azure Batch service. For Pools
         created with a Virtual Machine configuration, enable ports 29876 and 29877, as well as port 22
         for Linux and port 3389 for Windows. Port 443 is also required to be open for outbound
         connections for communications to Azure Storage. For more details see:
         https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
        :paramtype subnet_id: str
        """
        super(JobNetworkConfiguration, self).__init__(**kwargs)
        self.subnet_id = subnet_id


class JobPreparationAndReleaseTaskExecutionInformation(msrest.serialization.Model):
    """The status of the Job Preparation and Job Release Tasks on a Compute Node.

    :ivar pool_id: The ID of the Pool containing the Compute Node to which this entry refers.
    :vartype pool_id: str
    :ivar node_id: The ID of the Compute Node to which this entry refers.
    :vartype node_id: str
    :ivar node_url: The URL of the Compute Node to which this entry refers.
    :vartype node_url: str
    :ivar job_preparation_task_execution_info: Contains information about the execution of a Job
     Preparation Task on a Compute Node.
    :vartype job_preparation_task_execution_info:
     ~azure-batch.models.JobPreparationTaskExecutionInformation
    :ivar job_release_task_execution_info: This property is set only if the Job Release Task has
     run on the Compute Node.
    :vartype job_release_task_execution_info:
     ~azure-batch.models.JobReleaseTaskExecutionInformation
    """

    _attribute_map = {
        'pool_id': {'key': 'poolId', 'type': 'str'},
        'node_id': {'key': 'nodeId', 'type': 'str'},
        'node_url': {'key': 'nodeUrl', 'type': 'str'},
        'job_preparation_task_execution_info': {'key': 'jobPreparationTaskExecutionInfo', 'type': 'JobPreparationTaskExecutionInformation'},
        'job_release_task_execution_info': {'key': 'jobReleaseTaskExecutionInfo', 'type': 'JobReleaseTaskExecutionInformation'},
    }

    def __init__(
        self,
        *,
        pool_id: Optional[str] = None,
        node_id: Optional[str] = None,
        node_url: Optional[str] = None,
        job_preparation_task_execution_info: Optional["_models.JobPreparationTaskExecutionInformation"] = None,
        job_release_task_execution_info: Optional["_models.JobReleaseTaskExecutionInformation"] = None,
        **kwargs
    ):
        """
        :keyword pool_id: The ID of the Pool containing the Compute Node to which this entry refers.
        :paramtype pool_id: str
        :keyword node_id: The ID of the Compute Node to which this entry refers.
        :paramtype node_id: str
        :keyword node_url: The URL of the Compute Node to which this entry refers.
        :paramtype node_url: str
        :keyword job_preparation_task_execution_info: Contains information about the execution of a Job
         Preparation Task on a Compute Node.
        :paramtype job_preparation_task_execution_info:
         ~azure-batch.models.JobPreparationTaskExecutionInformation
        :keyword job_release_task_execution_info: This property is set only if the Job Release Task has
         run on the Compute Node.
        :paramtype job_release_task_execution_info:
         ~azure-batch.models.JobReleaseTaskExecutionInformation
        """
        super(JobPreparationAndReleaseTaskExecutionInformation, self).__init__(**kwargs)
        self.pool_id = pool_id
        self.node_id = node_id
        self.node_url = node_url
        self.job_preparation_task_execution_info = job_preparation_task_execution_info
        self.job_release_task_execution_info = job_release_task_execution_info


class JobPreparationTask(msrest.serialization.Model):
    """You can use Job Preparation to prepare a Node to run Tasks for the Job. Activities commonly performed in Job Preparation include: Downloading common resource files used by all the Tasks in the Job. The Job Preparation Task can download these common resource files to the shared location on the Node. (AZ_BATCH_NODE_ROOT_DIR\shared), or starting a local service on the Node so that all Tasks of that Job can communicate with it. If the Job Preparation Task fails (that is, exhausts its retry count before exiting with exit code 0), Batch will not run Tasks of this Job on the Node. The Compute Node remains ineligible to run Tasks of this Job until it is reimaged. The Compute Node remains active and can be used for other Jobs. The Job Preparation Task can run multiple times on the same Node. Therefore, you should write the Job Preparation Task to handle re-execution. If the Node is rebooted, the Job Preparation Task is run again on the Compute Node before scheduling any other Task of the Job, if rerunOnNodeRebootAfterSuccess is true or if the Job Preparation Task did not previously complete. If the Node is reimaged, the Job Preparation Task is run again before scheduling any Task of the Job. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing.

    All required parameters must be populated in order to send to Azure.

    :ivar id: The ID can contain any combination of alphanumeric characters including hyphens and
     underscores and cannot contain more than 64 characters. If you do not specify this property,
     the Batch service assigns a default value of 'jobpreparation'. No other Task in the Job can
     have the same ID as the Job Preparation Task. If you try to submit a Task with the same id, the
     Batch service rejects the request with error code TaskIdSameAsJobPreparationTask; if you are
     calling the REST API directly, the HTTP status code is 409 (Conflict).
    :vartype id: str
    :ivar command_line: Required. The command line does not run under a shell, and therefore cannot
     take advantage of shell features such as environment variable expansion. If you want to take
     advantage of such features, you should invoke the shell in the command line, for example using
     "cmd /c MyCommand" in Windows or "/bin/sh -c MyCommand" in Linux. If the command line refers to
     file paths, it should use a relative path (relative to the Task working directory), or use the
     Batch provided environment variable
     (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
    :vartype command_line: str
    :ivar container_settings: When this is specified, all directories recursively below the
     AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the
     container, all Task environment variables are mapped into the container, and the Task command
     line is executed in the container. Files produced in the container outside of
     AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs
     will not be able to access those files.
    :vartype container_settings: ~azure-batch.models.TaskContainerSettings
    :ivar resource_files: Files listed under this element are located in the Task's working
     directory.  There is a maximum size for the list of resource files.  When the max size is
     exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If
     this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved
     using .zip files, Application Packages, or Docker Containers.
    :vartype resource_files: list[~azure-batch.models.ResourceFile]
    :ivar environment_settings: A list of environment variable settings for the Job Preparation
     Task.
    :vartype environment_settings: list[~azure-batch.models.EnvironmentSetting]
    :ivar constraints: Execution constraints to apply to a Task.
    :vartype constraints: ~azure-batch.models.TaskConstraints
    :ivar wait_for_success: If true and the Job Preparation Task fails on a Node, the Batch service
     retries the Job Preparation Task up to its maximum retry count (as specified in the constraints
     element). If the Task has still not completed successfully after all retries, then the Batch
     service will not schedule Tasks of the Job to the Node. The Node remains active and eligible to
     run Tasks of other Jobs. If false, the Batch service will not wait for the Job Preparation Task
     to complete. In this case, other Tasks of the Job can start executing on the Compute Node while
     the Job Preparation Task is still running; and even if the Job Preparation Task fails, new
     Tasks will continue to be scheduled on the Compute Node. The default value is true.
    :vartype wait_for_success: bool
    :ivar user_identity: If omitted, the Task runs as a non-administrative user unique to the Task
     on Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux Compute
     Nodes.
    :vartype user_identity: ~azure-batch.models.UserIdentity
    :ivar rerun_on_node_reboot_after_success: The Job Preparation Task is always rerun if a Compute
     Node is reimaged, or if the Job Preparation Task did not complete (e.g. because the reboot
     occurred while the Task was running). Therefore, you should always write a Job Preparation Task
     to be idempotent and to behave correctly if run multiple times. The default value is true.
    :vartype rerun_on_node_reboot_after_success: bool
    """

    _validation = {
        'command_line': {'required': True},
    }

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'command_line': {'key': 'commandLine', 'type': 'str'},
        'container_settings': {'key': 'containerSettings', 'type': 'TaskContainerSettings'},
        'resource_files': {'key': 'resourceFiles', 'type': '[ResourceFile]'},
        'environment_settings': {'key': 'environmentSettings', 'type': '[EnvironmentSetting]'},
        'constraints': {'key': 'constraints', 'type': 'TaskConstraints'},
        'wait_for_success': {'key': 'waitForSuccess', 'type': 'bool'},
        'user_identity': {'key': 'userIdentity', 'type': 'UserIdentity'},
        'rerun_on_node_reboot_after_success': {'key': 'rerunOnNodeRebootAfterSuccess', 'type': 'bool'},
    }

    def __init__(
        self,
        *,
        command_line: str,
        id: Optional[str] = None,
        container_settings: Optional["_models.TaskContainerSettings"] = None,
        resource_files: Optional[List["_models.ResourceFile"]] = None,
        environment_settings: Optional[List["_models.EnvironmentSetting"]] = None,
        constraints: Optional["_models.TaskConstraints"] = None,
        wait_for_success: Optional[bool] = None,
        user_identity: Optional["_models.UserIdentity"] = None,
        rerun_on_node_reboot_after_success: Optional[bool] = None,
        **kwargs
    ):
        """
        :keyword id: The ID can contain any combination of alphanumeric characters including hyphens
         and underscores and cannot contain more than 64 characters. If you do not specify this
         property, the Batch service assigns a default value of 'jobpreparation'. No other Task in the
         Job can have the same ID as the Job Preparation Task. If you try to submit a Task with the same
         id, the Batch service rejects the request with error code TaskIdSameAsJobPreparationTask; if
         you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        :paramtype id: str
        :keyword command_line: Required. The command line does not run under a shell, and therefore
         cannot take advantage of shell features such as environment variable expansion. If you want to
         take advantage of such features, you should invoke the shell in the command line, for example
         using "cmd /c MyCommand" in Windows or "/bin/sh -c MyCommand" in Linux. If the command line
         refers to file paths, it should use a relative path (relative to the Task working directory),
         or use the Batch provided environment variable
         (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        :paramtype command_line: str
        :keyword container_settings: When this is specified, all directories recursively below the
         AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the
         container, all Task environment variables are mapped into the container, and the Task command
         line is executed in the container. Files produced in the container outside of
         AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs
         will not be able to access those files.
        :paramtype container_settings: ~azure-batch.models.TaskContainerSettings
        :keyword resource_files: Files listed under this element are located in the Task's working
         directory.  There is a maximum size for the list of resource files.  When the max size is
         exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If
         this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved
         using .zip files, Application Packages, or Docker Containers.
        :paramtype resource_files: list[~azure-batch.models.ResourceFile]
        :keyword environment_settings: A list of environment variable settings for the Job Preparation
         Task.
        :paramtype environment_settings: list[~azure-batch.models.EnvironmentSetting]
        :keyword constraints: Execution constraints to apply to a Task.
        :paramtype constraints: ~azure-batch.models.TaskConstraints
        :keyword wait_for_success: If true and the Job Preparation Task fails on a Node, the Batch
         service retries the Job Preparation Task up to its maximum retry count (as specified in the
         constraints element). If the Task has still not completed successfully after all retries, then
         the Batch service will not schedule Tasks of the Job to the Node. The Node remains active and
         eligible to run Tasks of other Jobs. If false, the Batch service will not wait for the Job
         Preparation Task to complete. In this case, other Tasks of the Job can start executing on the
         Compute Node while the Job Preparation Task is still running; and even if the Job Preparation
         Task fails, new Tasks will continue to be scheduled on the Compute Node. The default value is
         true.
        :paramtype wait_for_success: bool
        :keyword user_identity: If omitted, the Task runs as a non-administrative user unique to the
         Task on Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux Compute
         Nodes.
        :paramtype user_identity: ~azure-batch.models.UserIdentity
        :keyword rerun_on_node_reboot_after_success: The Job Preparation Task is always rerun if a
         Compute Node is reimaged, or if the Job Preparation Task did not complete (e.g. because the
         reboot occurred while the Task was running). Therefore, you should always write a Job
         Preparation Task to be idempotent and to behave correctly if run multiple times. The default
         value is true.
        :paramtype rerun_on_node_reboot_after_success: bool
        """
        super(JobPreparationTask, self).__init__(**kwargs)
        self.id = id
        self.command_line = command_line
        self.container_settings = container_settings
        self.resource_files = resource_files
        self.environment_settings = environment_settings
        self.constraints = constraints
        self.wait_for_success = wait_for_success
        self.user_identity = user_identity
        self.rerun_on_node_reboot_after_success = rerun_on_node_reboot_after_success


class JobPreparationTaskExecutionInformation(msrest.serialization.Model):
    """Contains information about the execution of a Job Preparation Task on a Compute Node.

    All required parameters must be populated in order to send to Azure.

    :ivar start_time: Required. If the Task has been restarted or retried, this is the most recent
     time at which the Task started running.
    :vartype start_time: ~datetime.datetime
    :ivar end_time: This property is set only if the Task is in the Completed state.
    :vartype end_time: ~datetime.datetime
    :ivar state: Required. The current state of the Job Preparation Task on the Compute Node. Known
     values are: "running", "completed".
    :vartype state: str or ~azure-batch.models.JobPreparationTaskState
    :ivar task_root_directory: The root directory of the Job Preparation Task on the Compute Node.
     You can use this path to retrieve files created by the Task, such as log files.
    :vartype task_root_directory: str
    :ivar task_root_directory_url: The URL to the root directory of the Job Preparation Task on the
     Compute Node.
    :vartype task_root_directory_url: str
    :ivar exit_code: This parameter is returned only if the Task is in the completed state. The
     exit code for a process reflects the specific convention implemented by the application
     developer for that process. If you use the exit code value to make decisions in your code, be
     sure that you know the exit code convention used by the application process. Note that the exit
     code may also be generated by the Compute Node operating system, such as when a process is
     forcibly terminated.
    :vartype exit_code: int
    :ivar container_info: This property is set only if the Task runs in a container context.
    :vartype container_info: ~azure-batch.models.TaskContainerExecutionInformation
    :ivar failure_info: This property is set only if the Task is in the completed state and
     encountered a failure.
    :vartype failure_info: ~azure-batch.models.TaskFailureInformation
    :ivar retry_count: Required. Task application failures (non-zero exit code) are retried,
     pre-processing errors (the Task could not be run) and file upload errors are not retried. The
     Batch service will retry the Task up to the limit specified by the constraints.
    :vartype retry_count: int
    :ivar last_retry_time: This property is set only if the Task was retried (i.e. retryCount is
     nonzero). If present, this is typically the same as startTime, but may be different if the Task
     has been restarted for reasons other than retry; for example, if the Compute Node was rebooted
     during a retry, then the startTime is updated but the lastRetryTime is not.
    :vartype last_retry_time: ~datetime.datetime
    :ivar result: If the value is 'failed', then the details of the failure can be found in the
     failureInfo property. Known values are: "success", "failure".
    :vartype result: str or ~azure-batch.models.TaskExecutionResult
    """

    _validation = {
        'start_time': {'required': True},
        'state': {'required': True},
        'retry_count': {'required': True},
    }

    _attribute_map = {
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'end_time': {'key': 'endTime', 'type': 'iso-8601'},
        'state': {'key': 'state', 'type': 'str'},
        'task_root_directory': {'key': 'taskRootDirectory', 'type': 'str'},
        'task_root_directory_url': {'key': 'taskRootDirectoryUrl', 'type': 'str'},
        'exit_code': {'key': 'exitCode', 'type': 'int'},
        'container_info': {'key': 'containerInfo', 'type': 'TaskContainerExecutionInformation'},
        'failure_info': {'key': 'failureInfo', 'type': 'TaskFailureInformation'},
        'retry_count': {'key': 'retryCount', 'type': 'int'},
        'last_retry_time': {'key': 'lastRetryTime', 'type': 'iso-8601'},
        'result': {'key': 'result', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        start_time: datetime.datetime,
        state: Union[str, "_models.JobPreparationTaskState"],
        retry_count: int,
        end_time: Optional[datetime.datetime] = None,
        task_root_directory: Optional[str] = None,
        task_root_directory_url: Optional[str] = None,
        exit_code: Optional[int] = None,
        container_info: Optional["_models.TaskContainerExecutionInformation"] = None,
        failure_info: Optional["_models.TaskFailureInformation"] = None,
        last_retry_time: Optional[datetime.datetime] = None,
        result: Optional[Union[str, "_models.TaskExecutionResult"]] = None,
        **kwargs
    ):
        """
        :keyword start_time: Required. If the Task has been restarted or retried, this is the most
         recent time at which the Task started running.
        :paramtype start_time: ~datetime.datetime
        :keyword end_time: This property is set only if the Task is in the Completed state.
        :paramtype end_time: ~datetime.datetime
        :keyword state: Required. The current state of the Job Preparation Task on the Compute Node.
         Known values are: "running", "completed".
        :paramtype state: str or ~azure-batch.models.JobPreparationTaskState
        :keyword task_root_directory: The root directory of the Job Preparation Task on the Compute
         Node. You can use this path to retrieve files created by the Task, such as log files.
        :paramtype task_root_directory: str
        :keyword task_root_directory_url: The URL to the root directory of the Job Preparation Task on
         the Compute Node.
        :paramtype task_root_directory_url: str
        :keyword exit_code: This parameter is returned only if the Task is in the completed state. The
         exit code for a process reflects the specific convention implemented by the application
         developer for that process. If you use the exit code value to make decisions in your code, be
         sure that you know the exit code convention used by the application process. Note that the exit
         code may also be generated by the Compute Node operating system, such as when a process is
         forcibly terminated.
        :paramtype exit_code: int
        :keyword container_info: This property is set only if the Task runs in a container context.
        :paramtype container_info: ~azure-batch.models.TaskContainerExecutionInformation
        :keyword failure_info: This property is set only if the Task is in the completed state and
         encountered a failure.
        :paramtype failure_info: ~azure-batch.models.TaskFailureInformation
        :keyword retry_count: Required. Task application failures (non-zero exit code) are retried,
         pre-processing errors (the Task could not be run) and file upload errors are not retried. The
         Batch service will retry the Task up to the limit specified by the constraints.
        :paramtype retry_count: int
        :keyword last_retry_time: This property is set only if the Task was retried (i.e. retryCount is
         nonzero). If present, this is typically the same as startTime, but may be different if the Task
         has been restarted for reasons other than retry; for example, if the Compute Node was rebooted
         during a retry, then the startTime is updated but the lastRetryTime is not.
        :paramtype last_retry_time: ~datetime.datetime
        :keyword result: If the value is 'failed', then the details of the failure can be found in the
         failureInfo property. Known values are: "success", "failure".
        :paramtype result: str or ~azure-batch.models.TaskExecutionResult
        """
        super(JobPreparationTaskExecutionInformation, self).__init__(**kwargs)
        self.start_time = start_time
        self.end_time = end_time
        self.state = state
        self.task_root_directory = task_root_directory
        self.task_root_directory_url = task_root_directory_url
        self.exit_code = exit_code
        self.container_info = container_info
        self.failure_info = failure_info
        self.retry_count = retry_count
        self.last_retry_time = last_retry_time
        self.result = result


class JobReleaseTask(msrest.serialization.Model):
    """The Job Release Task runs when the Job ends, because of one of the following: The user calls the Terminate Job API, or the Delete Job API while the Job is still active, the Job's maximum wall clock time constraint is reached, and the Job is still active, or the Job's Job Manager Task completed, and the Job is configured to terminate when the Job Manager completes. The Job Release Task runs on each Node where Tasks of the Job have run and the Job Preparation Task ran and completed. If you reimage a Node after it has run the Job Preparation Task, and the Job ends without any further Tasks of the Job running on that Node (and hence the Job Preparation Task does not re-run), then the Job Release Task does not run on that Compute Node. If a Node reboots while the Job Release Task is still running, the Job Release Task runs again when the Compute Node starts up. The Job is not marked as complete until all Job Release Tasks have completed. The Job Release Task runs in the background. It does not occupy a scheduling slot; that is, it does not count towards the taskSlotsPerNode limit specified on the Pool.

    All required parameters must be populated in order to send to Azure.

    :ivar id: The ID can contain any combination of alphanumeric characters including hyphens and
     underscores and cannot contain more than 64 characters. If you do not specify this property,
     the Batch service assigns a default value of 'jobrelease'. No other Task in the Job can have
     the same ID as the Job Release Task. If you try to submit a Task with the same id, the Batch
     service rejects the request with error code TaskIdSameAsJobReleaseTask; if you are calling the
     REST API directly, the HTTP status code is 409 (Conflict).
    :vartype id: str
    :ivar command_line: Required. The command line does not run under a shell, and therefore cannot
     take advantage of shell features such as environment variable expansion. If you want to take
     advantage of such features, you should invoke the shell in the command line, for example using
     "cmd /c MyCommand" in Windows or "/bin/sh -c MyCommand" in Linux. If the command line refers to
     file paths, it should use a relative path (relative to the Task working directory), or use the
     Batch provided environment variable
     (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
    :vartype command_line: str
    :ivar container_settings: When this is specified, all directories recursively below the
     AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the
     container, all Task environment variables are mapped into the container, and the Task command
     line is executed in the container. Files produced in the container outside of
     AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs
     will not be able to access those files.
    :vartype container_settings: ~azure-batch.models.TaskContainerSettings
    :ivar resource_files: Files listed under this element are located in the Task's working
     directory.
    :vartype resource_files: list[~azure-batch.models.ResourceFile]
    :ivar environment_settings: A list of environment variable settings for the Job Release Task.
    :vartype environment_settings: list[~azure-batch.models.EnvironmentSetting]
    :ivar max_wall_clock_time: The maximum elapsed time that the Job Release Task may run on a
     given Compute Node, measured from the time the Task starts. If the Task does not complete
     within the time limit, the Batch service terminates it. The default value is 15 minutes. You
     may not specify a timeout longer than 15 minutes. If you do, the Batch service rejects it with
     an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
    :vartype max_wall_clock_time: ~datetime.timedelta
    :ivar retention_time: The default is 7 days, i.e. the Task directory will be retained for 7
     days unless the Compute Node is removed or the Job is deleted.
    :vartype retention_time: ~datetime.timedelta
    :ivar user_identity: If omitted, the Task runs as a non-administrative user unique to the Task.
    :vartype user_identity: ~azure-batch.models.UserIdentity
    """

    _validation = {
        'command_line': {'required': True},
    }

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'command_line': {'key': 'commandLine', 'type': 'str'},
        'container_settings': {'key': 'containerSettings', 'type': 'TaskContainerSettings'},
        'resource_files': {'key': 'resourceFiles', 'type': '[ResourceFile]'},
        'environment_settings': {'key': 'environmentSettings', 'type': '[EnvironmentSetting]'},
        'max_wall_clock_time': {'key': 'maxWallClockTime', 'type': 'duration'},
        'retention_time': {'key': 'retentionTime', 'type': 'duration'},
        'user_identity': {'key': 'userIdentity', 'type': 'UserIdentity'},
    }

    def __init__(
        self,
        *,
        command_line: str,
        id: Optional[str] = None,
        container_settings: Optional["_models.TaskContainerSettings"] = None,
        resource_files: Optional[List["_models.ResourceFile"]] = None,
        environment_settings: Optional[List["_models.EnvironmentSetting"]] = None,
        max_wall_clock_time: Optional[datetime.timedelta] = None,
        retention_time: Optional[datetime.timedelta] = None,
        user_identity: Optional["_models.UserIdentity"] = None,
        **kwargs
    ):
        """
        :keyword id: The ID can contain any combination of alphanumeric characters including hyphens
         and underscores and cannot contain more than 64 characters. If you do not specify this
         property, the Batch service assigns a default value of 'jobrelease'. No other Task in the Job
         can have the same ID as the Job Release Task. If you try to submit a Task with the same id, the
         Batch service rejects the request with error code TaskIdSameAsJobReleaseTask; if you are
         calling the REST API directly, the HTTP status code is 409 (Conflict).
        :paramtype id: str
        :keyword command_line: Required. The command line does not run under a shell, and therefore
         cannot take advantage of shell features such as environment variable expansion. If you want to
         take advantage of such features, you should invoke the shell in the command line, for example
         using "cmd /c MyCommand" in Windows or "/bin/sh -c MyCommand" in Linux. If the command line
         refers to file paths, it should use a relative path (relative to the Task working directory),
         or use the Batch provided environment variable
         (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        :paramtype command_line: str
        :keyword container_settings: When this is specified, all directories recursively below the
         AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the
         container, all Task environment variables are mapped into the container, and the Task command
         line is executed in the container. Files produced in the container outside of
         AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs
         will not be able to access those files.
        :paramtype container_settings: ~azure-batch.models.TaskContainerSettings
        :keyword resource_files: Files listed under this element are located in the Task's working
         directory.
        :paramtype resource_files: list[~azure-batch.models.ResourceFile]
        :keyword environment_settings: A list of environment variable settings for the Job Release
         Task.
        :paramtype environment_settings: list[~azure-batch.models.EnvironmentSetting]
        :keyword max_wall_clock_time: The maximum elapsed time that the Job Release Task may run on a
         given Compute Node, measured from the time the Task starts. If the Task does not complete
         within the time limit, the Batch service terminates it. The default value is 15 minutes. You
         may not specify a timeout longer than 15 minutes. If you do, the Batch service rejects it with
         an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        :paramtype max_wall_clock_time: ~datetime.timedelta
        :keyword retention_time: The default is 7 days, i.e. the Task directory will be retained for 7
         days unless the Compute Node is removed or the Job is deleted.
        :paramtype retention_time: ~datetime.timedelta
        :keyword user_identity: If omitted, the Task runs as a non-administrative user unique to the
         Task.
        :paramtype user_identity: ~azure-batch.models.UserIdentity
        """
        super(JobReleaseTask, self).__init__(**kwargs)
        self.id = id
        self.command_line = command_line
        self.container_settings = container_settings
        self.resource_files = resource_files
        self.environment_settings = environment_settings
        self.max_wall_clock_time = max_wall_clock_time
        self.retention_time = retention_time
        self.user_identity = user_identity


class JobReleaseTaskExecutionInformation(msrest.serialization.Model):
    """Contains information about the execution of a Job Release Task on a Compute Node.

    All required parameters must be populated in order to send to Azure.

    :ivar start_time: Required. If the Task has been restarted or retried, this is the most recent
     time at which the Task started running.
    :vartype start_time: ~datetime.datetime
    :ivar end_time: This property is set only if the Task is in the Completed state.
    :vartype end_time: ~datetime.datetime
    :ivar state: Required. The current state of the Job Release Task on the Compute Node. Known
     values are: "running", "completed".
    :vartype state: str or ~azure-batch.models.JobReleaseTaskState
    :ivar task_root_directory: The root directory of the Job Release Task on the Compute Node. You
     can use this path to retrieve files created by the Task, such as log files.
    :vartype task_root_directory: str
    :ivar task_root_directory_url: The URL to the root directory of the Job Release Task on the
     Compute Node.
    :vartype task_root_directory_url: str
    :ivar exit_code: This parameter is returned only if the Task is in the completed state. The
     exit code for a process reflects the specific convention implemented by the application
     developer for that process. If you use the exit code value to make decisions in your code, be
     sure that you know the exit code convention used by the application process. Note that the exit
     code may also be generated by the Compute Node operating system, such as when a process is
     forcibly terminated.
    :vartype exit_code: int
    :ivar container_info: This property is set only if the Task runs in a container context.
    :vartype container_info: ~azure-batch.models.TaskContainerExecutionInformation
    :ivar failure_info: This property is set only if the Task is in the completed state and
     encountered a failure.
    :vartype failure_info: ~azure-batch.models.TaskFailureInformation
    :ivar result: If the value is 'failed', then the details of the failure can be found in the
     failureInfo property. Known values are: "success", "failure".
    :vartype result: str or ~azure-batch.models.TaskExecutionResult
    """

    _validation = {
        'start_time': {'required': True},
        'state': {'required': True},
    }

    _attribute_map = {
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'end_time': {'key': 'endTime', 'type': 'iso-8601'},
        'state': {'key': 'state', 'type': 'str'},
        'task_root_directory': {'key': 'taskRootDirectory', 'type': 'str'},
        'task_root_directory_url': {'key': 'taskRootDirectoryUrl', 'type': 'str'},
        'exit_code': {'key': 'exitCode', 'type': 'int'},
        'container_info': {'key': 'containerInfo', 'type': 'TaskContainerExecutionInformation'},
        'failure_info': {'key': 'failureInfo', 'type': 'TaskFailureInformation'},
        'result': {'key': 'result', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        start_time: datetime.datetime,
        state: Union[str, "_models.JobReleaseTaskState"],
        end_time: Optional[datetime.datetime] = None,
        task_root_directory: Optional[str] = None,
        task_root_directory_url: Optional[str] = None,
        exit_code: Optional[int] = None,
        container_info: Optional["_models.TaskContainerExecutionInformation"] = None,
        failure_info: Optional["_models.TaskFailureInformation"] = None,
        result: Optional[Union[str, "_models.TaskExecutionResult"]] = None,
        **kwargs
    ):
        """
        :keyword start_time: Required. If the Task has been restarted or retried, this is the most
         recent time at which the Task started running.
        :paramtype start_time: ~datetime.datetime
        :keyword end_time: This property is set only if the Task is in the Completed state.
        :paramtype end_time: ~datetime.datetime
        :keyword state: Required. The current state of the Job Release Task on the Compute Node. Known
         values are: "running", "completed".
        :paramtype state: str or ~azure-batch.models.JobReleaseTaskState
        :keyword task_root_directory: The root directory of the Job Release Task on the Compute Node.
         You can use this path to retrieve files created by the Task, such as log files.
        :paramtype task_root_directory: str
        :keyword task_root_directory_url: The URL to the root directory of the Job Release Task on the
         Compute Node.
        :paramtype task_root_directory_url: str
        :keyword exit_code: This parameter is returned only if the Task is in the completed state. The
         exit code for a process reflects the specific convention implemented by the application
         developer for that process. If you use the exit code value to make decisions in your code, be
         sure that you know the exit code convention used by the application process. Note that the exit
         code may also be generated by the Compute Node operating system, such as when a process is
         forcibly terminated.
        :paramtype exit_code: int
        :keyword container_info: This property is set only if the Task runs in a container context.
        :paramtype container_info: ~azure-batch.models.TaskContainerExecutionInformation
        :keyword failure_info: This property is set only if the Task is in the completed state and
         encountered a failure.
        :paramtype failure_info: ~azure-batch.models.TaskFailureInformation
        :keyword result: If the value is 'failed', then the details of the failure can be found in the
         failureInfo property. Known values are: "success", "failure".
        :paramtype result: str or ~azure-batch.models.TaskExecutionResult
        """
        super(JobReleaseTaskExecutionInformation, self).__init__(**kwargs)
        self.start_time = start_time
        self.end_time = end_time
        self.state = state
        self.task_root_directory = task_root_directory
        self.task_root_directory_url = task_root_directory_url
        self.exit_code = exit_code
        self.container_info = container_info
        self.failure_info = failure_info
        self.result = result


class JobScheduleExecutionInformation(msrest.serialization.Model):
    """Contains information about Jobs that have been and will be run under a Job Schedule.

    :ivar next_run_time: This property is meaningful only if the schedule is in the active state
     when the time comes around. For example, if the schedule is disabled, no Job will be created at
     nextRunTime unless the Job is enabled before then.
    :vartype next_run_time: ~datetime.datetime
    :ivar recent_job: This property is present only if the at least one Job has run under the
     schedule.
    :vartype recent_job: ~azure-batch.models.RecentJob
    :ivar end_time: This property is set only if the Job Schedule is in the completed state.
    :vartype end_time: ~datetime.datetime
    """

    _attribute_map = {
        'next_run_time': {'key': 'nextRunTime', 'type': 'iso-8601'},
        'recent_job': {'key': 'recentJob', 'type': 'RecentJob'},
        'end_time': {'key': 'endTime', 'type': 'iso-8601'},
    }

    def __init__(
        self,
        *,
        next_run_time: Optional[datetime.datetime] = None,
        recent_job: Optional["_models.RecentJob"] = None,
        end_time: Optional[datetime.datetime] = None,
        **kwargs
    ):
        """
        :keyword next_run_time: This property is meaningful only if the schedule is in the active state
         when the time comes around. For example, if the schedule is disabled, no Job will be created at
         nextRunTime unless the Job is enabled before then.
        :paramtype next_run_time: ~datetime.datetime
        :keyword recent_job: This property is present only if the at least one Job has run under the
         schedule.
        :paramtype recent_job: ~azure-batch.models.RecentJob
        :keyword end_time: This property is set only if the Job Schedule is in the completed state.
        :paramtype end_time: ~datetime.datetime
        """
        super(JobScheduleExecutionInformation, self).__init__(**kwargs)
        self.next_run_time = next_run_time
        self.recent_job = recent_job
        self.end_time = end_time


class JobScheduleStatistics(msrest.serialization.Model):
    """Resource usage statistics for a Job Schedule.

    All required parameters must be populated in order to send to Azure.

    :ivar url: Required. The URL of the statistics.
    :vartype url: str
    :ivar start_time: Required. The start time of the time range covered by the statistics.
    :vartype start_time: ~datetime.datetime
    :ivar last_update_time: Required. The time at which the statistics were last updated. All
     statistics are limited to the range between startTime and lastUpdateTime.
    :vartype last_update_time: ~datetime.datetime
    :ivar user_cpu_time: Required. The total user mode CPU time (summed across all cores and all
     Compute Nodes) consumed by all Tasks in all Jobs created under the schedule.
    :vartype user_cpu_time: ~datetime.timedelta
    :ivar kernel_cpu_time: Required. The total kernel mode CPU time (summed across all cores and
     all Compute Nodes) consumed by all Tasks in all Jobs created under the schedule.
    :vartype kernel_cpu_time: ~datetime.timedelta
    :ivar wall_clock_time: Required. The wall clock time is the elapsed time from when the Task
     started running on a Compute Node to when it finished (or to the last time the statistics were
     updated, if the Task had not finished by then). If a Task was retried, this includes the wall
     clock time of all the Task retries.
    :vartype wall_clock_time: ~datetime.timedelta
    :ivar read_i_ops: Required. The total number of disk read operations made by all Tasks in all
     Jobs created under the schedule.
    :vartype read_i_ops: long
    :ivar write_i_ops: Required. The total number of disk write operations made by all Tasks in all
     Jobs created under the schedule.
    :vartype write_i_ops: long
    :ivar read_io_gi_b: Required. The total gibibytes read from disk by all Tasks in all Jobs
     created under the schedule.
    :vartype read_io_gi_b: float
    :ivar write_io_gi_b: Required. The total gibibytes written to disk by all Tasks in all Jobs
     created under the schedule.
    :vartype write_io_gi_b: float
    :ivar num_succeeded_tasks: Required. The total number of Tasks successfully completed during
     the given time range in Jobs created under the schedule. A Task completes successfully if it
     returns exit code 0.
    :vartype num_succeeded_tasks: long
    :ivar num_failed_tasks: Required. The total number of Tasks that failed during the given time
     range in Jobs created under the schedule. A Task fails if it exhausts its maximum retry count
     without returning exit code 0.
    :vartype num_failed_tasks: long
    :ivar num_task_retries: Required. The total number of retries during the given time range on
     all Tasks in all Jobs created under the schedule.
    :vartype num_task_retries: long
    :ivar wait_time: Required. This value is only reported in the Account lifetime statistics; it
     is not included in the Job statistics.
    :vartype wait_time: ~datetime.timedelta
    """

    _validation = {
        'url': {'required': True},
        'start_time': {'required': True},
        'last_update_time': {'required': True},
        'user_cpu_time': {'required': True},
        'kernel_cpu_time': {'required': True},
        'wall_clock_time': {'required': True},
        'read_i_ops': {'required': True},
        'write_i_ops': {'required': True},
        'read_io_gi_b': {'required': True},
        'write_io_gi_b': {'required': True},
        'num_succeeded_tasks': {'required': True},
        'num_failed_tasks': {'required': True},
        'num_task_retries': {'required': True},
        'wait_time': {'required': True},
    }

    _attribute_map = {
        'url': {'key': 'url', 'type': 'str'},
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'last_update_time': {'key': 'lastUpdateTime', 'type': 'iso-8601'},
        'user_cpu_time': {'key': 'userCPUTime', 'type': 'duration'},
        'kernel_cpu_time': {'key': 'kernelCPUTime', 'type': 'duration'},
        'wall_clock_time': {'key': 'wallClockTime', 'type': 'duration'},
        'read_i_ops': {'key': 'readIOps', 'type': 'long'},
        'write_i_ops': {'key': 'writeIOps', 'type': 'long'},
        'read_io_gi_b': {'key': 'readIOGiB', 'type': 'float'},
        'write_io_gi_b': {'key': 'writeIOGiB', 'type': 'float'},
        'num_succeeded_tasks': {'key': 'numSucceededTasks', 'type': 'long'},
        'num_failed_tasks': {'key': 'numFailedTasks', 'type': 'long'},
        'num_task_retries': {'key': 'numTaskRetries', 'type': 'long'},
        'wait_time': {'key': 'waitTime', 'type': 'duration'},
    }

    def __init__(
        self,
        *,
        url: str,
        start_time: datetime.datetime,
        last_update_time: datetime.datetime,
        user_cpu_time: datetime.timedelta,
        kernel_cpu_time: datetime.timedelta,
        wall_clock_time: datetime.timedelta,
        read_i_ops: int,
        write_i_ops: int,
        read_io_gi_b: float,
        write_io_gi_b: float,
        num_succeeded_tasks: int,
        num_failed_tasks: int,
        num_task_retries: int,
        wait_time: datetime.timedelta,
        **kwargs
    ):
        """
        :keyword url: Required. The URL of the statistics.
        :paramtype url: str
        :keyword start_time: Required. The start time of the time range covered by the statistics.
        :paramtype start_time: ~datetime.datetime
        :keyword last_update_time: Required. The time at which the statistics were last updated. All
         statistics are limited to the range between startTime and lastUpdateTime.
        :paramtype last_update_time: ~datetime.datetime
        :keyword user_cpu_time: Required. The total user mode CPU time (summed across all cores and all
         Compute Nodes) consumed by all Tasks in all Jobs created under the schedule.
        :paramtype user_cpu_time: ~datetime.timedelta
        :keyword kernel_cpu_time: Required. The total kernel mode CPU time (summed across all cores and
         all Compute Nodes) consumed by all Tasks in all Jobs created under the schedule.
        :paramtype kernel_cpu_time: ~datetime.timedelta
        :keyword wall_clock_time: Required. The wall clock time is the elapsed time from when the Task
         started running on a Compute Node to when it finished (or to the last time the statistics were
         updated, if the Task had not finished by then). If a Task was retried, this includes the wall
         clock time of all the Task retries.
        :paramtype wall_clock_time: ~datetime.timedelta
        :keyword read_i_ops: Required. The total number of disk read operations made by all Tasks in
         all Jobs created under the schedule.
        :paramtype read_i_ops: long
        :keyword write_i_ops: Required. The total number of disk write operations made by all Tasks in
         all Jobs created under the schedule.
        :paramtype write_i_ops: long
        :keyword read_io_gi_b: Required. The total gibibytes read from disk by all Tasks in all Jobs
         created under the schedule.
        :paramtype read_io_gi_b: float
        :keyword write_io_gi_b: Required. The total gibibytes written to disk by all Tasks in all Jobs
         created under the schedule.
        :paramtype write_io_gi_b: float
        :keyword num_succeeded_tasks: Required. The total number of Tasks successfully completed during
         the given time range in Jobs created under the schedule. A Task completes successfully if it
         returns exit code 0.
        :paramtype num_succeeded_tasks: long
        :keyword num_failed_tasks: Required. The total number of Tasks that failed during the given
         time range in Jobs created under the schedule. A Task fails if it exhausts its maximum retry
         count without returning exit code 0.
        :paramtype num_failed_tasks: long
        :keyword num_task_retries: Required. The total number of retries during the given time range on
         all Tasks in all Jobs created under the schedule.
        :paramtype num_task_retries: long
        :keyword wait_time: Required. This value is only reported in the Account lifetime statistics;
         it is not included in the Job statistics.
        :paramtype wait_time: ~datetime.timedelta
        """
        super(JobScheduleStatistics, self).__init__(**kwargs)
        self.url = url
        self.start_time = start_time
        self.last_update_time = last_update_time
        self.user_cpu_time = user_cpu_time
        self.kernel_cpu_time = kernel_cpu_time
        self.wall_clock_time = wall_clock_time
        self.read_i_ops = read_i_ops
        self.write_i_ops = write_i_ops
        self.read_io_gi_b = read_io_gi_b
        self.write_io_gi_b = write_io_gi_b
        self.num_succeeded_tasks = num_succeeded_tasks
        self.num_failed_tasks = num_failed_tasks
        self.num_task_retries = num_task_retries
        self.wait_time = wait_time


class JobSchedulingError(msrest.serialization.Model):
    """An error encountered by the Batch service when scheduling a Job.

    All required parameters must be populated in order to send to Azure.

    :ivar category: Required. The category of the error. Known values are: "usererror",
     "servererror".
    :vartype category: str or ~azure-batch.models.ErrorCategory
    :ivar code: An identifier for the Job scheduling error. Codes are invariant and are intended to
     be consumed programmatically.
    :vartype code: str
    :ivar message: A message describing the Job scheduling error, intended to be suitable for
     display in a user interface.
    :vartype message: str
    :ivar details: A list of additional error details related to the scheduling error.
    :vartype details: list[~azure-batch.models.NameValuePair]
    """

    _validation = {
        'category': {'required': True},
    }

    _attribute_map = {
        'category': {'key': 'category', 'type': 'str'},
        'code': {'key': 'code', 'type': 'str'},
        'message': {'key': 'message', 'type': 'str'},
        'details': {'key': 'details', 'type': '[NameValuePair]'},
    }

    def __init__(
        self,
        *,
        category: Union[str, "_models.ErrorCategory"],
        code: Optional[str] = None,
        message: Optional[str] = None,
        details: Optional[List["_models.NameValuePair"]] = None,
        **kwargs
    ):
        """
        :keyword category: Required. The category of the error. Known values are: "usererror",
         "servererror".
        :paramtype category: str or ~azure-batch.models.ErrorCategory
        :keyword code: An identifier for the Job scheduling error. Codes are invariant and are intended
         to be consumed programmatically.
        :paramtype code: str
        :keyword message: A message describing the Job scheduling error, intended to be suitable for
         display in a user interface.
        :paramtype message: str
        :keyword details: A list of additional error details related to the scheduling error.
        :paramtype details: list[~azure-batch.models.NameValuePair]
        """
        super(JobSchedulingError, self).__init__(**kwargs)
        self.category = category
        self.code = code
        self.message = message
        self.details = details


class JobSpecification(msrest.serialization.Model):
    """Specifies details of the Jobs to be created on a schedule.

    All required parameters must be populated in order to send to Azure.

    :ivar priority: Priority values can range from -1000 to 1000, with -1000 being the lowest
     priority and 1000 being the highest priority. The default value is 0. This priority is used as
     the default for all Jobs under the Job Schedule. You can update a Job's priority after it has
     been created using by using the update Job API.
    :vartype priority: int
    :ivar allow_task_preemption: If the value is set to True, other high priority jobs submitted to
     the system will take precedence and will be able requeue tasks from this job. You can update a
     job's allowTaskPreemption after it has been created using the update job API.
    :vartype allow_task_preemption: bool
    :ivar max_parallel_tasks: The value of maxParallelTasks must be -1 or greater than 0 if
     specified. If not specified, the default value is -1, which means there's no limit to the
     number of tasks that can be run at once. You can update a job's maxParallelTasks after it has
     been created using the update job API.
    :vartype max_parallel_tasks: int
    :ivar display_name: The name need not be unique and can contain any Unicode characters up to a
     maximum length of 1024.
    :vartype display_name: str
    :ivar uses_task_dependencies: Whether Tasks in the Job can define dependencies on each other.
     The default is false.
    :vartype uses_task_dependencies: bool
    :ivar on_all_tasks_complete: Note that if a Job contains no Tasks, then all Tasks are
     considered complete. This option is therefore most commonly used with a Job Manager task; if
     you want to use automatic Job termination without a Job Manager, you should initially set
     onAllTasksComplete to noaction and update the Job properties to set onAllTasksComplete to
     terminatejob once you have finished adding Tasks. The default is noaction. Known values are:
     "noaction", "terminatejob".
    :vartype on_all_tasks_complete: str or ~azure-batch.models.OnAllTasksComplete
    :ivar on_task_failure: The default is noaction. Known values are: "noaction",
     "performexitoptionsjobaction".
    :vartype on_task_failure: str or ~azure-batch.models.OnTaskFailure
    :ivar network_configuration: The network configuration for the Job.
    :vartype network_configuration: ~azure-batch.models.JobNetworkConfiguration
    :ivar constraints: The execution constraints for a Job.
    :vartype constraints: ~azure-batch.models.JobConstraints
    :ivar job_manager_task: If the Job does not specify a Job Manager Task, the user must
     explicitly add Tasks to the Job using the Task API. If the Job does specify a Job Manager Task,
     the Batch service creates the Job Manager Task when the Job is created, and will try to
     schedule the Job Manager Task before scheduling other Tasks in the Job.
    :vartype job_manager_task: ~azure-batch.models.JobManagerTask
    :ivar job_preparation_task: If a Job has a Job Preparation Task, the Batch service will run the
     Job Preparation Task on a Node before starting any Tasks of that Job on that Compute Node.
    :vartype job_preparation_task: ~azure-batch.models.JobPreparationTask
    :ivar job_release_task: The primary purpose of the Job Release Task is to undo changes to Nodes
     made by the Job Preparation Task. Example activities include deleting local files, or shutting
     down services that were started as part of Job preparation. A Job Release Task cannot be
     specified without also specifying a Job Preparation Task for the Job. The Batch service runs
     the Job Release Task on the Compute Nodes that have run the Job Preparation Task.
    :vartype job_release_task: ~azure-batch.models.JobReleaseTask
    :ivar common_environment_settings: Individual Tasks can override an environment setting
     specified here by specifying the same setting name with a different value.
    :vartype common_environment_settings: list[~azure-batch.models.EnvironmentSetting]
    :ivar pool_info: Required. Specifies how a Job should be assigned to a Pool.
    :vartype pool_info: ~azure-batch.models.PoolInformation
    :ivar metadata: The Batch service does not assign any meaning to metadata; it is solely for the
     use of user code.
    :vartype metadata: list[~azure-batch.models.MetadataItem]
    """

    _validation = {
        'pool_info': {'required': True},
    }

    _attribute_map = {
        'priority': {'key': 'priority', 'type': 'int'},
        'allow_task_preemption': {'key': 'allowTaskPreemption', 'type': 'bool'},
        'max_parallel_tasks': {'key': 'maxParallelTasks', 'type': 'int'},
        'display_name': {'key': 'displayName', 'type': 'str'},
        'uses_task_dependencies': {'key': 'usesTaskDependencies', 'type': 'bool'},
        'on_all_tasks_complete': {'key': 'onAllTasksComplete', 'type': 'str'},
        'on_task_failure': {'key': 'onTaskFailure', 'type': 'str'},
        'network_configuration': {'key': 'networkConfiguration', 'type': 'JobNetworkConfiguration'},
        'constraints': {'key': 'constraints', 'type': 'JobConstraints'},
        'job_manager_task': {'key': 'jobManagerTask', 'type': 'JobManagerTask'},
        'job_preparation_task': {'key': 'jobPreparationTask', 'type': 'JobPreparationTask'},
        'job_release_task': {'key': 'jobReleaseTask', 'type': 'JobReleaseTask'},
        'common_environment_settings': {'key': 'commonEnvironmentSettings', 'type': '[EnvironmentSetting]'},
        'pool_info': {'key': 'poolInfo', 'type': 'PoolInformation'},
        'metadata': {'key': 'metadata', 'type': '[MetadataItem]'},
    }

    def __init__(
        self,
        *,
        pool_info: "_models.PoolInformation",
        priority: Optional[int] = None,
        allow_task_preemption: Optional[bool] = None,
        max_parallel_tasks: Optional[int] = -1,
        display_name: Optional[str] = None,
        uses_task_dependencies: Optional[bool] = None,
        on_all_tasks_complete: Optional[Union[str, "_models.OnAllTasksComplete"]] = None,
        on_task_failure: Optional[Union[str, "_models.OnTaskFailure"]] = None,
        network_configuration: Optional["_models.JobNetworkConfiguration"] = None,
        constraints: Optional["_models.JobConstraints"] = None,
        job_manager_task: Optional["_models.JobManagerTask"] = None,
        job_preparation_task: Optional["_models.JobPreparationTask"] = None,
        job_release_task: Optional["_models.JobReleaseTask"] = None,
        common_environment_settings: Optional[List["_models.EnvironmentSetting"]] = None,
        metadata: Optional[List["_models.MetadataItem"]] = None,
        **kwargs
    ):
        """
        :keyword priority: Priority values can range from -1000 to 1000, with -1000 being the lowest
         priority and 1000 being the highest priority. The default value is 0. This priority is used as
         the default for all Jobs under the Job Schedule. You can update a Job's priority after it has
         been created using by using the update Job API.
        :paramtype priority: int
        :keyword allow_task_preemption: If the value is set to True, other high priority jobs submitted
         to the system will take precedence and will be able requeue tasks from this job. You can update
         a job's allowTaskPreemption after it has been created using the update job API.
        :paramtype allow_task_preemption: bool
        :keyword max_parallel_tasks: The value of maxParallelTasks must be -1 or greater than 0 if
         specified. If not specified, the default value is -1, which means there's no limit to the
         number of tasks that can be run at once. You can update a job's maxParallelTasks after it has
         been created using the update job API.
        :paramtype max_parallel_tasks: int
        :keyword display_name: The name need not be unique and can contain any Unicode characters up to
         a maximum length of 1024.
        :paramtype display_name: str
        :keyword uses_task_dependencies: Whether Tasks in the Job can define dependencies on each
         other. The default is false.
        :paramtype uses_task_dependencies: bool
        :keyword on_all_tasks_complete: Note that if a Job contains no Tasks, then all Tasks are
         considered complete. This option is therefore most commonly used with a Job Manager task; if
         you want to use automatic Job termination without a Job Manager, you should initially set
         onAllTasksComplete to noaction and update the Job properties to set onAllTasksComplete to
         terminatejob once you have finished adding Tasks. The default is noaction. Known values are:
         "noaction", "terminatejob".
        :paramtype on_all_tasks_complete: str or ~azure-batch.models.OnAllTasksComplete
        :keyword on_task_failure: The default is noaction. Known values are: "noaction",
         "performexitoptionsjobaction".
        :paramtype on_task_failure: str or ~azure-batch.models.OnTaskFailure
        :keyword network_configuration: The network configuration for the Job.
        :paramtype network_configuration: ~azure-batch.models.JobNetworkConfiguration
        :keyword constraints: The execution constraints for a Job.
        :paramtype constraints: ~azure-batch.models.JobConstraints
        :keyword job_manager_task: If the Job does not specify a Job Manager Task, the user must
         explicitly add Tasks to the Job using the Task API. If the Job does specify a Job Manager Task,
         the Batch service creates the Job Manager Task when the Job is created, and will try to
         schedule the Job Manager Task before scheduling other Tasks in the Job.
        :paramtype job_manager_task: ~azure-batch.models.JobManagerTask
        :keyword job_preparation_task: If a Job has a Job Preparation Task, the Batch service will run
         the Job Preparation Task on a Node before starting any Tasks of that Job on that Compute Node.
        :paramtype job_preparation_task: ~azure-batch.models.JobPreparationTask
        :keyword job_release_task: The primary purpose of the Job Release Task is to undo changes to
         Nodes made by the Job Preparation Task. Example activities include deleting local files, or
         shutting down services that were started as part of Job preparation. A Job Release Task cannot
         be specified without also specifying a Job Preparation Task for the Job. The Batch service runs
         the Job Release Task on the Compute Nodes that have run the Job Preparation Task.
        :paramtype job_release_task: ~azure-batch.models.JobReleaseTask
        :keyword common_environment_settings: Individual Tasks can override an environment setting
         specified here by specifying the same setting name with a different value.
        :paramtype common_environment_settings: list[~azure-batch.models.EnvironmentSetting]
        :keyword pool_info: Required. Specifies how a Job should be assigned to a Pool.
        :paramtype pool_info: ~azure-batch.models.PoolInformation
        :keyword metadata: The Batch service does not assign any meaning to metadata; it is solely for
         the use of user code.
        :paramtype metadata: list[~azure-batch.models.MetadataItem]
        """
        super(JobSpecification, self).__init__(**kwargs)
        self.priority = priority
        self.allow_task_preemption = allow_task_preemption
        self.max_parallel_tasks = max_parallel_tasks
        self.display_name = display_name
        self.uses_task_dependencies = uses_task_dependencies
        self.on_all_tasks_complete = on_all_tasks_complete
        self.on_task_failure = on_task_failure
        self.network_configuration = network_configuration
        self.constraints = constraints
        self.job_manager_task = job_manager_task
        self.job_preparation_task = job_preparation_task
        self.job_release_task = job_release_task
        self.common_environment_settings = common_environment_settings
        self.pool_info = pool_info
        self.metadata = metadata


class JobStatistics(msrest.serialization.Model):
    """Resource usage statistics for a Job.

    All required parameters must be populated in order to send to Azure.

    :ivar url: Required. The URL of the statistics.
    :vartype url: str
    :ivar start_time: Required. The start time of the time range covered by the statistics.
    :vartype start_time: ~datetime.datetime
    :ivar last_update_time: Required. The time at which the statistics were last updated. All
     statistics are limited to the range between startTime and lastUpdateTime.
    :vartype last_update_time: ~datetime.datetime
    :ivar user_cpu_time: Required. The total user mode CPU time (summed across all cores and all
     Compute Nodes) consumed by all Tasks in the Job.
    :vartype user_cpu_time: ~datetime.timedelta
    :ivar kernel_cpu_time: Required. The total kernel mode CPU time (summed across all cores and
     all Compute Nodes) consumed by all Tasks in the Job.
    :vartype kernel_cpu_time: ~datetime.timedelta
    :ivar wall_clock_time: Required. The wall clock time is the elapsed time from when the Task
     started running on a Compute Node to when it finished (or to the last time the statistics were
     updated, if the Task had not finished by then). If a Task was retried, this includes the wall
     clock time of all the Task retries.
    :vartype wall_clock_time: ~datetime.timedelta
    :ivar read_i_ops: Required. The total number of disk read operations made by all Tasks in the
     Job.
    :vartype read_i_ops: long
    :ivar write_i_ops: Required. The total number of disk write operations made by all Tasks in the
     Job.
    :vartype write_i_ops: long
    :ivar read_io_gi_b: Required. The total amount of data in GiB read from disk by all Tasks in
     the Job.
    :vartype read_io_gi_b: float
    :ivar write_io_gi_b: Required. The total amount of data in GiB written to disk by all Tasks in
     the Job.
    :vartype write_io_gi_b: float
    :ivar num_succeeded_tasks: Required. A Task completes successfully if it returns exit code 0.
    :vartype num_succeeded_tasks: long
    :ivar num_failed_tasks: Required. A Task fails if it exhausts its maximum retry count without
     returning exit code 0.
    :vartype num_failed_tasks: long
    :ivar num_task_retries: Required. The total number of retries on all the Tasks in the Job
     during the given time range.
    :vartype num_task_retries: long
    :ivar wait_time: Required. The wait time for a Task is defined as the elapsed time between the
     creation of the Task and the start of Task execution. (If the Task is retried due to failures,
     the wait time is the time to the most recent Task execution.) This value is only reported in
     the Account lifetime statistics; it is not included in the Job statistics.
    :vartype wait_time: ~datetime.timedelta
    """

    _validation = {
        'url': {'required': True},
        'start_time': {'required': True},
        'last_update_time': {'required': True},
        'user_cpu_time': {'required': True},
        'kernel_cpu_time': {'required': True},
        'wall_clock_time': {'required': True},
        'read_i_ops': {'required': True},
        'write_i_ops': {'required': True},
        'read_io_gi_b': {'required': True},
        'write_io_gi_b': {'required': True},
        'num_succeeded_tasks': {'required': True},
        'num_failed_tasks': {'required': True},
        'num_task_retries': {'required': True},
        'wait_time': {'required': True},
    }

    _attribute_map = {
        'url': {'key': 'url', 'type': 'str'},
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'last_update_time': {'key': 'lastUpdateTime', 'type': 'iso-8601'},
        'user_cpu_time': {'key': 'userCPUTime', 'type': 'duration'},
        'kernel_cpu_time': {'key': 'kernelCPUTime', 'type': 'duration'},
        'wall_clock_time': {'key': 'wallClockTime', 'type': 'duration'},
        'read_i_ops': {'key': 'readIOps', 'type': 'long'},
        'write_i_ops': {'key': 'writeIOps', 'type': 'long'},
        'read_io_gi_b': {'key': 'readIOGiB', 'type': 'float'},
        'write_io_gi_b': {'key': 'writeIOGiB', 'type': 'float'},
        'num_succeeded_tasks': {'key': 'numSucceededTasks', 'type': 'long'},
        'num_failed_tasks': {'key': 'numFailedTasks', 'type': 'long'},
        'num_task_retries': {'key': 'numTaskRetries', 'type': 'long'},
        'wait_time': {'key': 'waitTime', 'type': 'duration'},
    }

    def __init__(
        self,
        *,
        url: str,
        start_time: datetime.datetime,
        last_update_time: datetime.datetime,
        user_cpu_time: datetime.timedelta,
        kernel_cpu_time: datetime.timedelta,
        wall_clock_time: datetime.timedelta,
        read_i_ops: int,
        write_i_ops: int,
        read_io_gi_b: float,
        write_io_gi_b: float,
        num_succeeded_tasks: int,
        num_failed_tasks: int,
        num_task_retries: int,
        wait_time: datetime.timedelta,
        **kwargs
    ):
        """
        :keyword url: Required. The URL of the statistics.
        :paramtype url: str
        :keyword start_time: Required. The start time of the time range covered by the statistics.
        :paramtype start_time: ~datetime.datetime
        :keyword last_update_time: Required. The time at which the statistics were last updated. All
         statistics are limited to the range between startTime and lastUpdateTime.
        :paramtype last_update_time: ~datetime.datetime
        :keyword user_cpu_time: Required. The total user mode CPU time (summed across all cores and all
         Compute Nodes) consumed by all Tasks in the Job.
        :paramtype user_cpu_time: ~datetime.timedelta
        :keyword kernel_cpu_time: Required. The total kernel mode CPU time (summed across all cores and
         all Compute Nodes) consumed by all Tasks in the Job.
        :paramtype kernel_cpu_time: ~datetime.timedelta
        :keyword wall_clock_time: Required. The wall clock time is the elapsed time from when the Task
         started running on a Compute Node to when it finished (or to the last time the statistics were
         updated, if the Task had not finished by then). If a Task was retried, this includes the wall
         clock time of all the Task retries.
        :paramtype wall_clock_time: ~datetime.timedelta
        :keyword read_i_ops: Required. The total number of disk read operations made by all Tasks in
         the Job.
        :paramtype read_i_ops: long
        :keyword write_i_ops: Required. The total number of disk write operations made by all Tasks in
         the Job.
        :paramtype write_i_ops: long
        :keyword read_io_gi_b: Required. The total amount of data in GiB read from disk by all Tasks in
         the Job.
        :paramtype read_io_gi_b: float
        :keyword write_io_gi_b: Required. The total amount of data in GiB written to disk by all Tasks
         in the Job.
        :paramtype write_io_gi_b: float
        :keyword num_succeeded_tasks: Required. A Task completes successfully if it returns exit code
         0.
        :paramtype num_succeeded_tasks: long
        :keyword num_failed_tasks: Required. A Task fails if it exhausts its maximum retry count
         without returning exit code 0.
        :paramtype num_failed_tasks: long
        :keyword num_task_retries: Required. The total number of retries on all the Tasks in the Job
         during the given time range.
        :paramtype num_task_retries: long
        :keyword wait_time: Required. The wait time for a Task is defined as the elapsed time between
         the creation of the Task and the start of Task execution. (If the Task is retried due to
         failures, the wait time is the time to the most recent Task execution.) This value is only
         reported in the Account lifetime statistics; it is not included in the Job statistics.
        :paramtype wait_time: ~datetime.timedelta
        """
        super(JobStatistics, self).__init__(**kwargs)
        self.url = url
        self.start_time = start_time
        self.last_update_time = last_update_time
        self.user_cpu_time = user_cpu_time
        self.kernel_cpu_time = kernel_cpu_time
        self.wall_clock_time = wall_clock_time
        self.read_i_ops = read_i_ops
        self.write_i_ops = write_i_ops
        self.read_io_gi_b = read_io_gi_b
        self.write_io_gi_b = write_io_gi_b
        self.num_succeeded_tasks = num_succeeded_tasks
        self.num_failed_tasks = num_failed_tasks
        self.num_task_retries = num_task_retries
        self.wait_time = wait_time


class LinuxUserConfiguration(msrest.serialization.Model):
    """Properties used to create a user Account on a Linux Compute Node.

    :ivar uid: The uid and gid properties must be specified together or not at all. If not
     specified the underlying operating system picks the uid.
    :vartype uid: int
    :ivar gid: The uid and gid properties must be specified together or not at all. If not
     specified the underlying operating system picks the gid.
    :vartype gid: int
    :ivar ssh_private_key: The private key must not be password protected. The private key is used
     to automatically configure asymmetric-key based authentication for SSH between Compute Nodes in
     a Linux Pool when the Pool's enableInterNodeCommunication property is true (it is ignored if
     enableInterNodeCommunication is false). It does this by placing the key pair into the user's
     .ssh directory. If not specified, password-less SSH is not configured between Compute Nodes (no
     modification of the user's .ssh directory is done).
    :vartype ssh_private_key: str
    """

    _attribute_map = {
        'uid': {'key': 'uid', 'type': 'int'},
        'gid': {'key': 'gid', 'type': 'int'},
        'ssh_private_key': {'key': 'sshPrivateKey', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        uid: Optional[int] = None,
        gid: Optional[int] = None,
        ssh_private_key: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword uid: The uid and gid properties must be specified together or not at all. If not
         specified the underlying operating system picks the uid.
        :paramtype uid: int
        :keyword gid: The uid and gid properties must be specified together or not at all. If not
         specified the underlying operating system picks the gid.
        :paramtype gid: int
        :keyword ssh_private_key: The private key must not be password protected. The private key is
         used to automatically configure asymmetric-key based authentication for SSH between Compute
         Nodes in a Linux Pool when the Pool's enableInterNodeCommunication property is true (it is
         ignored if enableInterNodeCommunication is false). It does this by placing the key pair into
         the user's .ssh directory. If not specified, password-less SSH is not configured between
         Compute Nodes (no modification of the user's .ssh directory is done).
        :paramtype ssh_private_key: str
        """
        super(LinuxUserConfiguration, self).__init__(**kwargs)
        self.uid = uid
        self.gid = gid
        self.ssh_private_key = ssh_private_key


class MetadataItem(msrest.serialization.Model):
    """The Batch service does not assign any meaning to this metadata; it is solely for the use of user code.

    All required parameters must be populated in order to send to Azure.

    :ivar name: Required. The name of the metadata item.
    :vartype name: str
    :ivar value: Required. The value of the metadata item.
    :vartype value: str
    """

    _validation = {
        'name': {'required': True},
        'value': {'required': True},
    }

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'value': {'key': 'value', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: str,
        value: str,
        **kwargs
    ):
        """
        :keyword name: Required. The name of the metadata item.
        :paramtype name: str
        :keyword value: Required. The value of the metadata item.
        :paramtype value: str
        """
        super(MetadataItem, self).__init__(**kwargs)
        self.name = name
        self.value = value


class MountConfiguration(msrest.serialization.Model):
    """The file system to mount on each node.

    :ivar azure_blob_file_system_configuration: This property is mutually exclusive with all other
     properties.
    :vartype azure_blob_file_system_configuration:
     ~azure-batch.models.AzureBlobFileSystemConfiguration
    :ivar nfs_mount_configuration: This property is mutually exclusive with all other properties.
    :vartype nfs_mount_configuration: ~azure-batch.models.NFSMountConfiguration
    :ivar cifs_mount_configuration: This property is mutually exclusive with all other properties.
    :vartype cifs_mount_configuration: ~azure-batch.models.CIFSMountConfiguration
    :ivar azure_file_share_configuration: This property is mutually exclusive with all other
     properties.
    :vartype azure_file_share_configuration: ~azure-batch.models.AzureFileShareConfiguration
    """

    _attribute_map = {
        'azure_blob_file_system_configuration': {'key': 'azureBlobFileSystemConfiguration', 'type': 'AzureBlobFileSystemConfiguration'},
        'nfs_mount_configuration': {'key': 'nfsMountConfiguration', 'type': 'NFSMountConfiguration'},
        'cifs_mount_configuration': {'key': 'cifsMountConfiguration', 'type': 'CIFSMountConfiguration'},
        'azure_file_share_configuration': {'key': 'azureFileShareConfiguration', 'type': 'AzureFileShareConfiguration'},
    }

    def __init__(
        self,
        *,
        azure_blob_file_system_configuration: Optional["_models.AzureBlobFileSystemConfiguration"] = None,
        nfs_mount_configuration: Optional["_models.NFSMountConfiguration"] = None,
        cifs_mount_configuration: Optional["_models.CIFSMountConfiguration"] = None,
        azure_file_share_configuration: Optional["_models.AzureFileShareConfiguration"] = None,
        **kwargs
    ):
        """
        :keyword azure_blob_file_system_configuration: This property is mutually exclusive with all
         other properties.
        :paramtype azure_blob_file_system_configuration:
         ~azure-batch.models.AzureBlobFileSystemConfiguration
        :keyword nfs_mount_configuration: This property is mutually exclusive with all other
         properties.
        :paramtype nfs_mount_configuration: ~azure-batch.models.NFSMountConfiguration
        :keyword cifs_mount_configuration: This property is mutually exclusive with all other
         properties.
        :paramtype cifs_mount_configuration: ~azure-batch.models.CIFSMountConfiguration
        :keyword azure_file_share_configuration: This property is mutually exclusive with all other
         properties.
        :paramtype azure_file_share_configuration: ~azure-batch.models.AzureFileShareConfiguration
        """
        super(MountConfiguration, self).__init__(**kwargs)
        self.azure_blob_file_system_configuration = azure_blob_file_system_configuration
        self.nfs_mount_configuration = nfs_mount_configuration
        self.cifs_mount_configuration = cifs_mount_configuration
        self.azure_file_share_configuration = azure_file_share_configuration


class MultiInstanceSettings(msrest.serialization.Model):
    """Multi-instance Tasks are commonly used to support MPI Tasks. In the MPI case, if any of the subtasks fail (for example due to exiting with a non-zero exit code) the entire multi-instance Task fails. The multi-instance Task is then terminated and retried, up to its retry limit.

    All required parameters must be populated in order to send to Azure.

    :ivar number_of_instances: If omitted, the default is 1.
    :vartype number_of_instances: int
    :ivar coordination_command_line: Required. A typical coordination command line launches a
     background service and verifies that the service is ready to process inter-node messages.
    :vartype coordination_command_line: str
    :ivar common_resource_files: The difference between common resource files and Task resource
     files is that common resource files are downloaded for all subtasks including the primary,
     whereas Task resource files are downloaded only for the primary. Also note that these resource
     files are not downloaded to the Task working directory, but instead are downloaded to the Task
     root directory (one directory above the working directory).  There is a maximum size for the
     list of resource files.  When the max size is exceeded, the request will fail and the response
     error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must
     be reduced in size. This can be achieved using .zip files, Application Packages, or Docker
     Containers.
    :vartype common_resource_files: list[~azure-batch.models.ResourceFile]
    """

    _validation = {
        'coordination_command_line': {'required': True},
    }

    _attribute_map = {
        'number_of_instances': {'key': 'numberOfInstances', 'type': 'int'},
        'coordination_command_line': {'key': 'coordinationCommandLine', 'type': 'str'},
        'common_resource_files': {'key': 'commonResourceFiles', 'type': '[ResourceFile]'},
    }

    def __init__(
        self,
        *,
        coordination_command_line: str,
        number_of_instances: Optional[int] = None,
        common_resource_files: Optional[List["_models.ResourceFile"]] = None,
        **kwargs
    ):
        """
        :keyword number_of_instances: If omitted, the default is 1.
        :paramtype number_of_instances: int
        :keyword coordination_command_line: Required. A typical coordination command line launches a
         background service and verifies that the service is ready to process inter-node messages.
        :paramtype coordination_command_line: str
        :keyword common_resource_files: The difference between common resource files and Task resource
         files is that common resource files are downloaded for all subtasks including the primary,
         whereas Task resource files are downloaded only for the primary. Also note that these resource
         files are not downloaded to the Task working directory, but instead are downloaded to the Task
         root directory (one directory above the working directory).  There is a maximum size for the
         list of resource files.  When the max size is exceeded, the request will fail and the response
         error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must
         be reduced in size. This can be achieved using .zip files, Application Packages, or Docker
         Containers.
        :paramtype common_resource_files: list[~azure-batch.models.ResourceFile]
        """
        super(MultiInstanceSettings, self).__init__(**kwargs)
        self.number_of_instances = number_of_instances
        self.coordination_command_line = coordination_command_line
        self.common_resource_files = common_resource_files


class NameValuePair(msrest.serialization.Model):
    """Represents a name-value pair.

    :ivar name: The name in the name-value pair.
    :vartype name: str
    :ivar value: The value in the name-value pair.
    :vartype value: str
    """

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'value': {'key': 'value', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        name: Optional[str] = None,
        value: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword name: The name in the name-value pair.
        :paramtype name: str
        :keyword value: The value in the name-value pair.
        :paramtype value: str
        """
        super(NameValuePair, self).__init__(**kwargs)
        self.name = name
        self.value = value


class NetworkConfiguration(msrest.serialization.Model):
    """The network configuration for a Pool.

    :ivar subnet_id: The virtual network must be in the same region and subscription as the Azure
     Batch Account. The specified subnet should have enough free IP addresses to accommodate the
     number of Compute Nodes in the Pool. If the subnet doesn't have enough free IP addresses, the
     Pool will partially allocate Nodes and a resize error will occur. The 'MicrosoftAzureBatch'
     service principal must have the 'Classic Virtual Machine Contributor' Role-Based Access Control
     (RBAC) role for the specified VNet. The specified subnet must allow communication from the
     Azure Batch service to be able to schedule Tasks on the Nodes. This can be verified by checking
     if the specified VNet has any associated Network Security Groups (NSG). If communication to the
     Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of
     the Compute Nodes to unusable. For Pools created with virtualMachineConfiguration only ARM
     virtual networks ('Microsoft.Network/virtualNetworks') are supported, but for Pools created
     with cloudServiceConfiguration both ARM and classic virtual networks are supported. If the
     specified VNet has any associated Network Security Groups (NSG), then a few reserved system
     ports must be enabled for inbound communication. For Pools created with a virtual machine
     configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for
     Windows. For Pools created with a cloud service configuration, enable ports 10100, 20100, and
     30100. Also enable outbound connections to Azure Storage on port 443. For more details see:
     https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
    :vartype subnet_id: str
    :ivar dynamic_v_net_assignment_scope: The scope of dynamic vnet assignment. Known values are:
     "none", "job".
    :vartype dynamic_v_net_assignment_scope: str or ~azure-batch.models.DynamicVNetAssignmentScope
    :ivar endpoint_configuration: Pool endpoint configuration is only supported on Pools with the
     virtualMachineConfiguration property.
    :vartype endpoint_configuration: ~azure-batch.models.PoolEndpointConfiguration
    :ivar public_ip_address_configuration: Public IP configuration property is only supported on
     Pools with the virtualMachineConfiguration property.
    :vartype public_ip_address_configuration: ~azure-batch.models.PublicIPAddressConfiguration
    """

    _attribute_map = {
        'subnet_id': {'key': 'subnetId', 'type': 'str'},
        'dynamic_v_net_assignment_scope': {'key': 'dynamicVNetAssignmentScope', 'type': 'str'},
        'endpoint_configuration': {'key': 'endpointConfiguration', 'type': 'PoolEndpointConfiguration'},
        'public_ip_address_configuration': {'key': 'publicIPAddressConfiguration', 'type': 'PublicIPAddressConfiguration'},
    }

    def __init__(
        self,
        *,
        subnet_id: Optional[str] = None,
        dynamic_v_net_assignment_scope: Optional[Union[str, "_models.DynamicVNetAssignmentScope"]] = None,
        endpoint_configuration: Optional["_models.PoolEndpointConfiguration"] = None,
        public_ip_address_configuration: Optional["_models.PublicIPAddressConfiguration"] = None,
        **kwargs
    ):
        """
        :keyword subnet_id: The virtual network must be in the same region and subscription as the
         Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate
         the number of Compute Nodes in the Pool. If the subnet doesn't have enough free IP addresses,
         the Pool will partially allocate Nodes and a resize error will occur. The 'MicrosoftAzureBatch'
         service principal must have the 'Classic Virtual Machine Contributor' Role-Based Access Control
         (RBAC) role for the specified VNet. The specified subnet must allow communication from the
         Azure Batch service to be able to schedule Tasks on the Nodes. This can be verified by checking
         if the specified VNet has any associated Network Security Groups (NSG). If communication to the
         Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of
         the Compute Nodes to unusable. For Pools created with virtualMachineConfiguration only ARM
         virtual networks ('Microsoft.Network/virtualNetworks') are supported, but for Pools created
         with cloudServiceConfiguration both ARM and classic virtual networks are supported. If the
         specified VNet has any associated Network Security Groups (NSG), then a few reserved system
         ports must be enabled for inbound communication. For Pools created with a virtual machine
         configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for
         Windows. For Pools created with a cloud service configuration, enable ports 10100, 20100, and
         30100. Also enable outbound connections to Azure Storage on port 443. For more details see:
         https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
        :paramtype subnet_id: str
        :keyword dynamic_v_net_assignment_scope: The scope of dynamic vnet assignment. Known values
         are: "none", "job".
        :paramtype dynamic_v_net_assignment_scope: str or
         ~azure-batch.models.DynamicVNetAssignmentScope
        :keyword endpoint_configuration: Pool endpoint configuration is only supported on Pools with
         the virtualMachineConfiguration property.
        :paramtype endpoint_configuration: ~azure-batch.models.PoolEndpointConfiguration
        :keyword public_ip_address_configuration: Public IP configuration property is only supported on
         Pools with the virtualMachineConfiguration property.
        :paramtype public_ip_address_configuration: ~azure-batch.models.PublicIPAddressConfiguration
        """
        super(NetworkConfiguration, self).__init__(**kwargs)
        self.subnet_id = subnet_id
        self.dynamic_v_net_assignment_scope = dynamic_v_net_assignment_scope
        self.endpoint_configuration = endpoint_configuration
        self.public_ip_address_configuration = public_ip_address_configuration


class NetworkSecurityGroupRule(msrest.serialization.Model):
    """A network security group rule to apply to an inbound endpoint.

    All required parameters must be populated in order to send to Azure.

    :ivar priority: Required. Priorities within a Pool must be unique and are evaluated in order of
     priority. The lower the number the higher the priority. For example, rules could be specified
     with order numbers of 150, 250, and 350. The rule with the order number of 150 takes precedence
     over the rule that has an order of 250. Allowed priorities are 150 to 4096. If any reserved or
     duplicate values are provided the request fails with HTTP status code 400.
    :vartype priority: int
    :ivar access: Required. The action that should be taken for a specified IP address, subnet
     range or tag. Known values are: "allow", "deny".
    :vartype access: str or ~azure-batch.models.NetworkSecurityGroupRuleAccess
    :ivar source_address_prefix: Required. Valid values are a single IP address (i.e. 10.10.10.10),
     IP subnet (i.e. 192.168.1.0/24), default tag, or * (for all addresses).  If any other values
     are provided the request fails with HTTP status code 400.
    :vartype source_address_prefix: str
    :ivar source_port_ranges: Valid values are '\ *' (for all ports 0 - 65535), a specific port
     (i.e. 22), or a port range (i.e. 100-200). The ports must be in the range of 0 to 65535. Each
     entry in this collection must not overlap any other entry (either a range or an individual
     port). If any other values are provided the request fails with HTTP status code 400. The
     default value is '*\ '.
    :vartype source_port_ranges: list[str]
    """

    _validation = {
        'priority': {'required': True},
        'access': {'required': True},
        'source_address_prefix': {'required': True},
    }

    _attribute_map = {
        'priority': {'key': 'priority', 'type': 'int'},
        'access': {'key': 'access', 'type': 'str'},
        'source_address_prefix': {'key': 'sourceAddressPrefix', 'type': 'str'},
        'source_port_ranges': {'key': 'sourcePortRanges', 'type': '[str]'},
    }

    def __init__(
        self,
        *,
        priority: int,
        access: Union[str, "_models.NetworkSecurityGroupRuleAccess"],
        source_address_prefix: str,
        source_port_ranges: Optional[List[str]] = None,
        **kwargs
    ):
        """
        :keyword priority: Required. Priorities within a Pool must be unique and are evaluated in order
         of priority. The lower the number the higher the priority. For example, rules could be
         specified with order numbers of 150, 250, and 350. The rule with the order number of 150 takes
         precedence over the rule that has an order of 250. Allowed priorities are 150 to 4096. If any
         reserved or duplicate values are provided the request fails with HTTP status code 400.
        :paramtype priority: int
        :keyword access: Required. The action that should be taken for a specified IP address, subnet
         range or tag. Known values are: "allow", "deny".
        :paramtype access: str or ~azure-batch.models.NetworkSecurityGroupRuleAccess
        :keyword source_address_prefix: Required. Valid values are a single IP address (i.e.
         10.10.10.10), IP subnet (i.e. 192.168.1.0/24), default tag, or * (for all addresses).  If any
         other values are provided the request fails with HTTP status code 400.
        :paramtype source_address_prefix: str
        :keyword source_port_ranges: Valid values are '\ *' (for all ports 0 - 65535), a specific port
         (i.e. 22), or a port range (i.e. 100-200). The ports must be in the range of 0 to 65535. Each
         entry in this collection must not overlap any other entry (either a range or an individual
         port). If any other values are provided the request fails with HTTP status code 400. The
         default value is '*\ '.
        :paramtype source_port_ranges: list[str]
        """
        super(NetworkSecurityGroupRule, self).__init__(**kwargs)
        self.priority = priority
        self.access = access
        self.source_address_prefix = source_address_prefix
        self.source_port_ranges = source_port_ranges


class NFSMountConfiguration(msrest.serialization.Model):
    """Information used to connect to an NFS file system.

    All required parameters must be populated in order to send to Azure.

    :ivar source: Required. The URI of the file system to mount.
    :vartype source: str
    :ivar relative_mount_path: Required. All file systems are mounted relative to the Batch mounts
     directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
    :vartype relative_mount_path: str
    :ivar mount_options: These are 'net use' options in Windows and 'mount' options in Linux.
    :vartype mount_options: str
    """

    _validation = {
        'source': {'required': True},
        'relative_mount_path': {'required': True},
    }

    _attribute_map = {
        'source': {'key': 'source', 'type': 'str'},
        'relative_mount_path': {'key': 'relativeMountPath', 'type': 'str'},
        'mount_options': {'key': 'mountOptions', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        source: str,
        relative_mount_path: str,
        mount_options: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword source: Required. The URI of the file system to mount.
        :paramtype source: str
        :keyword relative_mount_path: Required. All file systems are mounted relative to the Batch
         mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        :paramtype relative_mount_path: str
        :keyword mount_options: These are 'net use' options in Windows and 'mount' options in Linux.
        :paramtype mount_options: str
        """
        super(NFSMountConfiguration, self).__init__(**kwargs)
        self.source = source
        self.relative_mount_path = relative_mount_path
        self.mount_options = mount_options


class NodeAgentInformation(msrest.serialization.Model):
    """The Batch Compute Node agent is a program that runs on each Compute Node in the Pool and provides Batch capability on the Compute Node.

    All required parameters must be populated in order to send to Azure.

    :ivar version: Required. This version number can be checked against the Compute Node agent
     release notes located at
     https://github.com/Azure/Batch/blob/master/changelogs/nodeagent/CHANGELOG.md.
    :vartype version: str
    :ivar last_update_time: Required. This is the most recent time that the Compute Node agent was
     updated to a new version.
    :vartype last_update_time: ~datetime.datetime
    """

    _validation = {
        'version': {'required': True},
        'last_update_time': {'required': True},
    }

    _attribute_map = {
        'version': {'key': 'version', 'type': 'str'},
        'last_update_time': {'key': 'lastUpdateTime', 'type': 'iso-8601'},
    }

    def __init__(
        self,
        *,
        version: str,
        last_update_time: datetime.datetime,
        **kwargs
    ):
        """
        :keyword version: Required. This version number can be checked against the Compute Node agent
         release notes located at
         https://github.com/Azure/Batch/blob/master/changelogs/nodeagent/CHANGELOG.md.
        :paramtype version: str
        :keyword last_update_time: Required. This is the most recent time that the Compute Node agent
         was updated to a new version.
        :paramtype last_update_time: ~datetime.datetime
        """
        super(NodeAgentInformation, self).__init__(**kwargs)
        self.version = version
        self.last_update_time = last_update_time


class NodeCounts(msrest.serialization.Model):
    """The number of Compute Nodes in each Compute Node state.

    All required parameters must be populated in order to send to Azure.

    :ivar creating: Required. The number of Compute Nodes in the creating state.
    :vartype creating: int
    :ivar idle: Required. The number of Compute Nodes in the idle state.
    :vartype idle: int
    :ivar offline: Required. The number of Compute Nodes in the offline state.
    :vartype offline: int
    :ivar preempted: Required. The number of Compute Nodes in the preempted state.
    :vartype preempted: int
    :ivar rebooting: Required. The count of Compute Nodes in the rebooting state.
    :vartype rebooting: int
    :ivar reimaging: Required. The number of Compute Nodes in the reimaging state.
    :vartype reimaging: int
    :ivar running: Required. The number of Compute Nodes in the running state.
    :vartype running: int
    :ivar starting: Required. The number of Compute Nodes in the starting state.
    :vartype starting: int
    :ivar start_task_failed: Required. The number of Compute Nodes in the startTaskFailed state.
    :vartype start_task_failed: int
    :ivar leaving_pool: Required. The number of Compute Nodes in the leavingPool state.
    :vartype leaving_pool: int
    :ivar unknown: Required. The number of Compute Nodes in the unknown state.
    :vartype unknown: int
    :ivar unusable: Required. The number of Compute Nodes in the unusable state.
    :vartype unusable: int
    :ivar waiting_for_start_task: Required. The number of Compute Nodes in the waitingForStartTask
     state.
    :vartype waiting_for_start_task: int
    :ivar total: Required. The total number of Compute Nodes.
    :vartype total: int
    """

    _validation = {
        'creating': {'required': True},
        'idle': {'required': True},
        'offline': {'required': True},
        'preempted': {'required': True},
        'rebooting': {'required': True},
        'reimaging': {'required': True},
        'running': {'required': True},
        'starting': {'required': True},
        'start_task_failed': {'required': True},
        'leaving_pool': {'required': True},
        'unknown': {'required': True},
        'unusable': {'required': True},
        'waiting_for_start_task': {'required': True},
        'total': {'required': True},
    }

    _attribute_map = {
        'creating': {'key': 'creating', 'type': 'int'},
        'idle': {'key': 'idle', 'type': 'int'},
        'offline': {'key': 'offline', 'type': 'int'},
        'preempted': {'key': 'preempted', 'type': 'int'},
        'rebooting': {'key': 'rebooting', 'type': 'int'},
        'reimaging': {'key': 'reimaging', 'type': 'int'},
        'running': {'key': 'running', 'type': 'int'},
        'starting': {'key': 'starting', 'type': 'int'},
        'start_task_failed': {'key': 'startTaskFailed', 'type': 'int'},
        'leaving_pool': {'key': 'leavingPool', 'type': 'int'},
        'unknown': {'key': 'unknown', 'type': 'int'},
        'unusable': {'key': 'unusable', 'type': 'int'},
        'waiting_for_start_task': {'key': 'waitingForStartTask', 'type': 'int'},
        'total': {'key': 'total', 'type': 'int'},
    }

    def __init__(
        self,
        *,
        creating: int,
        idle: int,
        offline: int,
        preempted: int,
        rebooting: int,
        reimaging: int,
        running: int,
        starting: int,
        start_task_failed: int,
        leaving_pool: int,
        unknown: int,
        unusable: int,
        waiting_for_start_task: int,
        total: int,
        **kwargs
    ):
        """
        :keyword creating: Required. The number of Compute Nodes in the creating state.
        :paramtype creating: int
        :keyword idle: Required. The number of Compute Nodes in the idle state.
        :paramtype idle: int
        :keyword offline: Required. The number of Compute Nodes in the offline state.
        :paramtype offline: int
        :keyword preempted: Required. The number of Compute Nodes in the preempted state.
        :paramtype preempted: int
        :keyword rebooting: Required. The count of Compute Nodes in the rebooting state.
        :paramtype rebooting: int
        :keyword reimaging: Required. The number of Compute Nodes in the reimaging state.
        :paramtype reimaging: int
        :keyword running: Required. The number of Compute Nodes in the running state.
        :paramtype running: int
        :keyword starting: Required. The number of Compute Nodes in the starting state.
        :paramtype starting: int
        :keyword start_task_failed: Required. The number of Compute Nodes in the startTaskFailed state.
        :paramtype start_task_failed: int
        :keyword leaving_pool: Required. The number of Compute Nodes in the leavingPool state.
        :paramtype leaving_pool: int
        :keyword unknown: Required. The number of Compute Nodes in the unknown state.
        :paramtype unknown: int
        :keyword unusable: Required. The number of Compute Nodes in the unusable state.
        :paramtype unusable: int
        :keyword waiting_for_start_task: Required. The number of Compute Nodes in the
         waitingForStartTask state.
        :paramtype waiting_for_start_task: int
        :keyword total: Required. The total number of Compute Nodes.
        :paramtype total: int
        """
        super(NodeCounts, self).__init__(**kwargs)
        self.creating = creating
        self.idle = idle
        self.offline = offline
        self.preempted = preempted
        self.rebooting = rebooting
        self.reimaging = reimaging
        self.running = running
        self.starting = starting
        self.start_task_failed = start_task_failed
        self.leaving_pool = leaving_pool
        self.unknown = unknown
        self.unusable = unusable
        self.waiting_for_start_task = waiting_for_start_task
        self.total = total


class NodeDisableSchedulingParameters(msrest.serialization.Model):
    """Options for disabling scheduling on a Compute Node.

    :ivar node_disable_scheduling_option: The default value is requeue. Known values are:
     "requeue", "terminate", "taskcompletion".
    :vartype node_disable_scheduling_option: str or
     ~azure-batch.models.DisableComputeNodeSchedulingOption
    """

    _attribute_map = {
        'node_disable_scheduling_option': {'key': 'nodeDisableSchedulingOption', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        node_disable_scheduling_option: Optional[Union[str, "_models.DisableComputeNodeSchedulingOption"]] = None,
        **kwargs
    ):
        """
        :keyword node_disable_scheduling_option: The default value is requeue. Known values are:
         "requeue", "terminate", "taskcompletion".
        :paramtype node_disable_scheduling_option: str or
         ~azure-batch.models.DisableComputeNodeSchedulingOption
        """
        super(NodeDisableSchedulingParameters, self).__init__(**kwargs)
        self.node_disable_scheduling_option = node_disable_scheduling_option


class NodeFile(msrest.serialization.Model):
    """Information about a file or directory on a Compute Node.

    :ivar name: The file path.
    :vartype name: str
    :ivar url: The URL of the file.
    :vartype url: str
    :ivar is_directory: Whether the object represents a directory.
    :vartype is_directory: bool
    :ivar properties: The properties of a file on a Compute Node.
    :vartype properties: ~azure-batch.models.FileProperties
    """

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'url': {'key': 'url', 'type': 'str'},
        'is_directory': {'key': 'isDirectory', 'type': 'bool'},
        'properties': {'key': 'properties', 'type': 'FileProperties'},
    }

    def __init__(
        self,
        *,
        name: Optional[str] = None,
        url: Optional[str] = None,
        is_directory: Optional[bool] = None,
        properties: Optional["_models.FileProperties"] = None,
        **kwargs
    ):
        """
        :keyword name: The file path.
        :paramtype name: str
        :keyword url: The URL of the file.
        :paramtype url: str
        :keyword is_directory: Whether the object represents a directory.
        :paramtype is_directory: bool
        :keyword properties: The properties of a file on a Compute Node.
        :paramtype properties: ~azure-batch.models.FileProperties
        """
        super(NodeFile, self).__init__(**kwargs)
        self.name = name
        self.url = url
        self.is_directory = is_directory
        self.properties = properties


class NodeFileListResult(msrest.serialization.Model):
    """The result of listing the files on a Compute Node, or the files associated with a Task on a Compute Node.

    :ivar value: The list of files.
    :vartype value: list[~azure-batch.models.NodeFile]
    :ivar odata_next_link: The URL to get the next set of results.
    :vartype odata_next_link: str
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[NodeFile]'},
        'odata_next_link': {'key': 'odata\\.nextLink', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["_models.NodeFile"]] = None,
        odata_next_link: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword value: The list of files.
        :paramtype value: list[~azure-batch.models.NodeFile]
        :keyword odata_next_link: The URL to get the next set of results.
        :paramtype odata_next_link: str
        """
        super(NodeFileListResult, self).__init__(**kwargs)
        self.value = value
        self.odata_next_link = odata_next_link


class NodePlacementConfiguration(msrest.serialization.Model):
    """For regional placement, nodes in the pool will be allocated in the same region. For zonal placement, nodes in the pool will be spread across different zones with best effort balancing.

    :ivar policy: Allocation policy used by Batch Service to provision the nodes. If not specified,
     Batch will use the regional policy. Known values are: "regional", "zonal".
    :vartype policy: str or ~azure-batch.models.NodePlacementPolicyType
    """

    _attribute_map = {
        'policy': {'key': 'policy', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        policy: Optional[Union[str, "_models.NodePlacementPolicyType"]] = None,
        **kwargs
    ):
        """
        :keyword policy: Allocation policy used by Batch Service to provision the nodes. If not
         specified, Batch will use the regional policy. Known values are: "regional", "zonal".
        :paramtype policy: str or ~azure-batch.models.NodePlacementPolicyType
        """
        super(NodePlacementConfiguration, self).__init__(**kwargs)
        self.policy = policy


class NodeRebootParameters(msrest.serialization.Model):
    """Options for rebooting a Compute Node.

    :ivar node_reboot_option: The default value is requeue. Known values are: "requeue",
     "terminate", "taskcompletion", "retaineddata".
    :vartype node_reboot_option: str or ~azure-batch.models.ComputeNodeRebootOption
    """

    _attribute_map = {
        'node_reboot_option': {'key': 'nodeRebootOption', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        node_reboot_option: Optional[Union[str, "_models.ComputeNodeRebootOption"]] = None,
        **kwargs
    ):
        """
        :keyword node_reboot_option: The default value is requeue. Known values are: "requeue",
         "terminate", "taskcompletion", "retaineddata".
        :paramtype node_reboot_option: str or ~azure-batch.models.ComputeNodeRebootOption
        """
        super(NodeRebootParameters, self).__init__(**kwargs)
        self.node_reboot_option = node_reboot_option


class NodeReimageParameters(msrest.serialization.Model):
    """Options for reimaging a Compute Node.

    :ivar node_reimage_option: The default value is requeue. Known values are: "requeue",
     "terminate", "taskcompletion", "retaineddata".
    :vartype node_reimage_option: str or ~azure-batch.models.ComputeNodeReimageOption
    """

    _attribute_map = {
        'node_reimage_option': {'key': 'nodeReimageOption', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        node_reimage_option: Optional[Union[str, "_models.ComputeNodeReimageOption"]] = None,
        **kwargs
    ):
        """
        :keyword node_reimage_option: The default value is requeue. Known values are: "requeue",
         "terminate", "taskcompletion", "retaineddata".
        :paramtype node_reimage_option: str or ~azure-batch.models.ComputeNodeReimageOption
        """
        super(NodeReimageParameters, self).__init__(**kwargs)
        self.node_reimage_option = node_reimage_option


class NodeRemoveParameters(msrest.serialization.Model):
    """Options for removing Compute Nodes from a Pool.

    All required parameters must be populated in order to send to Azure.

    :ivar node_list: Required. A maximum of 100 nodes may be removed per request.
    :vartype node_list: list[str]
    :ivar resize_timeout: The default value is 15 minutes. The minimum value is 5 minutes. If you
     specify a value less than 5 minutes, the Batch service returns an error; if you are calling the
     REST API directly, the HTTP status code is 400 (Bad Request).
    :vartype resize_timeout: ~datetime.timedelta
    :ivar node_deallocation_option: The default value is requeue. Known values are: "requeue",
     "terminate", "taskcompletion", "retaineddata".
    :vartype node_deallocation_option: str or ~azure-batch.models.ComputeNodeDeallocationOption
    """

    _validation = {
        'node_list': {'required': True, 'max_items': 100, 'min_items': 0},
    }

    _attribute_map = {
        'node_list': {'key': 'nodeList', 'type': '[str]'},
        'resize_timeout': {'key': 'resizeTimeout', 'type': 'duration'},
        'node_deallocation_option': {'key': 'nodeDeallocationOption', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        node_list: List[str],
        resize_timeout: Optional[datetime.timedelta] = None,
        node_deallocation_option: Optional[Union[str, "_models.ComputeNodeDeallocationOption"]] = None,
        **kwargs
    ):
        """
        :keyword node_list: Required. A maximum of 100 nodes may be removed per request.
        :paramtype node_list: list[str]
        :keyword resize_timeout: The default value is 15 minutes. The minimum value is 5 minutes. If
         you specify a value less than 5 minutes, the Batch service returns an error; if you are calling
         the REST API directly, the HTTP status code is 400 (Bad Request).
        :paramtype resize_timeout: ~datetime.timedelta
        :keyword node_deallocation_option: The default value is requeue. Known values are: "requeue",
         "terminate", "taskcompletion", "retaineddata".
        :paramtype node_deallocation_option: str or ~azure-batch.models.ComputeNodeDeallocationOption
        """
        super(NodeRemoveParameters, self).__init__(**kwargs)
        self.node_list = node_list
        self.resize_timeout = resize_timeout
        self.node_deallocation_option = node_deallocation_option


class NodeUpdateUserParameters(msrest.serialization.Model):
    """The set of changes to be made to a user Account on a Compute Node.

    :ivar password: The password is required for Windows Compute Nodes (those created with
     'cloudServiceConfiguration', or created with 'virtualMachineConfiguration' using a Windows
     Image reference). For Linux Compute Nodes, the password can optionally be specified along with
     the sshPublicKey property. If omitted, any existing password is removed.
    :vartype password: str
    :ivar expiry_time: If omitted, the default is 1 day from the current time. For Linux Compute
     Nodes, the expiryTime has a precision up to a day.
    :vartype expiry_time: ~datetime.datetime
    :ivar ssh_public_key: The public key should be compatible with OpenSSH encoding and should be
     base 64 encoded. This property can be specified only for Linux Compute Nodes. If this is
     specified for a Windows Compute Node, then the Batch service rejects the request; if you are
     calling the REST API directly, the HTTP status code is 400 (Bad Request). If omitted, any
     existing SSH public key is removed.
    :vartype ssh_public_key: str
    """

    _attribute_map = {
        'password': {'key': 'password', 'type': 'str'},
        'expiry_time': {'key': 'expiryTime', 'type': 'iso-8601'},
        'ssh_public_key': {'key': 'sshPublicKey', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        password: Optional[str] = None,
        expiry_time: Optional[datetime.datetime] = None,
        ssh_public_key: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword password: The password is required for Windows Compute Nodes (those created with
         'cloudServiceConfiguration', or created with 'virtualMachineConfiguration' using a Windows
         Image reference). For Linux Compute Nodes, the password can optionally be specified along with
         the sshPublicKey property. If omitted, any existing password is removed.
        :paramtype password: str
        :keyword expiry_time: If omitted, the default is 1 day from the current time. For Linux Compute
         Nodes, the expiryTime has a precision up to a day.
        :paramtype expiry_time: ~datetime.datetime
        :keyword ssh_public_key: The public key should be compatible with OpenSSH encoding and should
         be base 64 encoded. This property can be specified only for Linux Compute Nodes. If this is
         specified for a Windows Compute Node, then the Batch service rejects the request; if you are
         calling the REST API directly, the HTTP status code is 400 (Bad Request). If omitted, any
         existing SSH public key is removed.
        :paramtype ssh_public_key: str
        """
        super(NodeUpdateUserParameters, self).__init__(**kwargs)
        self.password = password
        self.expiry_time = expiry_time
        self.ssh_public_key = ssh_public_key


class NodeVMExtension(msrest.serialization.Model):
    """The configuration for virtual machine extension instance view.

    :ivar provisioning_state: The provisioning state of the virtual machine extension.
    :vartype provisioning_state: str
    :ivar vm_extension: The configuration for virtual machine extensions.
    :vartype vm_extension: ~azure-batch.models.VMExtension
    :ivar instance_view: The vm extension instance view.
    :vartype instance_view: ~azure-batch.models.VMExtensionInstanceView
    """

    _attribute_map = {
        'provisioning_state': {'key': 'provisioningState', 'type': 'str'},
        'vm_extension': {'key': 'vmExtension', 'type': 'VMExtension'},
        'instance_view': {'key': 'instanceView', 'type': 'VMExtensionInstanceView'},
    }

    def __init__(
        self,
        *,
        provisioning_state: Optional[str] = None,
        vm_extension: Optional["_models.VMExtension"] = None,
        instance_view: Optional["_models.VMExtensionInstanceView"] = None,
        **kwargs
    ):
        """
        :keyword provisioning_state: The provisioning state of the virtual machine extension.
        :paramtype provisioning_state: str
        :keyword vm_extension: The configuration for virtual machine extensions.
        :paramtype vm_extension: ~azure-batch.models.VMExtension
        :keyword instance_view: The vm extension instance view.
        :paramtype instance_view: ~azure-batch.models.VMExtensionInstanceView
        """
        super(NodeVMExtension, self).__init__(**kwargs)
        self.provisioning_state = provisioning_state
        self.vm_extension = vm_extension
        self.instance_view = instance_view


class NodeVMExtensionList(msrest.serialization.Model):
    """The result of listing the Compute Node extensions in a Node.

    :ivar value: The list of Compute Node extensions.
    :vartype value: list[~azure-batch.models.NodeVMExtension]
    :ivar odata_next_link: The URL to get the next set of results.
    :vartype odata_next_link: str
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[NodeVMExtension]'},
        'odata_next_link': {'key': 'odata\\.nextLink', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["_models.NodeVMExtension"]] = None,
        odata_next_link: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword value: The list of Compute Node extensions.
        :paramtype value: list[~azure-batch.models.NodeVMExtension]
        :keyword odata_next_link: The URL to get the next set of results.
        :paramtype odata_next_link: str
        """
        super(NodeVMExtensionList, self).__init__(**kwargs)
        self.value = value
        self.odata_next_link = odata_next_link


class OSDisk(msrest.serialization.Model):
    """Settings for the operating system disk of the compute node (VM).

    :ivar ephemeral_os_disk_settings: Specifies the ephemeral Disk Settings for the operating
     system disk used by the compute node (VM).
    :vartype ephemeral_os_disk_settings: ~azure-batch.models.DiffDiskSettings
    """

    _attribute_map = {
        'ephemeral_os_disk_settings': {'key': 'ephemeralOSDiskSettings', 'type': 'DiffDiskSettings'},
    }

    def __init__(
        self,
        *,
        ephemeral_os_disk_settings: Optional["_models.DiffDiskSettings"] = None,
        **kwargs
    ):
        """
        :keyword ephemeral_os_disk_settings: Specifies the ephemeral Disk Settings for the operating
         system disk used by the compute node (VM).
        :paramtype ephemeral_os_disk_settings: ~azure-batch.models.DiffDiskSettings
        """
        super(OSDisk, self).__init__(**kwargs)
        self.ephemeral_os_disk_settings = ephemeral_os_disk_settings


class OutputFile(msrest.serialization.Model):
    """On every file uploads, Batch service writes two log files to the compute node, 'fileuploadout.txt' and 'fileuploaderr.txt'. These log files are used to learn more about a specific failure.

    All required parameters must be populated in order to send to Azure.

    :ivar file_pattern: Required. Both relative and absolute paths are supported. Relative paths
     are relative to the Task working directory. The following wildcards are supported: * matches 0
     or more characters (for example pattern abc* would match abc or abcdef), ** matches any
     directory, ? matches any single character, [abc] matches one character in the brackets, and
     [a-c] matches one character in the range. Brackets can include a negation to match any
     character not specified (for example [!abc] matches any character but a, b, or c). If a file
     name starts with "." it is ignored by default but may be matched by specifying it explicitly
     (for example *.gif will not match .a.gif, but .*.gif will). A simple example: **\ *.txt matches
     any file that does not start in '.' and ends with .txt in the Task working directory or any
     subdirectory. If the filename contains a wildcard character it can be escaped using brackets
     (for example abc[\ *] would match a file named abc*\ ). Note that both and / are treated as
     directory separators on Windows, but only / is on Linux. Environment variables (%var% on
     Windows or $var on Linux) are expanded prior to the pattern being applied.
    :vartype file_pattern: str
    :ivar destination: Required. The destination to which a file should be uploaded.
    :vartype destination: ~azure-batch.models.OutputFileDestination
    :ivar upload_options: Required. Details about an output file upload operation, including under
     what conditions to perform the upload.
    :vartype upload_options: ~azure-batch.models.OutputFileUploadOptions
    """

    _validation = {
        'file_pattern': {'required': True},
        'destination': {'required': True},
        'upload_options': {'required': True},
    }

    _attribute_map = {
        'file_pattern': {'key': 'filePattern', 'type': 'str'},
        'destination': {'key': 'destination', 'type': 'OutputFileDestination'},
        'upload_options': {'key': 'uploadOptions', 'type': 'OutputFileUploadOptions'},
    }

    def __init__(
        self,
        *,
        file_pattern: str,
        destination: "_models.OutputFileDestination",
        upload_options: "_models.OutputFileUploadOptions",
        **kwargs
    ):
        """
        :keyword file_pattern: Required. Both relative and absolute paths are supported. Relative paths
         are relative to the Task working directory. The following wildcards are supported: * matches 0
         or more characters (for example pattern abc* would match abc or abcdef), ** matches any
         directory, ? matches any single character, [abc] matches one character in the brackets, and
         [a-c] matches one character in the range. Brackets can include a negation to match any
         character not specified (for example [!abc] matches any character but a, b, or c). If a file
         name starts with "." it is ignored by default but may be matched by specifying it explicitly
         (for example *.gif will not match .a.gif, but .*.gif will). A simple example: **\ *.txt matches
         any file that does not start in '.' and ends with .txt in the Task working directory or any
         subdirectory. If the filename contains a wildcard character it can be escaped using brackets
         (for example abc[\ *] would match a file named abc*\ ). Note that both and / are treated as
         directory separators on Windows, but only / is on Linux. Environment variables (%var% on
         Windows or $var on Linux) are expanded prior to the pattern being applied.
        :paramtype file_pattern: str
        :keyword destination: Required. The destination to which a file should be uploaded.
        :paramtype destination: ~azure-batch.models.OutputFileDestination
        :keyword upload_options: Required. Details about an output file upload operation, including
         under what conditions to perform the upload.
        :paramtype upload_options: ~azure-batch.models.OutputFileUploadOptions
        """
        super(OutputFile, self).__init__(**kwargs)
        self.file_pattern = file_pattern
        self.destination = destination
        self.upload_options = upload_options


class OutputFileBlobContainerDestination(msrest.serialization.Model):
    """Specifies a file upload destination within an Azure blob storage container.

    All required parameters must be populated in order to send to Azure.

    :ivar path: If filePattern refers to a specific file (i.e. contains no wildcards), then path is
     the name of the blob to which to upload that file. If filePattern contains one or more
     wildcards (and therefore may match multiple files), then path is the name of the blob virtual
     directory (which is prepended to each blob name) to which to upload the file(s). If omitted,
     file(s) are uploaded to the root of the container with a blob name matching their file name.
    :vartype path: str
    :ivar container_url: Required. If not using a managed identity, the URL must include a Shared
     Access Signature (SAS) granting write permissions to the container.
    :vartype container_url: str
    :ivar identity_reference: The identity must have write access to the Azure Blob Storage
     container.
    :vartype identity_reference: ~azure-batch.models.ComputeNodeIdentityReference
    :ivar upload_headers: These headers will be specified when uploading files to Azure Storage.
     For more information, see `Request Headers (All Blob Types)
     <https://docs.microsoft.com/rest/api/storageservices/put-blob#request-headers-all-blob-types>`_.
    :vartype upload_headers: list[~azure-batch.models.HttpHeader]
    """

    _validation = {
        'container_url': {'required': True},
    }

    _attribute_map = {
        'path': {'key': 'path', 'type': 'str'},
        'container_url': {'key': 'containerUrl', 'type': 'str'},
        'identity_reference': {'key': 'identityReference', 'type': 'ComputeNodeIdentityReference'},
        'upload_headers': {'key': 'uploadHeaders', 'type': '[HttpHeader]'},
    }

    def __init__(
        self,
        *,
        container_url: str,
        path: Optional[str] = None,
        identity_reference: Optional["_models.ComputeNodeIdentityReference"] = None,
        upload_headers: Optional[List["_models.HttpHeader"]] = None,
        **kwargs
    ):
        """
        :keyword path: If filePattern refers to a specific file (i.e. contains no wildcards), then path
         is the name of the blob to which to upload that file. If filePattern contains one or more
         wildcards (and therefore may match multiple files), then path is the name of the blob virtual
         directory (which is prepended to each blob name) to which to upload the file(s). If omitted,
         file(s) are uploaded to the root of the container with a blob name matching their file name.
        :paramtype path: str
        :keyword container_url: Required. If not using a managed identity, the URL must include a
         Shared Access Signature (SAS) granting write permissions to the container.
        :paramtype container_url: str
        :keyword identity_reference: The identity must have write access to the Azure Blob Storage
         container.
        :paramtype identity_reference: ~azure-batch.models.ComputeNodeIdentityReference
        :keyword upload_headers: These headers will be specified when uploading files to Azure Storage.
         For more information, see `Request Headers (All Blob Types)
         <https://docs.microsoft.com/rest/api/storageservices/put-blob#request-headers-all-blob-types>`_.
        :paramtype upload_headers: list[~azure-batch.models.HttpHeader]
        """
        super(OutputFileBlobContainerDestination, self).__init__(**kwargs)
        self.path = path
        self.container_url = container_url
        self.identity_reference = identity_reference
        self.upload_headers = upload_headers


class OutputFileDestination(msrest.serialization.Model):
    """The destination to which a file should be uploaded.

    :ivar container: Specifies a file upload destination within an Azure blob storage container.
    :vartype container: ~azure-batch.models.OutputFileBlobContainerDestination
    """

    _attribute_map = {
        'container': {'key': 'container', 'type': 'OutputFileBlobContainerDestination'},
    }

    def __init__(
        self,
        *,
        container: Optional["_models.OutputFileBlobContainerDestination"] = None,
        **kwargs
    ):
        """
        :keyword container: Specifies a file upload destination within an Azure blob storage container.
        :paramtype container: ~azure-batch.models.OutputFileBlobContainerDestination
        """
        super(OutputFileDestination, self).__init__(**kwargs)
        self.container = container


class OutputFileUploadOptions(msrest.serialization.Model):
    """Details about an output file upload operation, including under what conditions to perform the upload.

    All required parameters must be populated in order to send to Azure.

    :ivar upload_condition: Required. The default is taskcompletion. Known values are:
     "tasksuccess", "taskfailure", "taskcompletion".
    :vartype upload_condition: str or ~azure-batch.models.OutputFileUploadCondition
    """

    _validation = {
        'upload_condition': {'required': True},
    }

    _attribute_map = {
        'upload_condition': {'key': 'uploadCondition', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        upload_condition: Union[str, "_models.OutputFileUploadCondition"],
        **kwargs
    ):
        """
        :keyword upload_condition: Required. The default is taskcompletion. Known values are:
         "tasksuccess", "taskfailure", "taskcompletion".
        :paramtype upload_condition: str or ~azure-batch.models.OutputFileUploadCondition
        """
        super(OutputFileUploadOptions, self).__init__(**kwargs)
        self.upload_condition = upload_condition


class PoolEndpointConfiguration(msrest.serialization.Model):
    """The endpoint configuration for a Pool.

    All required parameters must be populated in order to send to Azure.

    :ivar inbound_nat_pools: Required. The maximum number of inbound NAT Pools per Batch Pool is 5.
     If the maximum number of inbound NAT Pools is exceeded the request fails with HTTP status code
     400. This cannot be specified if the IPAddressProvisioningType is NoPublicIPAddresses.
    :vartype inbound_nat_pools: list[~azure-batch.models.InboundNATPool]
    """

    _validation = {
        'inbound_nat_pools': {'required': True},
    }

    _attribute_map = {
        'inbound_nat_pools': {'key': 'inboundNATPools', 'type': '[InboundNATPool]'},
    }

    def __init__(
        self,
        *,
        inbound_nat_pools: List["_models.InboundNATPool"],
        **kwargs
    ):
        """
        :keyword inbound_nat_pools: Required. The maximum number of inbound NAT Pools per Batch Pool is
         5. If the maximum number of inbound NAT Pools is exceeded the request fails with HTTP status
         code 400. This cannot be specified if the IPAddressProvisioningType is NoPublicIPAddresses.
        :paramtype inbound_nat_pools: list[~azure-batch.models.InboundNATPool]
        """
        super(PoolEndpointConfiguration, self).__init__(**kwargs)
        self.inbound_nat_pools = inbound_nat_pools


class PoolInformation(msrest.serialization.Model):
    """Specifies how a Job should be assigned to a Pool.

    :ivar pool_id: You must ensure that the Pool referenced by this property exists. If the Pool
     does not exist at the time the Batch service tries to schedule a Job, no Tasks for the Job will
     run until you create a Pool with that id. Note that the Batch service will not reject the Job
     request; it will simply not run Tasks until the Pool exists. You must specify either the Pool
     ID or the auto Pool specification, but not both.
    :vartype pool_id: str
    :ivar auto_pool_specification: If auto Pool creation fails, the Batch service moves the Job to
     a completed state, and the Pool creation error is set in the Job's scheduling error property.
     The Batch service manages the lifetime (both creation and, unless keepAlive is specified,
     deletion) of the auto Pool. Any user actions that affect the lifetime of the auto Pool while
     the Job is active will result in unexpected behavior. You must specify either the Pool ID or
     the auto Pool specification, but not both.
    :vartype auto_pool_specification: ~azure-batch.models.AutoPoolSpecification
    """

    _attribute_map = {
        'pool_id': {'key': 'poolId', 'type': 'str'},
        'auto_pool_specification': {'key': 'autoPoolSpecification', 'type': 'AutoPoolSpecification'},
    }

    def __init__(
        self,
        *,
        pool_id: Optional[str] = None,
        auto_pool_specification: Optional["_models.AutoPoolSpecification"] = None,
        **kwargs
    ):
        """
        :keyword pool_id: You must ensure that the Pool referenced by this property exists. If the Pool
         does not exist at the time the Batch service tries to schedule a Job, no Tasks for the Job will
         run until you create a Pool with that id. Note that the Batch service will not reject the Job
         request; it will simply not run Tasks until the Pool exists. You must specify either the Pool
         ID or the auto Pool specification, but not both.
        :paramtype pool_id: str
        :keyword auto_pool_specification: If auto Pool creation fails, the Batch service moves the Job
         to a completed state, and the Pool creation error is set in the Job's scheduling error
         property. The Batch service manages the lifetime (both creation and, unless keepAlive is
         specified, deletion) of the auto Pool. Any user actions that affect the lifetime of the auto
         Pool while the Job is active will result in unexpected behavior. You must specify either the
         Pool ID or the auto Pool specification, but not both.
        :paramtype auto_pool_specification: ~azure-batch.models.AutoPoolSpecification
        """
        super(PoolInformation, self).__init__(**kwargs)
        self.pool_id = pool_id
        self.auto_pool_specification = auto_pool_specification


class PoolListUsageMetricsResult(msrest.serialization.Model):
    """The result of a listing the usage metrics for an Account.

    :ivar value: The Pool usage metrics data.
    :vartype value: list[~azure-batch.models.PoolUsageMetrics]
    :ivar odata_next_link: The URL to get the next set of results.
    :vartype odata_next_link: str
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[PoolUsageMetrics]'},
        'odata_next_link': {'key': 'odata\\.nextLink', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["_models.PoolUsageMetrics"]] = None,
        odata_next_link: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword value: The Pool usage metrics data.
        :paramtype value: list[~azure-batch.models.PoolUsageMetrics]
        :keyword odata_next_link: The URL to get the next set of results.
        :paramtype odata_next_link: str
        """
        super(PoolListUsageMetricsResult, self).__init__(**kwargs)
        self.value = value
        self.odata_next_link = odata_next_link


class PoolNodeCounts(msrest.serialization.Model):
    """The number of Compute Nodes in each state for a Pool.

    All required parameters must be populated in order to send to Azure.

    :ivar pool_id: Required. The ID of the Pool.
    :vartype pool_id: str
    :ivar dedicated: The number of Compute Nodes in each Compute Node state.
    :vartype dedicated: ~azure-batch.models.NodeCounts
    :ivar low_priority: The number of Compute Nodes in each Compute Node state.
    :vartype low_priority: ~azure-batch.models.NodeCounts
    """

    _validation = {
        'pool_id': {'required': True},
    }

    _attribute_map = {
        'pool_id': {'key': 'poolId', 'type': 'str'},
        'dedicated': {'key': 'dedicated', 'type': 'NodeCounts'},
        'low_priority': {'key': 'lowPriority', 'type': 'NodeCounts'},
    }

    def __init__(
        self,
        *,
        pool_id: str,
        dedicated: Optional["_models.NodeCounts"] = None,
        low_priority: Optional["_models.NodeCounts"] = None,
        **kwargs
    ):
        """
        :keyword pool_id: Required. The ID of the Pool.
        :paramtype pool_id: str
        :keyword dedicated: The number of Compute Nodes in each Compute Node state.
        :paramtype dedicated: ~azure-batch.models.NodeCounts
        :keyword low_priority: The number of Compute Nodes in each Compute Node state.
        :paramtype low_priority: ~azure-batch.models.NodeCounts
        """
        super(PoolNodeCounts, self).__init__(**kwargs)
        self.pool_id = pool_id
        self.dedicated = dedicated
        self.low_priority = low_priority


class PoolNodeCountsListResult(msrest.serialization.Model):
    """The result of listing the Compute Node counts in the Account.

    :ivar value: A list of Compute Node counts by Pool.
    :vartype value: list[~azure-batch.models.PoolNodeCounts]
    :ivar odata_next_link: The URL to get the next set of results.
    :vartype odata_next_link: str
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[PoolNodeCounts]'},
        'odata_next_link': {'key': 'odata\\.nextLink', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["_models.PoolNodeCounts"]] = None,
        odata_next_link: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword value: A list of Compute Node counts by Pool.
        :paramtype value: list[~azure-batch.models.PoolNodeCounts]
        :keyword odata_next_link: The URL to get the next set of results.
        :paramtype odata_next_link: str
        """
        super(PoolNodeCountsListResult, self).__init__(**kwargs)
        self.value = value
        self.odata_next_link = odata_next_link


class PoolSpecification(msrest.serialization.Model):
    """Specification for creating a new Pool.

    All required parameters must be populated in order to send to Azure.

    :ivar display_name: The display name need not be unique and can contain any Unicode characters
     up to a maximum length of 1024.
    :vartype display_name: str
    :ivar vm_size: Required. For information about available sizes of virtual machines in Pools,
     see Choose a VM size for Compute Nodes in an Azure Batch Pool
     (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
    :vartype vm_size: str
    :ivar cloud_service_configuration: This property must be specified if the Pool needs to be
     created with Azure PaaS VMs. This property and virtualMachineConfiguration are mutually
     exclusive and one of the properties must be specified. If neither is specified then the Batch
     service returns an error; if you are calling the REST API directly, the HTTP status code is 400
     (Bad Request). This property cannot be specified if the Batch Account was created with its
     poolAllocationMode property set to 'UserSubscription'.
    :vartype cloud_service_configuration: ~azure-batch.models.CloudServiceConfiguration
    :ivar virtual_machine_configuration: This property must be specified if the Pool needs to be
     created with Azure IaaS VMs. This property and cloudServiceConfiguration are mutually exclusive
     and one of the properties must be specified. If neither is specified then the Batch service
     returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad
     Request).
    :vartype virtual_machine_configuration: ~azure-batch.models.VirtualMachineConfiguration
    :ivar task_slots_per_node: The default value is 1. The maximum value is the smaller of 4 times
     the number of cores of the vmSize of the pool or 256.
    :vartype task_slots_per_node: int
    :ivar task_scheduling_policy: If not specified, the default is spread.
    :vartype task_scheduling_policy: ~azure-batch.models.TaskSchedulingPolicy
    :ivar resize_timeout: This timeout applies only to manual scaling; it has no effect when
     enableAutoScale is set to true. The default value is 15 minutes. The minimum value is 5
     minutes. If you specify a value less than 5 minutes, the Batch service rejects the request with
     an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
    :vartype resize_timeout: ~datetime.timedelta
    :ivar target_dedicated_nodes: This property must not be specified if enableAutoScale is set to
     true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes,
     targetLowPriorityNodes, or both.
    :vartype target_dedicated_nodes: int
    :ivar target_low_priority_nodes: This property must not be specified if enableAutoScale is set
     to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes,
     targetLowPriorityNodes, or both.
    :vartype target_low_priority_nodes: int
    :ivar enable_auto_scale: If false, at least one of targetDedicatedNodes and
     targetLowPriorityNodes must be specified. If true, the autoScaleFormula element is required.
     The Pool automatically resizes according to the formula. The default value is false.
    :vartype enable_auto_scale: bool
    :ivar auto_scale_formula: This property must not be specified if enableAutoScale is set to
     false. It is required if enableAutoScale is set to true. The formula is checked for validity
     before the Pool is created. If the formula is not valid, the Batch service rejects the request
     with detailed error information.
    :vartype auto_scale_formula: str
    :ivar auto_scale_evaluation_interval: The default value is 15 minutes. The minimum and maximum
     value are 5 minutes and 168 hours respectively. If you specify a value less than 5 minutes or
     greater than 168 hours, the Batch service rejects the request with an invalid property value
     error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
    :vartype auto_scale_evaluation_interval: ~datetime.timedelta
    :ivar enable_inter_node_communication: Enabling inter-node communication limits the maximum
     size of the Pool due to deployment restrictions on the Compute Nodes of the Pool. This may
     result in the Pool not reaching its desired size. The default value is false.
    :vartype enable_inter_node_communication: bool
    :ivar network_configuration: The network configuration for a Pool.
    :vartype network_configuration: ~azure-batch.models.NetworkConfiguration
    :ivar start_task: Batch will retry Tasks when a recovery operation is triggered on a Node.
     Examples of recovery operations include (but are not limited to) when an unhealthy Node is
     rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations
     are independent of and are not counted against the maxTaskRetryCount. Even if the
     maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of
     this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and
     restarted without causing any corruption or duplicate data. The best practice for long running
     Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even
     though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks
     which create breakaway process or install/launch services from the StartTask working directory,
     as this will block Batch from being able to re-run the StartTask.
    :vartype start_task: ~azure-batch.models.StartTask
    :ivar certificate_references: For Windows Nodes, the Batch service installs the Certificates to
     the specified Certificate store and location. For Linux Compute Nodes, the Certificates are
     stored in a directory inside the Task working directory and an environment variable
     AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates
     with visibility of 'remoteUser', a 'certs' directory is created in the user's home directory
     (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
    :vartype certificate_references: list[~azure-batch.models.CertificateReference]
    :ivar application_package_references: Changes to Package references affect all new Nodes
     joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are
     rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
    :vartype application_package_references: list[~azure-batch.models.ApplicationPackageReference]
    :ivar application_licenses: The list of application licenses must be a subset of available
     Batch service application licenses. If a license is requested which is not supported, Pool
     creation will fail. The permitted licenses available on the Pool are 'maya', 'vray', '3dsmax',
     'arnold'. An additional charge applies for each application license added to the Pool.
    :vartype application_licenses: list[str]
    :ivar user_accounts: The list of user Accounts to be created on each Compute Node in the Pool.
    :vartype user_accounts: list[~azure-batch.models.UserAccount]
    :ivar metadata: The Batch service does not assign any meaning to metadata; it is solely for the
     use of user code.
    :vartype metadata: list[~azure-batch.models.MetadataItem]
    :ivar mount_configuration: This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
    :vartype mount_configuration: list[~azure-batch.models.MountConfiguration]
    """

    _validation = {
        'vm_size': {'required': True},
    }

    _attribute_map = {
        'display_name': {'key': 'displayName', 'type': 'str'},
        'vm_size': {'key': 'vmSize', 'type': 'str'},
        'cloud_service_configuration': {'key': 'cloudServiceConfiguration', 'type': 'CloudServiceConfiguration'},
        'virtual_machine_configuration': {'key': 'virtualMachineConfiguration', 'type': 'VirtualMachineConfiguration'},
        'task_slots_per_node': {'key': 'taskSlotsPerNode', 'type': 'int'},
        'task_scheduling_policy': {'key': 'taskSchedulingPolicy', 'type': 'TaskSchedulingPolicy'},
        'resize_timeout': {'key': 'resizeTimeout', 'type': 'duration'},
        'target_dedicated_nodes': {'key': 'targetDedicatedNodes', 'type': 'int'},
        'target_low_priority_nodes': {'key': 'targetLowPriorityNodes', 'type': 'int'},
        'enable_auto_scale': {'key': 'enableAutoScale', 'type': 'bool'},
        'auto_scale_formula': {'key': 'autoScaleFormula', 'type': 'str'},
        'auto_scale_evaluation_interval': {'key': 'autoScaleEvaluationInterval', 'type': 'duration'},
        'enable_inter_node_communication': {'key': 'enableInterNodeCommunication', 'type': 'bool'},
        'network_configuration': {'key': 'networkConfiguration', 'type': 'NetworkConfiguration'},
        'start_task': {'key': 'startTask', 'type': 'StartTask'},
        'certificate_references': {'key': 'certificateReferences', 'type': '[CertificateReference]'},
        'application_package_references': {'key': 'applicationPackageReferences', 'type': '[ApplicationPackageReference]'},
        'application_licenses': {'key': 'applicationLicenses', 'type': '[str]'},
        'user_accounts': {'key': 'userAccounts', 'type': '[UserAccount]'},
        'metadata': {'key': 'metadata', 'type': '[MetadataItem]'},
        'mount_configuration': {'key': 'mountConfiguration', 'type': '[MountConfiguration]'},
    }

    def __init__(
        self,
        *,
        vm_size: str,
        display_name: Optional[str] = None,
        cloud_service_configuration: Optional["_models.CloudServiceConfiguration"] = None,
        virtual_machine_configuration: Optional["_models.VirtualMachineConfiguration"] = None,
        task_slots_per_node: Optional[int] = None,
        task_scheduling_policy: Optional["_models.TaskSchedulingPolicy"] = None,
        resize_timeout: Optional[datetime.timedelta] = None,
        target_dedicated_nodes: Optional[int] = None,
        target_low_priority_nodes: Optional[int] = None,
        enable_auto_scale: Optional[bool] = None,
        auto_scale_formula: Optional[str] = None,
        auto_scale_evaluation_interval: Optional[datetime.timedelta] = None,
        enable_inter_node_communication: Optional[bool] = None,
        network_configuration: Optional["_models.NetworkConfiguration"] = None,
        start_task: Optional["_models.StartTask"] = None,
        certificate_references: Optional[List["_models.CertificateReference"]] = None,
        application_package_references: Optional[List["_models.ApplicationPackageReference"]] = None,
        application_licenses: Optional[List[str]] = None,
        user_accounts: Optional[List["_models.UserAccount"]] = None,
        metadata: Optional[List["_models.MetadataItem"]] = None,
        mount_configuration: Optional[List["_models.MountConfiguration"]] = None,
        **kwargs
    ):
        """
        :keyword display_name: The display name need not be unique and can contain any Unicode
         characters up to a maximum length of 1024.
        :paramtype display_name: str
        :keyword vm_size: Required. For information about available sizes of virtual machines in Pools,
         see Choose a VM size for Compute Nodes in an Azure Batch Pool
         (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        :paramtype vm_size: str
        :keyword cloud_service_configuration: This property must be specified if the Pool needs to be
         created with Azure PaaS VMs. This property and virtualMachineConfiguration are mutually
         exclusive and one of the properties must be specified. If neither is specified then the Batch
         service returns an error; if you are calling the REST API directly, the HTTP status code is 400
         (Bad Request). This property cannot be specified if the Batch Account was created with its
         poolAllocationMode property set to 'UserSubscription'.
        :paramtype cloud_service_configuration: ~azure-batch.models.CloudServiceConfiguration
        :keyword virtual_machine_configuration: This property must be specified if the Pool needs to be
         created with Azure IaaS VMs. This property and cloudServiceConfiguration are mutually exclusive
         and one of the properties must be specified. If neither is specified then the Batch service
         returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad
         Request).
        :paramtype virtual_machine_configuration: ~azure-batch.models.VirtualMachineConfiguration
        :keyword task_slots_per_node: The default value is 1. The maximum value is the smaller of 4
         times the number of cores of the vmSize of the pool or 256.
        :paramtype task_slots_per_node: int
        :keyword task_scheduling_policy: If not specified, the default is spread.
        :paramtype task_scheduling_policy: ~azure-batch.models.TaskSchedulingPolicy
        :keyword resize_timeout: This timeout applies only to manual scaling; it has no effect when
         enableAutoScale is set to true. The default value is 15 minutes. The minimum value is 5
         minutes. If you specify a value less than 5 minutes, the Batch service rejects the request with
         an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        :paramtype resize_timeout: ~datetime.timedelta
        :keyword target_dedicated_nodes: This property must not be specified if enableAutoScale is set
         to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes,
         targetLowPriorityNodes, or both.
        :paramtype target_dedicated_nodes: int
        :keyword target_low_priority_nodes: This property must not be specified if enableAutoScale is
         set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes,
         targetLowPriorityNodes, or both.
        :paramtype target_low_priority_nodes: int
        :keyword enable_auto_scale: If false, at least one of targetDedicatedNodes and
         targetLowPriorityNodes must be specified. If true, the autoScaleFormula element is required.
         The Pool automatically resizes according to the formula. The default value is false.
        :paramtype enable_auto_scale: bool
        :keyword auto_scale_formula: This property must not be specified if enableAutoScale is set to
         false. It is required if enableAutoScale is set to true. The formula is checked for validity
         before the Pool is created. If the formula is not valid, the Batch service rejects the request
         with detailed error information.
        :paramtype auto_scale_formula: str
        :keyword auto_scale_evaluation_interval: The default value is 15 minutes. The minimum and
         maximum value are 5 minutes and 168 hours respectively. If you specify a value less than 5
         minutes or greater than 168 hours, the Batch service rejects the request with an invalid
         property value error; if you are calling the REST API directly, the HTTP status code is 400
         (Bad Request).
        :paramtype auto_scale_evaluation_interval: ~datetime.timedelta
        :keyword enable_inter_node_communication: Enabling inter-node communication limits the maximum
         size of the Pool due to deployment restrictions on the Compute Nodes of the Pool. This may
         result in the Pool not reaching its desired size. The default value is false.
        :paramtype enable_inter_node_communication: bool
        :keyword network_configuration: The network configuration for a Pool.
        :paramtype network_configuration: ~azure-batch.models.NetworkConfiguration
        :keyword start_task: Batch will retry Tasks when a recovery operation is triggered on a Node.
         Examples of recovery operations include (but are not limited to) when an unhealthy Node is
         rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations
         are independent of and are not counted against the maxTaskRetryCount. Even if the
         maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of
         this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and
         restarted without causing any corruption or duplicate data. The best practice for long running
         Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even
         though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks
         which create breakaway process or install/launch services from the StartTask working directory,
         as this will block Batch from being able to re-run the StartTask.
        :paramtype start_task: ~azure-batch.models.StartTask
        :keyword certificate_references: For Windows Nodes, the Batch service installs the Certificates
         to the specified Certificate store and location. For Linux Compute Nodes, the Certificates are
         stored in a directory inside the Task working directory and an environment variable
         AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates
         with visibility of 'remoteUser', a 'certs' directory is created in the user's home directory
         (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        :paramtype certificate_references: list[~azure-batch.models.CertificateReference]
        :keyword application_package_references: Changes to Package references affect all new Nodes
         joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are
         rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
        :paramtype application_package_references:
         list[~azure-batch.models.ApplicationPackageReference]
        :keyword application_licenses: The list of application licenses must be a subset of available
         Batch service application licenses. If a license is requested which is not supported, Pool
         creation will fail. The permitted licenses available on the Pool are 'maya', 'vray', '3dsmax',
         'arnold'. An additional charge applies for each application license added to the Pool.
        :paramtype application_licenses: list[str]
        :keyword user_accounts: The list of user Accounts to be created on each Compute Node in the
         Pool.
        :paramtype user_accounts: list[~azure-batch.models.UserAccount]
        :keyword metadata: The Batch service does not assign any meaning to metadata; it is solely for
         the use of user code.
        :paramtype metadata: list[~azure-batch.models.MetadataItem]
        :keyword mount_configuration: This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
        :paramtype mount_configuration: list[~azure-batch.models.MountConfiguration]
        """
        super(PoolSpecification, self).__init__(**kwargs)
        self.display_name = display_name
        self.vm_size = vm_size
        self.cloud_service_configuration = cloud_service_configuration
        self.virtual_machine_configuration = virtual_machine_configuration
        self.task_slots_per_node = task_slots_per_node
        self.task_scheduling_policy = task_scheduling_policy
        self.resize_timeout = resize_timeout
        self.target_dedicated_nodes = target_dedicated_nodes
        self.target_low_priority_nodes = target_low_priority_nodes
        self.enable_auto_scale = enable_auto_scale
        self.auto_scale_formula = auto_scale_formula
        self.auto_scale_evaluation_interval = auto_scale_evaluation_interval
        self.enable_inter_node_communication = enable_inter_node_communication
        self.network_configuration = network_configuration
        self.start_task = start_task
        self.certificate_references = certificate_references
        self.application_package_references = application_package_references
        self.application_licenses = application_licenses
        self.user_accounts = user_accounts
        self.metadata = metadata
        self.mount_configuration = mount_configuration


class PoolStatistics(msrest.serialization.Model):
    """Contains utilization and resource usage statistics for the lifetime of a Pool.

    All required parameters must be populated in order to send to Azure.

    :ivar url: Required. The URL for the statistics.
    :vartype url: str
    :ivar start_time: Required. The start time of the time range covered by the statistics.
    :vartype start_time: ~datetime.datetime
    :ivar last_update_time: Required. The time at which the statistics were last updated. All
     statistics are limited to the range between startTime and lastUpdateTime.
    :vartype last_update_time: ~datetime.datetime
    :ivar usage_stats: Statistics related to Pool usage information.
    :vartype usage_stats: ~azure-batch.models.UsageStatistics
    :ivar resource_stats: Statistics related to resource consumption by Compute Nodes in a Pool.
    :vartype resource_stats: ~azure-batch.models.ResourceStatistics
    """

    _validation = {
        'url': {'required': True},
        'start_time': {'required': True},
        'last_update_time': {'required': True},
    }

    _attribute_map = {
        'url': {'key': 'url', 'type': 'str'},
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'last_update_time': {'key': 'lastUpdateTime', 'type': 'iso-8601'},
        'usage_stats': {'key': 'usageStats', 'type': 'UsageStatistics'},
        'resource_stats': {'key': 'resourceStats', 'type': 'ResourceStatistics'},
    }

    def __init__(
        self,
        *,
        url: str,
        start_time: datetime.datetime,
        last_update_time: datetime.datetime,
        usage_stats: Optional["_models.UsageStatistics"] = None,
        resource_stats: Optional["_models.ResourceStatistics"] = None,
        **kwargs
    ):
        """
        :keyword url: Required. The URL for the statistics.
        :paramtype url: str
        :keyword start_time: Required. The start time of the time range covered by the statistics.
        :paramtype start_time: ~datetime.datetime
        :keyword last_update_time: Required. The time at which the statistics were last updated. All
         statistics are limited to the range between startTime and lastUpdateTime.
        :paramtype last_update_time: ~datetime.datetime
        :keyword usage_stats: Statistics related to Pool usage information.
        :paramtype usage_stats: ~azure-batch.models.UsageStatistics
        :keyword resource_stats: Statistics related to resource consumption by Compute Nodes in a Pool.
        :paramtype resource_stats: ~azure-batch.models.ResourceStatistics
        """
        super(PoolStatistics, self).__init__(**kwargs)
        self.url = url
        self.start_time = start_time
        self.last_update_time = last_update_time
        self.usage_stats = usage_stats
        self.resource_stats = resource_stats


class PoolUsageMetrics(msrest.serialization.Model):
    """Usage metrics for a Pool across an aggregation interval.

    All required parameters must be populated in order to send to Azure.

    :ivar pool_id: Required. The ID of the Pool whose metrics are aggregated in this entry.
    :vartype pool_id: str
    :ivar start_time: Required. The start time of the aggregation interval covered by this entry.
    :vartype start_time: ~datetime.datetime
    :ivar end_time: Required. The end time of the aggregation interval covered by this entry.
    :vartype end_time: ~datetime.datetime
    :ivar vm_size: Required. For information about available sizes of virtual machines in Pools,
     see Choose a VM size for Compute Nodes in an Azure Batch Pool
     (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
    :vartype vm_size: str
    :ivar total_core_hours: Required. The total core hours used in the Pool during this aggregation
     interval.
    :vartype total_core_hours: float
    """

    _validation = {
        'pool_id': {'required': True},
        'start_time': {'required': True},
        'end_time': {'required': True},
        'vm_size': {'required': True},
        'total_core_hours': {'required': True},
    }

    _attribute_map = {
        'pool_id': {'key': 'poolId', 'type': 'str'},
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'end_time': {'key': 'endTime', 'type': 'iso-8601'},
        'vm_size': {'key': 'vmSize', 'type': 'str'},
        'total_core_hours': {'key': 'totalCoreHours', 'type': 'float'},
    }

    def __init__(
        self,
        *,
        pool_id: str,
        start_time: datetime.datetime,
        end_time: datetime.datetime,
        vm_size: str,
        total_core_hours: float,
        **kwargs
    ):
        """
        :keyword pool_id: Required. The ID of the Pool whose metrics are aggregated in this entry.
        :paramtype pool_id: str
        :keyword start_time: Required. The start time of the aggregation interval covered by this
         entry.
        :paramtype start_time: ~datetime.datetime
        :keyword end_time: Required. The end time of the aggregation interval covered by this entry.
        :paramtype end_time: ~datetime.datetime
        :keyword vm_size: Required. For information about available sizes of virtual machines in Pools,
         see Choose a VM size for Compute Nodes in an Azure Batch Pool
         (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        :paramtype vm_size: str
        :keyword total_core_hours: Required. The total core hours used in the Pool during this
         aggregation interval.
        :paramtype total_core_hours: float
        """
        super(PoolUsageMetrics, self).__init__(**kwargs)
        self.pool_id = pool_id
        self.start_time = start_time
        self.end_time = end_time
        self.vm_size = vm_size
        self.total_core_hours = total_core_hours


class PublicIPAddressConfiguration(msrest.serialization.Model):
    """The public IP Address configuration of the networking configuration of a Pool.

    :ivar provision: The default value is BatchManaged. Known values are: "batchmanaged",
     "usermanaged", "nopublicipaddresses".
    :vartype provision: str or ~azure-batch.models.IPAddressProvisioningType
    :ivar ip_address_ids: The number of IPs specified here limits the maximum size of the Pool -
     100 dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public IP. For
     example, a pool needing 250 dedicated VMs would need at least 3 public IPs specified. Each
     element of this collection is of the form:
     /subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
    :vartype ip_address_ids: list[str]
    """

    _attribute_map = {
        'provision': {'key': 'provision', 'type': 'str'},
        'ip_address_ids': {'key': 'ipAddressIds', 'type': '[str]'},
    }

    def __init__(
        self,
        *,
        provision: Optional[Union[str, "_models.IPAddressProvisioningType"]] = None,
        ip_address_ids: Optional[List[str]] = None,
        **kwargs
    ):
        """
        :keyword provision: The default value is BatchManaged. Known values are: "batchmanaged",
         "usermanaged", "nopublicipaddresses".
        :paramtype provision: str or ~azure-batch.models.IPAddressProvisioningType
        :keyword ip_address_ids: The number of IPs specified here limits the maximum size of the Pool -
         100 dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public IP. For
         example, a pool needing 250 dedicated VMs would need at least 3 public IPs specified. Each
         element of this collection is of the form:
         /subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
        :paramtype ip_address_ids: list[str]
        """
        super(PublicIPAddressConfiguration, self).__init__(**kwargs)
        self.provision = provision
        self.ip_address_ids = ip_address_ids


class RecentJob(msrest.serialization.Model):
    """Information about the most recent Job to run under the Job Schedule.

    :ivar id: The ID of the Job.
    :vartype id: str
    :ivar url: The URL of the Job.
    :vartype url: str
    """

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'url': {'key': 'url', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        id: Optional[str] = None,
        url: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword id: The ID of the Job.
        :paramtype id: str
        :keyword url: The URL of the Job.
        :paramtype url: str
        """
        super(RecentJob, self).__init__(**kwargs)
        self.id = id
        self.url = url


class ResizeError(msrest.serialization.Model):
    """An error that occurred when resizing a Pool.

    :ivar code: An identifier for the Pool resize error. Codes are invariant and are intended to be
     consumed programmatically.
    :vartype code: str
    :ivar message: A message describing the Pool resize error, intended to be suitable for display
     in a user interface.
    :vartype message: str
    :ivar values: A list of additional error details related to the Pool resize error.
    :vartype values: list[~azure-batch.models.NameValuePair]
    """

    _attribute_map = {
        'code': {'key': 'code', 'type': 'str'},
        'message': {'key': 'message', 'type': 'str'},
        'values': {'key': 'values', 'type': '[NameValuePair]'},
    }

    def __init__(
        self,
        *,
        code: Optional[str] = None,
        message: Optional[str] = None,
        values: Optional[List["_models.NameValuePair"]] = None,
        **kwargs
    ):
        """
        :keyword code: An identifier for the Pool resize error. Codes are invariant and are intended to
         be consumed programmatically.
        :paramtype code: str
        :keyword message: A message describing the Pool resize error, intended to be suitable for
         display in a user interface.
        :paramtype message: str
        :keyword values: A list of additional error details related to the Pool resize error.
        :paramtype values: list[~azure-batch.models.NameValuePair]
        """
        super(ResizeError, self).__init__(**kwargs)
        self.code = code
        self.message = message
        self.values = values


class ResourceFile(msrest.serialization.Model):
    """A single file or multiple files to be downloaded to a Compute Node.

    :ivar auto_storage_container_name: The autoStorageContainerName, storageContainerUrl and
     httpUrl properties are mutually exclusive and one of them must be specified.
    :vartype auto_storage_container_name: str
    :ivar storage_container_url: The autoStorageContainerName, storageContainerUrl and httpUrl
     properties are mutually exclusive and one of them must be specified. This URL must be readable
     and listable from compute nodes. There are three ways to get such a URL for a container in
     Azure storage: include a Shared Access Signature (SAS) granting read and list permissions on
     the container, use a managed identity with read and list permissions, or set the ACL for the
     container to allow public access.
    :vartype storage_container_url: str
    :ivar http_url: The autoStorageContainerName, storageContainerUrl and httpUrl properties are
     mutually exclusive and one of them must be specified. If the URL points to Azure Blob Storage,
     it must be readable from compute nodes. There are three ways to get such a URL for a blob in
     Azure storage: include a Shared Access Signature (SAS) granting read permissions on the blob,
     use a managed identity with read permission, or set the ACL for the blob or its container to
     allow public access.
    :vartype http_url: str
    :ivar blob_prefix: The property is valid only when autoStorageContainerName or
     storageContainerUrl is used. This prefix can be a partial filename or a subdirectory. If a
     prefix is not specified, all the files in the container will be downloaded.
    :vartype blob_prefix: str
    :ivar file_path: If the httpUrl property is specified, the filePath is required and describes
     the path which the file will be downloaded to, including the filename. Otherwise, if the
     autoStorageContainerName or storageContainerUrl property is specified, filePath is optional and
     is the directory to download the files to. In the case where filePath is used as a directory,
     any directory structure already associated with the input data will be retained in full and
     appended to the specified filePath directory. The specified relative path cannot break out of
     the Task's working directory (for example by using '..').
    :vartype file_path: str
    :ivar file_mode: This property applies only to files being downloaded to Linux Compute Nodes.
     It will be ignored if it is specified for a resourceFile which will be downloaded to a Windows
     Compute Node. If this property is not specified for a Linux Compute Node, then a default value
     of 0770 is applied to the file.
    :vartype file_mode: str
    :ivar identity_reference: The reference to a user assigned identity associated with the Batch
     pool which a compute node will use.
    :vartype identity_reference: ~azure-batch.models.ComputeNodeIdentityReference
    """

    _attribute_map = {
        'auto_storage_container_name': {'key': 'autoStorageContainerName', 'type': 'str'},
        'storage_container_url': {'key': 'storageContainerUrl', 'type': 'str'},
        'http_url': {'key': 'httpUrl', 'type': 'str'},
        'blob_prefix': {'key': 'blobPrefix', 'type': 'str'},
        'file_path': {'key': 'filePath', 'type': 'str'},
        'file_mode': {'key': 'fileMode', 'type': 'str'},
        'identity_reference': {'key': 'identityReference', 'type': 'ComputeNodeIdentityReference'},
    }

    def __init__(
        self,
        *,
        auto_storage_container_name: Optional[str] = None,
        storage_container_url: Optional[str] = None,
        http_url: Optional[str] = None,
        blob_prefix: Optional[str] = None,
        file_path: Optional[str] = None,
        file_mode: Optional[str] = None,
        identity_reference: Optional["_models.ComputeNodeIdentityReference"] = None,
        **kwargs
    ):
        """
        :keyword auto_storage_container_name: The autoStorageContainerName, storageContainerUrl and
         httpUrl properties are mutually exclusive and one of them must be specified.
        :paramtype auto_storage_container_name: str
        :keyword storage_container_url: The autoStorageContainerName, storageContainerUrl and httpUrl
         properties are mutually exclusive and one of them must be specified. This URL must be readable
         and listable from compute nodes. There are three ways to get such a URL for a container in
         Azure storage: include a Shared Access Signature (SAS) granting read and list permissions on
         the container, use a managed identity with read and list permissions, or set the ACL for the
         container to allow public access.
        :paramtype storage_container_url: str
        :keyword http_url: The autoStorageContainerName, storageContainerUrl and httpUrl properties are
         mutually exclusive and one of them must be specified. If the URL points to Azure Blob Storage,
         it must be readable from compute nodes. There are three ways to get such a URL for a blob in
         Azure storage: include a Shared Access Signature (SAS) granting read permissions on the blob,
         use a managed identity with read permission, or set the ACL for the blob or its container to
         allow public access.
        :paramtype http_url: str
        :keyword blob_prefix: The property is valid only when autoStorageContainerName or
         storageContainerUrl is used. This prefix can be a partial filename or a subdirectory. If a
         prefix is not specified, all the files in the container will be downloaded.
        :paramtype blob_prefix: str
        :keyword file_path: If the httpUrl property is specified, the filePath is required and
         describes the path which the file will be downloaded to, including the filename. Otherwise, if
         the autoStorageContainerName or storageContainerUrl property is specified, filePath is optional
         and is the directory to download the files to. In the case where filePath is used as a
         directory, any directory structure already associated with the input data will be retained in
         full and appended to the specified filePath directory. The specified relative path cannot break
         out of the Task's working directory (for example by using '..').
        :paramtype file_path: str
        :keyword file_mode: This property applies only to files being downloaded to Linux Compute
         Nodes. It will be ignored if it is specified for a resourceFile which will be downloaded to a
         Windows Compute Node. If this property is not specified for a Linux Compute Node, then a
         default value of 0770 is applied to the file.
        :paramtype file_mode: str
        :keyword identity_reference: The reference to a user assigned identity associated with the
         Batch pool which a compute node will use.
        :paramtype identity_reference: ~azure-batch.models.ComputeNodeIdentityReference
        """
        super(ResourceFile, self).__init__(**kwargs)
        self.auto_storage_container_name = auto_storage_container_name
        self.storage_container_url = storage_container_url
        self.http_url = http_url
        self.blob_prefix = blob_prefix
        self.file_path = file_path
        self.file_mode = file_mode
        self.identity_reference = identity_reference


class ResourceStatistics(msrest.serialization.Model):
    """Statistics related to resource consumption by Compute Nodes in a Pool.

    All required parameters must be populated in order to send to Azure.

    :ivar start_time: Required. The start time of the time range covered by the statistics.
    :vartype start_time: ~datetime.datetime
    :ivar last_update_time: Required. The time at which the statistics were last updated. All
     statistics are limited to the range between startTime and lastUpdateTime.
    :vartype last_update_time: ~datetime.datetime
    :ivar avg_cpu_percentage: Required. The average CPU usage across all Compute Nodes in the Pool
     (percentage per node).
    :vartype avg_cpu_percentage: float
    :ivar avg_memory_gi_b: Required. The average memory usage in GiB across all Compute Nodes in
     the Pool.
    :vartype avg_memory_gi_b: float
    :ivar peak_memory_gi_b: Required. The peak memory usage in GiB across all Compute Nodes in the
     Pool.
    :vartype peak_memory_gi_b: float
    :ivar avg_disk_gi_b: Required. The average used disk space in GiB across all Compute Nodes in
     the Pool.
    :vartype avg_disk_gi_b: float
    :ivar peak_disk_gi_b: Required. The peak used disk space in GiB across all Compute Nodes in the
     Pool.
    :vartype peak_disk_gi_b: float
    :ivar disk_read_i_ops: Required. The total number of disk read operations across all Compute
     Nodes in the Pool.
    :vartype disk_read_i_ops: long
    :ivar disk_write_i_ops: Required. The total number of disk write operations across all Compute
     Nodes in the Pool.
    :vartype disk_write_i_ops: long
    :ivar disk_read_gi_b: Required. The total amount of data in GiB of disk reads across all
     Compute Nodes in the Pool.
    :vartype disk_read_gi_b: float
    :ivar disk_write_gi_b: Required. The total amount of data in GiB of disk writes across all
     Compute Nodes in the Pool.
    :vartype disk_write_gi_b: float
    :ivar network_read_gi_b: Required. The total amount of data in GiB of network reads across all
     Compute Nodes in the Pool.
    :vartype network_read_gi_b: float
    :ivar network_write_gi_b: Required. The total amount of data in GiB of network writes across
     all Compute Nodes in the Pool.
    :vartype network_write_gi_b: float
    """

    _validation = {
        'start_time': {'required': True},
        'last_update_time': {'required': True},
        'avg_cpu_percentage': {'required': True},
        'avg_memory_gi_b': {'required': True},
        'peak_memory_gi_b': {'required': True},
        'avg_disk_gi_b': {'required': True},
        'peak_disk_gi_b': {'required': True},
        'disk_read_i_ops': {'required': True},
        'disk_write_i_ops': {'required': True},
        'disk_read_gi_b': {'required': True},
        'disk_write_gi_b': {'required': True},
        'network_read_gi_b': {'required': True},
        'network_write_gi_b': {'required': True},
    }

    _attribute_map = {
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'last_update_time': {'key': 'lastUpdateTime', 'type': 'iso-8601'},
        'avg_cpu_percentage': {'key': 'avgCPUPercentage', 'type': 'float'},
        'avg_memory_gi_b': {'key': 'avgMemoryGiB', 'type': 'float'},
        'peak_memory_gi_b': {'key': 'peakMemoryGiB', 'type': 'float'},
        'avg_disk_gi_b': {'key': 'avgDiskGiB', 'type': 'float'},
        'peak_disk_gi_b': {'key': 'peakDiskGiB', 'type': 'float'},
        'disk_read_i_ops': {'key': 'diskReadIOps', 'type': 'long'},
        'disk_write_i_ops': {'key': 'diskWriteIOps', 'type': 'long'},
        'disk_read_gi_b': {'key': 'diskReadGiB', 'type': 'float'},
        'disk_write_gi_b': {'key': 'diskWriteGiB', 'type': 'float'},
        'network_read_gi_b': {'key': 'networkReadGiB', 'type': 'float'},
        'network_write_gi_b': {'key': 'networkWriteGiB', 'type': 'float'},
    }

    def __init__(
        self,
        *,
        start_time: datetime.datetime,
        last_update_time: datetime.datetime,
        avg_cpu_percentage: float,
        avg_memory_gi_b: float,
        peak_memory_gi_b: float,
        avg_disk_gi_b: float,
        peak_disk_gi_b: float,
        disk_read_i_ops: int,
        disk_write_i_ops: int,
        disk_read_gi_b: float,
        disk_write_gi_b: float,
        network_read_gi_b: float,
        network_write_gi_b: float,
        **kwargs
    ):
        """
        :keyword start_time: Required. The start time of the time range covered by the statistics.
        :paramtype start_time: ~datetime.datetime
        :keyword last_update_time: Required. The time at which the statistics were last updated. All
         statistics are limited to the range between startTime and lastUpdateTime.
        :paramtype last_update_time: ~datetime.datetime
        :keyword avg_cpu_percentage: Required. The average CPU usage across all Compute Nodes in the
         Pool (percentage per node).
        :paramtype avg_cpu_percentage: float
        :keyword avg_memory_gi_b: Required. The average memory usage in GiB across all Compute Nodes in
         the Pool.
        :paramtype avg_memory_gi_b: float
        :keyword peak_memory_gi_b: Required. The peak memory usage in GiB across all Compute Nodes in
         the Pool.
        :paramtype peak_memory_gi_b: float
        :keyword avg_disk_gi_b: Required. The average used disk space in GiB across all Compute Nodes
         in the Pool.
        :paramtype avg_disk_gi_b: float
        :keyword peak_disk_gi_b: Required. The peak used disk space in GiB across all Compute Nodes in
         the Pool.
        :paramtype peak_disk_gi_b: float
        :keyword disk_read_i_ops: Required. The total number of disk read operations across all Compute
         Nodes in the Pool.
        :paramtype disk_read_i_ops: long
        :keyword disk_write_i_ops: Required. The total number of disk write operations across all
         Compute Nodes in the Pool.
        :paramtype disk_write_i_ops: long
        :keyword disk_read_gi_b: Required. The total amount of data in GiB of disk reads across all
         Compute Nodes in the Pool.
        :paramtype disk_read_gi_b: float
        :keyword disk_write_gi_b: Required. The total amount of data in GiB of disk writes across all
         Compute Nodes in the Pool.
        :paramtype disk_write_gi_b: float
        :keyword network_read_gi_b: Required. The total amount of data in GiB of network reads across
         all Compute Nodes in the Pool.
        :paramtype network_read_gi_b: float
        :keyword network_write_gi_b: Required. The total amount of data in GiB of network writes across
         all Compute Nodes in the Pool.
        :paramtype network_write_gi_b: float
        """
        super(ResourceStatistics, self).__init__(**kwargs)
        self.start_time = start_time
        self.last_update_time = last_update_time
        self.avg_cpu_percentage = avg_cpu_percentage
        self.avg_memory_gi_b = avg_memory_gi_b
        self.peak_memory_gi_b = peak_memory_gi_b
        self.avg_disk_gi_b = avg_disk_gi_b
        self.peak_disk_gi_b = peak_disk_gi_b
        self.disk_read_i_ops = disk_read_i_ops
        self.disk_write_i_ops = disk_write_i_ops
        self.disk_read_gi_b = disk_read_gi_b
        self.disk_write_gi_b = disk_write_gi_b
        self.network_read_gi_b = network_read_gi_b
        self.network_write_gi_b = network_write_gi_b


class Schedule(msrest.serialization.Model):
    """The schedule according to which Jobs will be created. All times are fixed respective to UTC and are not impacted by daylight saving time.

    :ivar do_not_run_until: If you do not specify a doNotRunUntil time, the schedule becomes ready
     to create Jobs immediately.
    :vartype do_not_run_until: ~datetime.datetime
    :ivar do_not_run_after: If you do not specify a doNotRunAfter time, and you are creating a
     recurring Job Schedule, the Job Schedule will remain active until you explicitly terminate it.
    :vartype do_not_run_after: ~datetime.datetime
    :ivar start_window: If a Job is not created within the startWindow interval, then the
     'opportunity' is lost; no Job will be created until the next recurrence of the schedule. If the
     schedule is recurring, and the startWindow is longer than the recurrence interval, then this is
     equivalent to an infinite startWindow, because the Job that is 'due' in one recurrenceInterval
     is not carried forward into the next recurrence interval. The default is infinite. The minimum
     value is 1 minute. If you specify a lower value, the Batch service rejects the schedule with an
     error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
    :vartype start_window: ~datetime.timedelta
    :ivar recurrence_interval: Because a Job Schedule can have at most one active Job under it at
     any given time, if it is time to create a new Job under a Job Schedule, but the previous Job is
     still running, the Batch service will not create the new Job until the previous Job finishes.
     If the previous Job does not finish within the startWindow period of the new
     recurrenceInterval, then no new Job will be scheduled for that interval. For recurring Jobs,
     you should normally specify a jobManagerTask in the jobSpecification. If you do not use
     jobManagerTask, you will need an external process to monitor when Jobs are created, add Tasks
     to the Jobs and terminate the Jobs ready for the next recurrence. The default is that the
     schedule does not recur: one Job is created, within the startWindow after the doNotRunUntil
     time, and the schedule is complete as soon as that Job finishes. The minimum value is 1 minute.
     If you specify a lower value, the Batch service rejects the schedule with an error; if you are
     calling the REST API directly, the HTTP status code is 400 (Bad Request).
    :vartype recurrence_interval: ~datetime.timedelta
    """

    _attribute_map = {
        'do_not_run_until': {'key': 'doNotRunUntil', 'type': 'iso-8601'},
        'do_not_run_after': {'key': 'doNotRunAfter', 'type': 'iso-8601'},
        'start_window': {'key': 'startWindow', 'type': 'duration'},
        'recurrence_interval': {'key': 'recurrenceInterval', 'type': 'duration'},
    }

    def __init__(
        self,
        *,
        do_not_run_until: Optional[datetime.datetime] = None,
        do_not_run_after: Optional[datetime.datetime] = None,
        start_window: Optional[datetime.timedelta] = None,
        recurrence_interval: Optional[datetime.timedelta] = None,
        **kwargs
    ):
        """
        :keyword do_not_run_until: If you do not specify a doNotRunUntil time, the schedule becomes
         ready to create Jobs immediately.
        :paramtype do_not_run_until: ~datetime.datetime
        :keyword do_not_run_after: If you do not specify a doNotRunAfter time, and you are creating a
         recurring Job Schedule, the Job Schedule will remain active until you explicitly terminate it.
        :paramtype do_not_run_after: ~datetime.datetime
        :keyword start_window: If a Job is not created within the startWindow interval, then the
         'opportunity' is lost; no Job will be created until the next recurrence of the schedule. If the
         schedule is recurring, and the startWindow is longer than the recurrence interval, then this is
         equivalent to an infinite startWindow, because the Job that is 'due' in one recurrenceInterval
         is not carried forward into the next recurrence interval. The default is infinite. The minimum
         value is 1 minute. If you specify a lower value, the Batch service rejects the schedule with an
         error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        :paramtype start_window: ~datetime.timedelta
        :keyword recurrence_interval: Because a Job Schedule can have at most one active Job under it
         at any given time, if it is time to create a new Job under a Job Schedule, but the previous Job
         is still running, the Batch service will not create the new Job until the previous Job
         finishes. If the previous Job does not finish within the startWindow period of the new
         recurrenceInterval, then no new Job will be scheduled for that interval. For recurring Jobs,
         you should normally specify a jobManagerTask in the jobSpecification. If you do not use
         jobManagerTask, you will need an external process to monitor when Jobs are created, add Tasks
         to the Jobs and terminate the Jobs ready for the next recurrence. The default is that the
         schedule does not recur: one Job is created, within the startWindow after the doNotRunUntil
         time, and the schedule is complete as soon as that Job finishes. The minimum value is 1 minute.
         If you specify a lower value, the Batch service rejects the schedule with an error; if you are
         calling the REST API directly, the HTTP status code is 400 (Bad Request).
        :paramtype recurrence_interval: ~datetime.timedelta
        """
        super(Schedule, self).__init__(**kwargs)
        self.do_not_run_until = do_not_run_until
        self.do_not_run_after = do_not_run_after
        self.start_window = start_window
        self.recurrence_interval = recurrence_interval


class StartTask(msrest.serialization.Model):
    """Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks which create breakaway process or install/launch services from the StartTask working directory, as this will block Batch from being able to re-run the StartTask.

    All required parameters must be populated in order to send to Azure.

    :ivar command_line: Required. The command line does not run under a shell, and therefore cannot
     take advantage of shell features such as environment variable expansion. If you want to take
     advantage of such features, you should invoke the shell in the command line, for example using
     "cmd /c MyCommand" in Windows or "/bin/sh -c MyCommand" in Linux. If the command line refers to
     file paths, it should use a relative path (relative to the Task working directory), or use the
     Batch provided environment variable
     (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
    :vartype command_line: str
    :ivar container_settings: When this is specified, all directories recursively below the
     AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the
     container, all Task environment variables are mapped into the container, and the Task command
     line is executed in the container. Files produced in the container outside of
     AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs
     will not be able to access those files.
    :vartype container_settings: ~azure-batch.models.TaskContainerSettings
    :ivar resource_files: Files listed under this element are located in the Task's working
     directory.
    :vartype resource_files: list[~azure-batch.models.ResourceFile]
    :ivar environment_settings: A list of environment variable settings for the StartTask.
    :vartype environment_settings: list[~azure-batch.models.EnvironmentSetting]
    :ivar user_identity: If omitted, the Task runs as a non-administrative user unique to the Task.
    :vartype user_identity: ~azure-batch.models.UserIdentity
    :ivar max_task_retry_count: The Batch service retries a Task if its exit code is nonzero. Note
     that this value specifically controls the number of retries. The Batch service will try the
     Task once, and may then retry up to this limit. For example, if the maximum retry count is 3,
     Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count
     is 0, the Batch service does not retry the Task. If the maximum retry count is -1, the Batch
     service retries the Task without limit, however this is not recommended for a start task or any
     task. The default value is 0 (no retries).
    :vartype max_task_retry_count: int
    :ivar wait_for_success: If true and the StartTask fails on a Node, the Batch service retries
     the StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has still not
     completed successfully after all retries, then the Batch service marks the Node unusable, and
     will not schedule Tasks to it. This condition can be detected via the Compute Node state and
     failure info details. If false, the Batch service will not wait for the StartTask to complete.
     In this case, other Tasks can start executing on the Compute Node while the StartTask is still
     running; and even if the StartTask fails, new Tasks will continue to be scheduled on the
     Compute Node. The default is true.
    :vartype wait_for_success: bool
    """

    _validation = {
        'command_line': {'required': True},
    }

    _attribute_map = {
        'command_line': {'key': 'commandLine', 'type': 'str'},
        'container_settings': {'key': 'containerSettings', 'type': 'TaskContainerSettings'},
        'resource_files': {'key': 'resourceFiles', 'type': '[ResourceFile]'},
        'environment_settings': {'key': 'environmentSettings', 'type': '[EnvironmentSetting]'},
        'user_identity': {'key': 'userIdentity', 'type': 'UserIdentity'},
        'max_task_retry_count': {'key': 'maxTaskRetryCount', 'type': 'int'},
        'wait_for_success': {'key': 'waitForSuccess', 'type': 'bool'},
    }

    def __init__(
        self,
        *,
        command_line: str,
        container_settings: Optional["_models.TaskContainerSettings"] = None,
        resource_files: Optional[List["_models.ResourceFile"]] = None,
        environment_settings: Optional[List["_models.EnvironmentSetting"]] = None,
        user_identity: Optional["_models.UserIdentity"] = None,
        max_task_retry_count: Optional[int] = None,
        wait_for_success: Optional[bool] = None,
        **kwargs
    ):
        """
        :keyword command_line: Required. The command line does not run under a shell, and therefore
         cannot take advantage of shell features such as environment variable expansion. If you want to
         take advantage of such features, you should invoke the shell in the command line, for example
         using "cmd /c MyCommand" in Windows or "/bin/sh -c MyCommand" in Linux. If the command line
         refers to file paths, it should use a relative path (relative to the Task working directory),
         or use the Batch provided environment variable
         (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        :paramtype command_line: str
        :keyword container_settings: When this is specified, all directories recursively below the
         AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the
         container, all Task environment variables are mapped into the container, and the Task command
         line is executed in the container. Files produced in the container outside of
         AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs
         will not be able to access those files.
        :paramtype container_settings: ~azure-batch.models.TaskContainerSettings
        :keyword resource_files: Files listed under this element are located in the Task's working
         directory.
        :paramtype resource_files: list[~azure-batch.models.ResourceFile]
        :keyword environment_settings: A list of environment variable settings for the StartTask.
        :paramtype environment_settings: list[~azure-batch.models.EnvironmentSetting]
        :keyword user_identity: If omitted, the Task runs as a non-administrative user unique to the
         Task.
        :paramtype user_identity: ~azure-batch.models.UserIdentity
        :keyword max_task_retry_count: The Batch service retries a Task if its exit code is nonzero.
         Note that this value specifically controls the number of retries. The Batch service will try
         the Task once, and may then retry up to this limit. For example, if the maximum retry count is
         3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry
         count is 0, the Batch service does not retry the Task. If the maximum retry count is -1, the
         Batch service retries the Task without limit, however this is not recommended for a start task
         or any task. The default value is 0 (no retries).
        :paramtype max_task_retry_count: int
        :keyword wait_for_success: If true and the StartTask fails on a Node, the Batch service retries
         the StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has still not
         completed successfully after all retries, then the Batch service marks the Node unusable, and
         will not schedule Tasks to it. This condition can be detected via the Compute Node state and
         failure info details. If false, the Batch service will not wait for the StartTask to complete.
         In this case, other Tasks can start executing on the Compute Node while the StartTask is still
         running; and even if the StartTask fails, new Tasks will continue to be scheduled on the
         Compute Node. The default is true.
        :paramtype wait_for_success: bool
        """
        super(StartTask, self).__init__(**kwargs)
        self.command_line = command_line
        self.container_settings = container_settings
        self.resource_files = resource_files
        self.environment_settings = environment_settings
        self.user_identity = user_identity
        self.max_task_retry_count = max_task_retry_count
        self.wait_for_success = wait_for_success


class StartTaskInformation(msrest.serialization.Model):
    """Information about a StartTask running on a Compute Node.

    All required parameters must be populated in order to send to Azure.

    :ivar state: Required. The state of the StartTask on the Compute Node. Known values are:
     "running", "completed".
    :vartype state: str or ~azure-batch.models.StartTaskState
    :ivar start_time: Required. This value is reset every time the Task is restarted or retried
     (that is, this is the most recent time at which the StartTask started running).
    :vartype start_time: ~datetime.datetime
    :ivar end_time: This is the end time of the most recent run of the StartTask, if that run has
     completed (even if that run failed and a retry is pending). This element is not present if the
     StartTask is currently running.
    :vartype end_time: ~datetime.datetime
    :ivar exit_code: This property is set only if the StartTask is in the completed state. In
     general, the exit code for a process reflects the specific convention implemented by the
     application developer for that process. If you use the exit code value to make decisions in
     your code, be sure that you know the exit code convention used by the application process.
     However, if the Batch service terminates the StartTask (due to timeout, or user termination via
     the API) you may see an operating system-defined exit code.
    :vartype exit_code: int
    :ivar container_info: This property is set only if the Task runs in a container context.
    :vartype container_info: ~azure-batch.models.TaskContainerExecutionInformation
    :ivar failure_info: This property is set only if the Task is in the completed state and
     encountered a failure.
    :vartype failure_info: ~azure-batch.models.TaskFailureInformation
    :ivar retry_count: Required. Task application failures (non-zero exit code) are retried,
     pre-processing errors (the Task could not be run) and file upload errors are not retried. The
     Batch service will retry the Task up to the limit specified by the constraints.
    :vartype retry_count: int
    :ivar last_retry_time: This element is present only if the Task was retried (i.e. retryCount is
     nonzero). If present, this is typically the same as startTime, but may be different if the Task
     has been restarted for reasons other than retry; for example, if the Compute Node was rebooted
     during a retry, then the startTime is updated but the lastRetryTime is not.
    :vartype last_retry_time: ~datetime.datetime
    :ivar result: If the value is 'failed', then the details of the failure can be found in the
     failureInfo property. Known values are: "success", "failure".
    :vartype result: str or ~azure-batch.models.TaskExecutionResult
    """

    _validation = {
        'state': {'required': True},
        'start_time': {'required': True},
        'retry_count': {'required': True},
    }

    _attribute_map = {
        'state': {'key': 'state', 'type': 'str'},
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'end_time': {'key': 'endTime', 'type': 'iso-8601'},
        'exit_code': {'key': 'exitCode', 'type': 'int'},
        'container_info': {'key': 'containerInfo', 'type': 'TaskContainerExecutionInformation'},
        'failure_info': {'key': 'failureInfo', 'type': 'TaskFailureInformation'},
        'retry_count': {'key': 'retryCount', 'type': 'int'},
        'last_retry_time': {'key': 'lastRetryTime', 'type': 'iso-8601'},
        'result': {'key': 'result', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        state: Union[str, "_models.StartTaskState"],
        start_time: datetime.datetime,
        retry_count: int,
        end_time: Optional[datetime.datetime] = None,
        exit_code: Optional[int] = None,
        container_info: Optional["_models.TaskContainerExecutionInformation"] = None,
        failure_info: Optional["_models.TaskFailureInformation"] = None,
        last_retry_time: Optional[datetime.datetime] = None,
        result: Optional[Union[str, "_models.TaskExecutionResult"]] = None,
        **kwargs
    ):
        """
        :keyword state: Required. The state of the StartTask on the Compute Node. Known values are:
         "running", "completed".
        :paramtype state: str or ~azure-batch.models.StartTaskState
        :keyword start_time: Required. This value is reset every time the Task is restarted or retried
         (that is, this is the most recent time at which the StartTask started running).
        :paramtype start_time: ~datetime.datetime
        :keyword end_time: This is the end time of the most recent run of the StartTask, if that run
         has completed (even if that run failed and a retry is pending). This element is not present if
         the StartTask is currently running.
        :paramtype end_time: ~datetime.datetime
        :keyword exit_code: This property is set only if the StartTask is in the completed state. In
         general, the exit code for a process reflects the specific convention implemented by the
         application developer for that process. If you use the exit code value to make decisions in
         your code, be sure that you know the exit code convention used by the application process.
         However, if the Batch service terminates the StartTask (due to timeout, or user termination via
         the API) you may see an operating system-defined exit code.
        :paramtype exit_code: int
        :keyword container_info: This property is set only if the Task runs in a container context.
        :paramtype container_info: ~azure-batch.models.TaskContainerExecutionInformation
        :keyword failure_info: This property is set only if the Task is in the completed state and
         encountered a failure.
        :paramtype failure_info: ~azure-batch.models.TaskFailureInformation
        :keyword retry_count: Required. Task application failures (non-zero exit code) are retried,
         pre-processing errors (the Task could not be run) and file upload errors are not retried. The
         Batch service will retry the Task up to the limit specified by the constraints.
        :paramtype retry_count: int
        :keyword last_retry_time: This element is present only if the Task was retried (i.e. retryCount
         is nonzero). If present, this is typically the same as startTime, but may be different if the
         Task has been restarted for reasons other than retry; for example, if the Compute Node was
         rebooted during a retry, then the startTime is updated but the lastRetryTime is not.
        :paramtype last_retry_time: ~datetime.datetime
        :keyword result: If the value is 'failed', then the details of the failure can be found in the
         failureInfo property. Known values are: "success", "failure".
        :paramtype result: str or ~azure-batch.models.TaskExecutionResult
        """
        super(StartTaskInformation, self).__init__(**kwargs)
        self.state = state
        self.start_time = start_time
        self.end_time = end_time
        self.exit_code = exit_code
        self.container_info = container_info
        self.failure_info = failure_info
        self.retry_count = retry_count
        self.last_retry_time = last_retry_time
        self.result = result


class SubtaskInformation(msrest.serialization.Model):
    """Information about an Azure Batch subtask.

    :ivar id: The ID of the subtask.
    :vartype id: int
    :ivar node_info: Information about the Compute Node on which a Task ran.
    :vartype node_info: ~azure-batch.models.ComputeNodeInformation
    :ivar start_time: The time at which the subtask started running. If the subtask has been
     restarted or retried, this is the most recent time at which the subtask started running.
    :vartype start_time: ~datetime.datetime
    :ivar end_time: This property is set only if the subtask is in the Completed state.
    :vartype end_time: ~datetime.datetime
    :ivar exit_code: This property is set only if the subtask is in the completed state. In
     general, the exit code for a process reflects the specific convention implemented by the
     application developer for that process. If you use the exit code value to make decisions in
     your code, be sure that you know the exit code convention used by the application process.
     However, if the Batch service terminates the subtask (due to timeout, or user termination via
     the API) you may see an operating system-defined exit code.
    :vartype exit_code: int
    :ivar container_info: This property is set only if the Task runs in a container context.
    :vartype container_info: ~azure-batch.models.TaskContainerExecutionInformation
    :ivar failure_info: This property is set only if the Task is in the completed state and
     encountered a failure.
    :vartype failure_info: ~azure-batch.models.TaskFailureInformation
    :ivar state: The state of the subtask. Known values are: "preparing", "running", "completed".
    :vartype state: str or ~azure-batch.models.SubtaskState
    :ivar state_transition_time: The time at which the subtask entered its current state.
    :vartype state_transition_time: ~datetime.datetime
    :ivar previous_state: This property is not set if the subtask is in its initial running state.
     Known values are: "preparing", "running", "completed".
    :vartype previous_state: str or ~azure-batch.models.SubtaskState
    :ivar previous_state_transition_time: This property is not set if the subtask is in its initial
     running state.
    :vartype previous_state_transition_time: ~datetime.datetime
    :ivar result: If the value is 'failed', then the details of the failure can be found in the
     failureInfo property. Known values are: "success", "failure".
    :vartype result: str or ~azure-batch.models.TaskExecutionResult
    """

    _attribute_map = {
        'id': {'key': 'id', 'type': 'int'},
        'node_info': {'key': 'nodeInfo', 'type': 'ComputeNodeInformation'},
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'end_time': {'key': 'endTime', 'type': 'iso-8601'},
        'exit_code': {'key': 'exitCode', 'type': 'int'},
        'container_info': {'key': 'containerInfo', 'type': 'TaskContainerExecutionInformation'},
        'failure_info': {'key': 'failureInfo', 'type': 'TaskFailureInformation'},
        'state': {'key': 'state', 'type': 'str'},
        'state_transition_time': {'key': 'stateTransitionTime', 'type': 'iso-8601'},
        'previous_state': {'key': 'previousState', 'type': 'str'},
        'previous_state_transition_time': {'key': 'previousStateTransitionTime', 'type': 'iso-8601'},
        'result': {'key': 'result', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        id: Optional[int] = None,
        node_info: Optional["_models.ComputeNodeInformation"] = None,
        start_time: Optional[datetime.datetime] = None,
        end_time: Optional[datetime.datetime] = None,
        exit_code: Optional[int] = None,
        container_info: Optional["_models.TaskContainerExecutionInformation"] = None,
        failure_info: Optional["_models.TaskFailureInformation"] = None,
        state: Optional[Union[str, "_models.SubtaskState"]] = None,
        state_transition_time: Optional[datetime.datetime] = None,
        previous_state: Optional[Union[str, "_models.SubtaskState"]] = None,
        previous_state_transition_time: Optional[datetime.datetime] = None,
        result: Optional[Union[str, "_models.TaskExecutionResult"]] = None,
        **kwargs
    ):
        """
        :keyword id: The ID of the subtask.
        :paramtype id: int
        :keyword node_info: Information about the Compute Node on which a Task ran.
        :paramtype node_info: ~azure-batch.models.ComputeNodeInformation
        :keyword start_time: The time at which the subtask started running. If the subtask has been
         restarted or retried, this is the most recent time at which the subtask started running.
        :paramtype start_time: ~datetime.datetime
        :keyword end_time: This property is set only if the subtask is in the Completed state.
        :paramtype end_time: ~datetime.datetime
        :keyword exit_code: This property is set only if the subtask is in the completed state. In
         general, the exit code for a process reflects the specific convention implemented by the
         application developer for that process. If you use the exit code value to make decisions in
         your code, be sure that you know the exit code convention used by the application process.
         However, if the Batch service terminates the subtask (due to timeout, or user termination via
         the API) you may see an operating system-defined exit code.
        :paramtype exit_code: int
        :keyword container_info: This property is set only if the Task runs in a container context.
        :paramtype container_info: ~azure-batch.models.TaskContainerExecutionInformation
        :keyword failure_info: This property is set only if the Task is in the completed state and
         encountered a failure.
        :paramtype failure_info: ~azure-batch.models.TaskFailureInformation
        :keyword state: The state of the subtask. Known values are: "preparing", "running",
         "completed".
        :paramtype state: str or ~azure-batch.models.SubtaskState
        :keyword state_transition_time: The time at which the subtask entered its current state.
        :paramtype state_transition_time: ~datetime.datetime
        :keyword previous_state: This property is not set if the subtask is in its initial running
         state. Known values are: "preparing", "running", "completed".
        :paramtype previous_state: str or ~azure-batch.models.SubtaskState
        :keyword previous_state_transition_time: This property is not set if the subtask is in its
         initial running state.
        :paramtype previous_state_transition_time: ~datetime.datetime
        :keyword result: If the value is 'failed', then the details of the failure can be found in the
         failureInfo property. Known values are: "success", "failure".
        :paramtype result: str or ~azure-batch.models.TaskExecutionResult
        """
        super(SubtaskInformation, self).__init__(**kwargs)
        self.id = id
        self.node_info = node_info
        self.start_time = start_time
        self.end_time = end_time
        self.exit_code = exit_code
        self.container_info = container_info
        self.failure_info = failure_info
        self.state = state
        self.state_transition_time = state_transition_time
        self.previous_state = previous_state
        self.previous_state_transition_time = previous_state_transition_time
        self.result = result


class TaskAddCollectionResult(msrest.serialization.Model):
    """The result of adding a collection of Tasks to a Job.

    :ivar value: The results of the add Task collection operation.
    :vartype value: list[~azure-batch.models.TaskAddResult]
    """

    _attribute_map = {
        'value': {'key': 'value', 'type': '[TaskAddResult]'},
    }

    def __init__(
        self,
        *,
        value: Optional[List["_models.TaskAddResult"]] = None,
        **kwargs
    ):
        """
        :keyword value: The results of the add Task collection operation.
        :paramtype value: list[~azure-batch.models.TaskAddResult]
        """
        super(TaskAddCollectionResult, self).__init__(**kwargs)
        self.value = value


class TaskAddResult(msrest.serialization.Model):
    """Result for a single Task added as part of an add Task collection operation.

    All required parameters must be populated in order to send to Azure.

    :ivar status: Required. The status of the add Task request. Known values are: "success",
     "clienterror", "servererror".
    :vartype status: str or ~azure-batch.models.TaskAddStatus
    :ivar task_id: Required. The ID of the Task for which this is the result.
    :vartype task_id: str
    :ivar e_tag: You can use this to detect whether the Task has changed between requests. In
     particular, you can be pass the ETag with an Update Task request to specify that your changes
     should take effect only if nobody else has modified the Job in the meantime.
    :vartype e_tag: str
    :ivar last_modified: The last modified time of the Task.
    :vartype last_modified: ~datetime.datetime
    :ivar location: The URL of the Task, if the Task was successfully added.
    :vartype location: str
    :ivar error: An error response received from the Azure Batch service.
    :vartype error: ~azure-batch.models.BatchError
    """

    _validation = {
        'status': {'required': True},
        'task_id': {'required': True},
    }

    _attribute_map = {
        'status': {'key': 'status', 'type': 'str'},
        'task_id': {'key': 'taskId', 'type': 'str'},
        'e_tag': {'key': 'eTag', 'type': 'str'},
        'last_modified': {'key': 'lastModified', 'type': 'iso-8601'},
        'location': {'key': 'location', 'type': 'str'},
        'error': {'key': 'error', 'type': 'BatchError'},
    }

    def __init__(
        self,
        *,
        status: Union[str, "_models.TaskAddStatus"],
        task_id: str,
        e_tag: Optional[str] = None,
        last_modified: Optional[datetime.datetime] = None,
        location: Optional[str] = None,
        error: Optional["_models.BatchError"] = None,
        **kwargs
    ):
        """
        :keyword status: Required. The status of the add Task request. Known values are: "success",
         "clienterror", "servererror".
        :paramtype status: str or ~azure-batch.models.TaskAddStatus
        :keyword task_id: Required. The ID of the Task for which this is the result.
        :paramtype task_id: str
        :keyword e_tag: You can use this to detect whether the Task has changed between requests. In
         particular, you can be pass the ETag with an Update Task request to specify that your changes
         should take effect only if nobody else has modified the Job in the meantime.
        :paramtype e_tag: str
        :keyword last_modified: The last modified time of the Task.
        :paramtype last_modified: ~datetime.datetime
        :keyword location: The URL of the Task, if the Task was successfully added.
        :paramtype location: str
        :keyword error: An error response received from the Azure Batch service.
        :paramtype error: ~azure-batch.models.BatchError
        """
        super(TaskAddResult, self).__init__(**kwargs)
        self.status = status
        self.task_id = task_id
        self.e_tag = e_tag
        self.last_modified = last_modified
        self.location = location
        self.error = error


class TaskConstraints(msrest.serialization.Model):
    """Execution constraints to apply to a Task.

    :ivar max_wall_clock_time: If this is not specified, there is no time limit on how long the
     Task may run.
    :vartype max_wall_clock_time: ~datetime.timedelta
    :ivar retention_time: The default is 7 days, i.e. the Task directory will be retained for 7
     days unless the Compute Node is removed or the Job is deleted.
    :vartype retention_time: ~datetime.timedelta
    :ivar max_task_retry_count: Note that this value specifically controls the number of retries
     for the Task executable due to a nonzero exit code. The Batch service will try the Task once,
     and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries
     the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the
     Batch service does not retry the Task after the first attempt. If the maximum retry count is
     -1, the Batch service retries the Task without limit, however this is not recommended for a
     start task or any task. The default value is 0 (no retries).
    :vartype max_task_retry_count: int
    """

    _attribute_map = {
        'max_wall_clock_time': {'key': 'maxWallClockTime', 'type': 'duration'},
        'retention_time': {'key': 'retentionTime', 'type': 'duration'},
        'max_task_retry_count': {'key': 'maxTaskRetryCount', 'type': 'int'},
    }

    def __init__(
        self,
        *,
        max_wall_clock_time: Optional[datetime.timedelta] = None,
        retention_time: Optional[datetime.timedelta] = None,
        max_task_retry_count: Optional[int] = None,
        **kwargs
    ):
        """
        :keyword max_wall_clock_time: If this is not specified, there is no time limit on how long the
         Task may run.
        :paramtype max_wall_clock_time: ~datetime.timedelta
        :keyword retention_time: The default is 7 days, i.e. the Task directory will be retained for 7
         days unless the Compute Node is removed or the Job is deleted.
        :paramtype retention_time: ~datetime.timedelta
        :keyword max_task_retry_count: Note that this value specifically controls the number of retries
         for the Task executable due to a nonzero exit code. The Batch service will try the Task once,
         and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries
         the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the
         Batch service does not retry the Task after the first attempt. If the maximum retry count is
         -1, the Batch service retries the Task without limit, however this is not recommended for a
         start task or any task. The default value is 0 (no retries).
        :paramtype max_task_retry_count: int
        """
        super(TaskConstraints, self).__init__(**kwargs)
        self.max_wall_clock_time = max_wall_clock_time
        self.retention_time = retention_time
        self.max_task_retry_count = max_task_retry_count


class TaskContainerExecutionInformation(msrest.serialization.Model):
    """Contains information about the container which a Task is executing.

    :ivar container_id: The ID of the container.
    :vartype container_id: str
    :ivar state: This is the state of the container according to the Docker service. It is
     equivalent to the status field returned by "docker inspect".
    :vartype state: str
    :ivar error: This is the detailed error string from the Docker service, if available. It is
     equivalent to the error field returned by "docker inspect".
    :vartype error: str
    """

    _attribute_map = {
        'container_id': {'key': 'containerId', 'type': 'str'},
        'state': {'key': 'state', 'type': 'str'},
        'error': {'key': 'error', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        container_id: Optional[str] = None,
        state: Optional[str] = None,
        error: Optional[str] = None,
        **kwargs
    ):
        """
        :keyword container_id: The ID of the container.
        :paramtype container_id: str
        :keyword state: This is the state of the container according to the Docker service. It is
         equivalent to the status field returned by "docker inspect".
        :paramtype state: str
        :keyword error: This is the detailed error string from the Docker service, if available. It is
         equivalent to the error field returned by "docker inspect".
        :paramtype error: str
        """
        super(TaskContainerExecutionInformation, self).__init__(**kwargs)
        self.container_id = container_id
        self.state = state
        self.error = error


class TaskContainerSettings(msrest.serialization.Model):
    """The container settings for a Task.

    All required parameters must be populated in order to send to Azure.

    :ivar container_run_options: These additional options are supplied as arguments to the "docker
     create" command, in addition to those controlled by the Batch Service.
    :vartype container_run_options: str
    :ivar image_name: Required. This is the full Image reference, as would be specified to "docker
     pull". If no tag is provided as part of the Image name, the tag ":latest" is used as a default.
    :vartype image_name: str
    :ivar registry: This setting can be omitted if was already provided at Pool creation.
    :vartype registry: ~azure-batch.models.ContainerRegistry
    :ivar working_directory: The default is 'taskWorkingDirectory'. Known values are:
     "taskWorkingDirectory", "containerImageDefault".
    :vartype working_directory: str or ~azure-batch.models.ContainerWorkingDirectory
    """

    _validation = {
        'image_name': {'required': True},
    }

    _attribute_map = {
        'container_run_options': {'key': 'containerRunOptions', 'type': 'str'},
        'image_name': {'key': 'imageName', 'type': 'str'},
        'registry': {'key': 'registry', 'type': 'ContainerRegistry'},
        'working_directory': {'key': 'workingDirectory', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        image_name: str,
        container_run_options: Optional[str] = None,
        registry: Optional["_models.ContainerRegistry"] = None,
        working_directory: Optional[Union[str, "_models.ContainerWorkingDirectory"]] = None,
        **kwargs
    ):
        """
        :keyword container_run_options: These additional options are supplied as arguments to the
         "docker create" command, in addition to those controlled by the Batch Service.
        :paramtype container_run_options: str
        :keyword image_name: Required. This is the full Image reference, as would be specified to
         "docker pull". If no tag is provided as part of the Image name, the tag ":latest" is used as a
         default.
        :paramtype image_name: str
        :keyword registry: This setting can be omitted if was already provided at Pool creation.
        :paramtype registry: ~azure-batch.models.ContainerRegistry
        :keyword working_directory: The default is 'taskWorkingDirectory'. Known values are:
         "taskWorkingDirectory", "containerImageDefault".
        :paramtype working_directory: str or ~azure-batch.models.ContainerWorkingDirectory
        """
        super(TaskContainerSettings, self).__init__(**kwargs)
        self.container_run_options = container_run_options
        self.image_name = image_name
        self.registry = registry
        self.working_directory = working_directory


class TaskCounts(msrest.serialization.Model):
    """The Task counts for a Job.

    All required parameters must be populated in order to send to Azure.

    :ivar active: Required. The number of Tasks in the active state.
    :vartype active: int
    :ivar running: Required. The number of Tasks in the running or preparing state.
    :vartype running: int
    :ivar completed: Required. The number of Tasks in the completed state.
    :vartype completed: int
    :ivar succeeded: Required. The number of Tasks which succeeded. A Task succeeds if its result
     (found in the executionInfo property) is 'success'.
    :vartype succeeded: int
    :ivar failed: Required. The number of Tasks which failed. A Task fails if its result (found in
     the executionInfo property) is 'failure'.
    :vartype failed: int
    """

    _validation = {
        'active': {'required': True},
        'running': {'required': True},
        'completed': {'required': True},
        'succeeded': {'required': True},
        'failed': {'required': True},
    }

    _attribute_map = {
        'active': {'key': 'active', 'type': 'int'},
        'running': {'key': 'running', 'type': 'int'},
        'completed': {'key': 'completed', 'type': 'int'},
        'succeeded': {'key': 'succeeded', 'type': 'int'},
        'failed': {'key': 'failed', 'type': 'int'},
    }

    def __init__(
        self,
        *,
        active: int,
        running: int,
        completed: int,
        succeeded: int,
        failed: int,
        **kwargs
    ):
        """
        :keyword active: Required. The number of Tasks in the active state.
        :paramtype active: int
        :keyword running: Required. The number of Tasks in the running or preparing state.
        :paramtype running: int
        :keyword completed: Required. The number of Tasks in the completed state.
        :paramtype completed: int
        :keyword succeeded: Required. The number of Tasks which succeeded. A Task succeeds if its
         result (found in the executionInfo property) is 'success'.
        :paramtype succeeded: int
        :keyword failed: Required. The number of Tasks which failed. A Task fails if its result (found
         in the executionInfo property) is 'failure'.
        :paramtype failed: int
        """
        super(TaskCounts, self).__init__(**kwargs)
        self.active = active
        self.running = running
        self.completed = completed
        self.succeeded = succeeded
        self.failed = failed


class TaskCountsResult(msrest.serialization.Model):
    """The Task and TaskSlot counts for a Job.

    All required parameters must be populated in order to send to Azure.

    :ivar task_counts: Required. The Task counts for a Job.
    :vartype task_counts: ~azure-batch.models.TaskCounts
    :ivar task_slot_counts: Required. The TaskSlot counts for a Job.
    :vartype task_slot_counts: ~azure-batch.models.TaskSlotCounts
    """

    _validation = {
        'task_counts': {'required': True},
        'task_slot_counts': {'required': True},
    }

    _attribute_map = {
        'task_counts': {'key': 'taskCounts', 'type': 'TaskCounts'},
        'task_slot_counts': {'key': 'taskSlotCounts', 'type': 'TaskSlotCounts'},
    }

    def __init__(
        self,
        *,
        task_counts: "_models.TaskCounts",
        task_slot_counts: "_models.TaskSlotCounts",
        **kwargs
    ):
        """
        :keyword task_counts: Required. The Task counts for a Job.
        :paramtype task_counts: ~azure-batch.models.TaskCounts
        :keyword task_slot_counts: Required. The TaskSlot counts for a Job.
        :paramtype task_slot_counts: ~azure-batch.models.TaskSlotCounts
        """
        super(TaskCountsResult, self).__init__(**kwargs)
        self.task_counts = task_counts
        self.task_slot_counts = task_slot_counts


class TaskDependencies(msrest.serialization.Model):
    """Specifies any dependencies of a Task. Any Task that is explicitly specified or within a dependency range must complete before the dependant Task will be scheduled.

    :ivar task_ids: The taskIds collection is limited to 64000 characters total (i.e. the combined
     length of all Task IDs). If the taskIds collection exceeds the maximum length, the Add Task
     request fails with error code TaskDependencyListTooLong. In this case consider using Task ID
     ranges instead.
    :vartype task_ids: list[str]
    :ivar task_id_ranges: The list of Task ID ranges that this Task depends on. All Tasks in all
     ranges must complete successfully before the dependent Task can be scheduled.
    :vartype task_id_ranges: list[~azure-batch.models.TaskIdRange]
    """

    _attribute_map = {
        'task_ids': {'key': 'taskIds', 'type': '[str]'},
        'task_id_ranges': {'key': 'taskIdRanges', 'type': '[TaskIdRange]'},
    }

    def __init__(
        self,
        *,
        task_ids: Optional[List[str]] = None,
        task_id_ranges: Optional[List["_models.TaskIdRange"]] = None,
        **kwargs
    ):
        """
        :keyword task_ids: The taskIds collection is limited to 64000 characters total (i.e. the
         combined length of all Task IDs). If the taskIds collection exceeds the maximum length, the Add
         Task request fails with error code TaskDependencyListTooLong. In this case consider using Task
         ID ranges instead.
        :paramtype task_ids: list[str]
        :keyword task_id_ranges: The list of Task ID ranges that this Task depends on. All Tasks in all
         ranges must complete successfully before the dependent Task can be scheduled.
        :paramtype task_id_ranges: list[~azure-batch.models.TaskIdRange]
        """
        super(TaskDependencies, self).__init__(**kwargs)
        self.task_ids = task_ids
        self.task_id_ranges = task_id_ranges


class TaskExecutionInformation(msrest.serialization.Model):
    """Information about the execution of a Task.

    All required parameters must be populated in order to send to Azure.

    :ivar start_time: 'Running' corresponds to the running state, so if the Task specifies resource
     files or Packages, then the start time reflects the time at which the Task started downloading
     or deploying these. If the Task has been restarted or retried, this is the most recent time at
     which the Task started running. This property is present only for Tasks that are in the running
     or completed state.
    :vartype start_time: ~datetime.datetime
    :ivar end_time: This property is set only if the Task is in the Completed state.
    :vartype end_time: ~datetime.datetime
    :ivar exit_code: This property is set only if the Task is in the completed state. In general,
     the exit code for a process reflects the specific convention implemented by the application
     developer for that process. If you use the exit code value to make decisions in your code, be
     sure that you know the exit code convention used by the application process. However, if the
     Batch service terminates the Task (due to timeout, or user termination via the API) you may see
     an operating system-defined exit code.
    :vartype exit_code: int
    :ivar container_info: This property is set only if the Task runs in a container context.
    :vartype container_info: ~azure-batch.models.TaskContainerExecutionInformation
    :ivar failure_info: This property is set only if the Task is in the completed state and
     encountered a failure.
    :vartype failure_info: ~azure-batch.models.TaskFailureInformation
    :ivar retry_count: Required. Task application failures (non-zero exit code) are retried,
     pre-processing errors (the Task could not be run) and file upload errors are not retried. The
     Batch service will retry the Task up to the limit specified by the constraints.
    :vartype retry_count: int
    :ivar last_retry_time: This element is present only if the Task was retried (i.e. retryCount is
     nonzero). If present, this is typically the same as startTime, but may be different if the Task
     has been restarted for reasons other than retry; for example, if the Compute Node was rebooted
     during a retry, then the startTime is updated but the lastRetryTime is not.
    :vartype last_retry_time: ~datetime.datetime
    :ivar requeue_count: Required. When the user removes Compute Nodes from a Pool (by
     resizing/shrinking the pool) or when the Job is being disabled, the user can specify that
     running Tasks on the Compute Nodes be requeued for execution. This count tracks how many times
     the Task has been requeued for these reasons.
    :vartype requeue_count: int
    :ivar last_requeue_time: This property is set only if the requeueCount is nonzero.
    :vartype last_requeue_time: ~datetime.datetime
    :ivar result: If the value is 'failed', then the details of the failure can be found in the
     failureInfo property. Known values are: "success", "failure".
    :vartype result: str or ~azure-batch.models.TaskExecutionResult
    """

    _validation = {
        'retry_count': {'required': True},
        'requeue_count': {'required': True},
    }

    _attribute_map = {
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'end_time': {'key': 'endTime', 'type': 'iso-8601'},
        'exit_code': {'key': 'exitCode', 'type': 'int'},
        'container_info': {'key': 'containerInfo', 'type': 'TaskContainerExecutionInformation'},
        'failure_info': {'key': 'failureInfo', 'type': 'TaskFailureInformation'},
        'retry_count': {'key': 'retryCount', 'type': 'int'},
        'last_retry_time': {'key': 'lastRetryTime', 'type': 'iso-8601'},
        'requeue_count': {'key': 'requeueCount', 'type': 'int'},
        'last_requeue_time': {'key': 'lastRequeueTime', 'type': 'iso-8601'},
        'result': {'key': 'result', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        retry_count: int,
        requeue_count: int,
        start_time: Optional[datetime.datetime] = None,
        end_time: Optional[datetime.datetime] = None,
        exit_code: Optional[int] = None,
        container_info: Optional["_models.TaskContainerExecutionInformation"] = None,
        failure_info: Optional["_models.TaskFailureInformation"] = None,
        last_retry_time: Optional[datetime.datetime] = None,
        last_requeue_time: Optional[datetime.datetime] = None,
        result: Optional[Union[str, "_models.TaskExecutionResult"]] = None,
        **kwargs
    ):
        """
        :keyword start_time: 'Running' corresponds to the running state, so if the Task specifies
         resource files or Packages, then the start time reflects the time at which the Task started
         downloading or deploying these. If the Task has been restarted or retried, this is the most
         recent time at which the Task started running. This property is present only for Tasks that are
         in the running or completed state.
        :paramtype start_time: ~datetime.datetime
        :keyword end_time: This property is set only if the Task is in the Completed state.
        :paramtype end_time: ~datetime.datetime
        :keyword exit_code: This property is set only if the Task is in the completed state. In
         general, the exit code for a process reflects the specific convention implemented by the
         application developer for that process. If you use the exit code value to make decisions in
         your code, be sure that you know the exit code convention used by the application process.
         However, if the Batch service terminates the Task (due to timeout, or user termination via the
         API) you may see an operating system-defined exit code.
        :paramtype exit_code: int
        :keyword container_info: This property is set only if the Task runs in a container context.
        :paramtype container_info: ~azure-batch.models.TaskContainerExecutionInformation
        :keyword failure_info: This property is set only if the Task is in the completed state and
         encountered a failure.
        :paramtype failure_info: ~azure-batch.models.TaskFailureInformation
        :keyword retry_count: Required. Task application failures (non-zero exit code) are retried,
         pre-processing errors (the Task could not be run) and file upload errors are not retried. The
         Batch service will retry the Task up to the limit specified by the constraints.
        :paramtype retry_count: int
        :keyword last_retry_time: This element is present only if the Task was retried (i.e. retryCount
         is nonzero). If present, this is typically the same as startTime, but may be different if the
         Task has been restarted for reasons other than retry; for example, if the Compute Node was
         rebooted during a retry, then the startTime is updated but the lastRetryTime is not.
        :paramtype last_retry_time: ~datetime.datetime
        :keyword requeue_count: Required. When the user removes Compute Nodes from a Pool (by
         resizing/shrinking the pool) or when the Job is being disabled, the user can specify that
         running Tasks on the Compute Nodes be requeued for execution. This count tracks how many times
         the Task has been requeued for these reasons.
        :paramtype requeue_count: int
        :keyword last_requeue_time: This property is set only if the requeueCount is nonzero.
        :paramtype last_requeue_time: ~datetime.datetime
        :keyword result: If the value is 'failed', then the details of the failure can be found in the
         failureInfo property. Known values are: "success", "failure".
        :paramtype result: str or ~azure-batch.models.TaskExecutionResult
        """
        super(TaskExecutionInformation, self).__init__(**kwargs)
        self.start_time = start_time
        self.end_time = end_time
        self.exit_code = exit_code
        self.container_info = container_info
        self.failure_info = failure_info
        self.retry_count = retry_count
        self.last_retry_time = last_retry_time
        self.requeue_count = requeue_count
        self.last_requeue_time = last_requeue_time
        self.result = result


class TaskFailureInformation(msrest.serialization.Model):
    """Information about a Task failure.

    All required parameters must be populated in order to send to Azure.

    :ivar category: Required. The category of the error. Known values are: "usererror",
     "servererror".
    :vartype category: str or ~azure-batch.models.ErrorCategory
    :ivar code: An identifier for the Task error. Codes are invariant and are intended to be
     consumed programmatically.
    :vartype code: str
    :ivar message: A message describing the Task error, intended to be suitable for display in a
     user interface.
    :vartype message: str
    :ivar details: A list of additional details related to the error.
    :vartype details: list[~azure-batch.models.NameValuePair]
    """

    _validation = {
        'category': {'required': True},
    }

    _attribute_map = {
        'category': {'key': 'category', 'type': 'str'},
        'code': {'key': 'code', 'type': 'str'},
        'message': {'key': 'message', 'type': 'str'},
        'details': {'key': 'details', 'type': '[NameValuePair]'},
    }

    def __init__(
        self,
        *,
        category: Union[str, "_models.ErrorCategory"],
        code: Optional[str] = None,
        message: Optional[str] = None,
        details: Optional[List["_models.NameValuePair"]] = None,
        **kwargs
    ):
        """
        :keyword category: Required. The category of the error. Known values are: "usererror",
         "servererror".
        :paramtype category: str or ~azure-batch.models.ErrorCategory
        :keyword code: An identifier for the Task error. Codes are invariant and are intended to be
         consumed programmatically.
        :paramtype code: str
        :keyword message: A message describing the Task error, intended to be suitable for display in a
         user interface.
        :paramtype message: str
        :keyword details: A list of additional details related to the error.
        :paramtype details: list[~azure-batch.models.NameValuePair]
        """
        super(TaskFailureInformation, self).__init__(**kwargs)
        self.category = category
        self.code = code
        self.message = message
        self.details = details


class TaskIdRange(msrest.serialization.Model):
    """The start and end of the range are inclusive. For example, if a range has start 9 and end 12, then it represents Tasks '9', '10', '11' and '12'.

    All required parameters must be populated in order to send to Azure.

    :ivar start: Required. The first Task ID in the range.
    :vartype start: int
    :ivar end: Required. The last Task ID in the range.
    :vartype end: int
    """

    _validation = {
        'start': {'required': True},
        'end': {'required': True},
    }

    _attribute_map = {
        'start': {'key': 'start', 'type': 'int'},
        'end': {'key': 'end', 'type': 'int'},
    }

    def __init__(
        self,
        *,
        start: int,
        end: int,
        **kwargs
    ):
        """
        :keyword start: Required. The first Task ID in the range.
        :paramtype start: int
        :keyword end: Required. The last Task ID in the range.
        :paramtype end: int
        """
        super(TaskIdRange, self).__init__(**kwargs)
        self.start = start
        self.end = end


class TaskInformation(msrest.serialization.Model):
    """Information about a Task running on a Compute Node.

    All required parameters must be populated in order to send to Azure.

    :ivar task_url: The URL of the Task.
    :vartype task_url: str
    :ivar job_id: The ID of the Job to which the Task belongs.
    :vartype job_id: str
    :ivar task_id: The ID of the Task.
    :vartype task_id: str
    :ivar subtask_id: The ID of the subtask if the Task is a multi-instance Task.
    :vartype subtask_id: int
    :ivar task_state: Required. The state of the Task. Known values are: "active", "preparing",
     "running", "completed".
    :vartype task_state: str or ~azure-batch.models.TaskState
    :ivar execution_info: Information about the execution of a Task.
    :vartype execution_info: ~azure-batch.models.TaskExecutionInformation
    """

    _validation = {
        'task_state': {'required': True},
    }

    _attribute_map = {
        'task_url': {'key': 'taskUrl', 'type': 'str'},
        'job_id': {'key': 'jobId', 'type': 'str'},
        'task_id': {'key': 'taskId', 'type': 'str'},
        'subtask_id': {'key': 'subtaskId', 'type': 'int'},
        'task_state': {'key': 'taskState', 'type': 'str'},
        'execution_info': {'key': 'executionInfo', 'type': 'TaskExecutionInformation'},
    }

    def __init__(
        self,
        *,
        task_state: Union[str, "_models.TaskState"],
        task_url: Optional[str] = None,
        job_id: Optional[str] = None,
        task_id: Optional[str] = None,
        subtask_id: Optional[int] = None,
        execution_info: Optional["_models.TaskExecutionInformation"] = None,
        **kwargs
    ):
        """
        :keyword task_url: The URL of the Task.
        :paramtype task_url: str
        :keyword job_id: The ID of the Job to which the Task belongs.
        :paramtype job_id: str
        :keyword task_id: The ID of the Task.
        :paramtype task_id: str
        :keyword subtask_id: The ID of the subtask if the Task is a multi-instance Task.
        :paramtype subtask_id: int
        :keyword task_state: Required. The state of the Task. Known values are: "active", "preparing",
         "running", "completed".
        :paramtype task_state: str or ~azure-batch.models.TaskState
        :keyword execution_info: Information about the execution of a Task.
        :paramtype execution_info: ~azure-batch.models.TaskExecutionInformation
        """
        super(TaskInformation, self).__init__(**kwargs)
        self.task_url = task_url
        self.job_id = job_id
        self.task_id = task_id
        self.subtask_id = subtask_id
        self.task_state = task_state
        self.execution_info = execution_info


class TaskSchedulingPolicy(msrest.serialization.Model):
    """Specifies how Tasks should be distributed across Compute Nodes.

    All required parameters must be populated in order to send to Azure.

    :ivar node_fill_type: Required. If not specified, the default is spread. Known values are:
     "spread", "pack".
    :vartype node_fill_type: str or ~azure-batch.models.ComputeNodeFillType
    """

    _validation = {
        'node_fill_type': {'required': True},
    }

    _attribute_map = {
        'node_fill_type': {'key': 'nodeFillType', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        node_fill_type: Union[str, "_models.ComputeNodeFillType"],
        **kwargs
    ):
        """
        :keyword node_fill_type: Required. If not specified, the default is spread. Known values are:
         "spread", "pack".
        :paramtype node_fill_type: str or ~azure-batch.models.ComputeNodeFillType
        """
        super(TaskSchedulingPolicy, self).__init__(**kwargs)
        self.node_fill_type = node_fill_type


class TaskSlotCounts(msrest.serialization.Model):
    """The TaskSlot counts for a Job.

    All required parameters must be populated in order to send to Azure.

    :ivar active: Required. The number of TaskSlots for active Tasks.
    :vartype active: int
    :ivar running: Required. The number of TaskSlots for running Tasks.
    :vartype running: int
    :ivar completed: Required. The number of TaskSlots for completed Tasks.
    :vartype completed: int
    :ivar succeeded: Required. The number of TaskSlots for succeeded Tasks.
    :vartype succeeded: int
    :ivar failed: Required. The number of TaskSlots for failed Tasks.
    :vartype failed: int
    """

    _validation = {
        'active': {'required': True},
        'running': {'required': True},
        'completed': {'required': True},
        'succeeded': {'required': True},
        'failed': {'required': True},
    }

    _attribute_map = {
        'active': {'key': 'active', 'type': 'int'},
        'running': {'key': 'running', 'type': 'int'},
        'completed': {'key': 'completed', 'type': 'int'},
        'succeeded': {'key': 'succeeded', 'type': 'int'},
        'failed': {'key': 'failed', 'type': 'int'},
    }

    def __init__(
        self,
        *,
        active: int,
        running: int,
        completed: int,
        succeeded: int,
        failed: int,
        **kwargs
    ):
        """
        :keyword active: Required. The number of TaskSlots for active Tasks.
        :paramtype active: int
        :keyword running: Required. The number of TaskSlots for running Tasks.
        :paramtype running: int
        :keyword completed: Required. The number of TaskSlots for completed Tasks.
        :paramtype completed: int
        :keyword succeeded: Required. The number of TaskSlots for succeeded Tasks.
        :paramtype succeeded: int
        :keyword failed: Required. The number of TaskSlots for failed Tasks.
        :paramtype failed: int
        """
        super(TaskSlotCounts, self).__init__(**kwargs)
        self.active = active
        self.running = running
        self.completed = completed
        self.succeeded = succeeded
        self.failed = failed


class TaskStatistics(msrest.serialization.Model):
    """Resource usage statistics for a Task.

    All required parameters must be populated in order to send to Azure.

    :ivar url: Required. The URL of the statistics.
    :vartype url: str
    :ivar start_time: Required. The start time of the time range covered by the statistics.
    :vartype start_time: ~datetime.datetime
    :ivar last_update_time: Required. The time at which the statistics were last updated. All
     statistics are limited to the range between startTime and lastUpdateTime.
    :vartype last_update_time: ~datetime.datetime
    :ivar user_cpu_time: Required. The total user mode CPU time (summed across all cores and all
     Compute Nodes) consumed by the Task.
    :vartype user_cpu_time: ~datetime.timedelta
    :ivar kernel_cpu_time: Required. The total kernel mode CPU time (summed across all cores and
     all Compute Nodes) consumed by the Task.
    :vartype kernel_cpu_time: ~datetime.timedelta
    :ivar wall_clock_time: Required. The wall clock time is the elapsed time from when the Task
     started running on a Compute Node to when it finished (or to the last time the statistics were
     updated, if the Task had not finished by then). If the Task was retried, this includes the wall
     clock time of all the Task retries.
    :vartype wall_clock_time: ~datetime.timedelta
    :ivar read_i_ops: Required. The total number of disk read operations made by the Task.
    :vartype read_i_ops: long
    :ivar write_i_ops: Required. The total number of disk write operations made by the Task.
    :vartype write_i_ops: long
    :ivar read_io_gi_b: Required. The total gibibytes read from disk by the Task.
    :vartype read_io_gi_b: float
    :ivar write_io_gi_b: Required. The total gibibytes written to disk by the Task.
    :vartype write_io_gi_b: float
    :ivar wait_time: Required. The total wait time of the Task. The wait time for a Task is defined
     as the elapsed time between the creation of the Task and the start of Task execution. (If the
     Task is retried due to failures, the wait time is the time to the most recent Task execution.).
    :vartype wait_time: ~datetime.timedelta
    """

    _validation = {
        'url': {'required': True},
        'start_time': {'required': True},
        'last_update_time': {'required': True},
        'user_cpu_time': {'required': True},
        'kernel_cpu_time': {'required': True},
        'wall_clock_time': {'required': True},
        'read_i_ops': {'required': True},
        'write_i_ops': {'required': True},
        'read_io_gi_b': {'required': True},
        'write_io_gi_b': {'required': True},
        'wait_time': {'required': True},
    }

    _attribute_map = {
        'url': {'key': 'url', 'type': 'str'},
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'last_update_time': {'key': 'lastUpdateTime', 'type': 'iso-8601'},
        'user_cpu_time': {'key': 'userCPUTime', 'type': 'duration'},
        'kernel_cpu_time': {'key': 'kernelCPUTime', 'type': 'duration'},
        'wall_clock_time': {'key': 'wallClockTime', 'type': 'duration'},
        'read_i_ops': {'key': 'readIOps', 'type': 'long'},
        'write_i_ops': {'key': 'writeIOps', 'type': 'long'},
        'read_io_gi_b': {'key': 'readIOGiB', 'type': 'float'},
        'write_io_gi_b': {'key': 'writeIOGiB', 'type': 'float'},
        'wait_time': {'key': 'waitTime', 'type': 'duration'},
    }

    def __init__(
        self,
        *,
        url: str,
        start_time: datetime.datetime,
        last_update_time: datetime.datetime,
        user_cpu_time: datetime.timedelta,
        kernel_cpu_time: datetime.timedelta,
        wall_clock_time: datetime.timedelta,
        read_i_ops: int,
        write_i_ops: int,
        read_io_gi_b: float,
        write_io_gi_b: float,
        wait_time: datetime.timedelta,
        **kwargs
    ):
        """
        :keyword url: Required. The URL of the statistics.
        :paramtype url: str
        :keyword start_time: Required. The start time of the time range covered by the statistics.
        :paramtype start_time: ~datetime.datetime
        :keyword last_update_time: Required. The time at which the statistics were last updated. All
         statistics are limited to the range between startTime and lastUpdateTime.
        :paramtype last_update_time: ~datetime.datetime
        :keyword user_cpu_time: Required. The total user mode CPU time (summed across all cores and all
         Compute Nodes) consumed by the Task.
        :paramtype user_cpu_time: ~datetime.timedelta
        :keyword kernel_cpu_time: Required. The total kernel mode CPU time (summed across all cores and
         all Compute Nodes) consumed by the Task.
        :paramtype kernel_cpu_time: ~datetime.timedelta
        :keyword wall_clock_time: Required. The wall clock time is the elapsed time from when the Task
         started running on a Compute Node to when it finished (or to the last time the statistics were
         updated, if the Task had not finished by then). If the Task was retried, this includes the wall
         clock time of all the Task retries.
        :paramtype wall_clock_time: ~datetime.timedelta
        :keyword read_i_ops: Required. The total number of disk read operations made by the Task.
        :paramtype read_i_ops: long
        :keyword write_i_ops: Required. The total number of disk write operations made by the Task.
        :paramtype write_i_ops: long
        :keyword read_io_gi_b: Required. The total gibibytes read from disk by the Task.
        :paramtype read_io_gi_b: float
        :keyword write_io_gi_b: Required. The total gibibytes written to disk by the Task.
        :paramtype write_io_gi_b: float
        :keyword wait_time: Required. The total wait time of the Task. The wait time for a Task is
         defined as the elapsed time between the creation of the Task and the start of Task execution.
         (If the Task is retried due to failures, the wait time is the time to the most recent Task
         execution.).
        :paramtype wait_time: ~datetime.timedelta
        """
        super(TaskStatistics, self).__init__(**kwargs)
        self.url = url
        self.start_time = start_time
        self.last_update_time = last_update_time
        self.user_cpu_time = user_cpu_time
        self.kernel_cpu_time = kernel_cpu_time
        self.wall_clock_time = wall_clock_time
        self.read_i_ops = read_i_ops
        self.write_i_ops = write_i_ops
        self.read_io_gi_b = read_io_gi_b
        self.write_io_gi_b = write_io_gi_b
        self.wait_time = wait_time


class UploadBatchServiceLogsConfiguration(msrest.serialization.Model):
    """The Azure Batch service log files upload configuration for a Compute Node.

    All required parameters must be populated in order to send to Azure.

    :ivar container_url: Required. If a user assigned managed identity is not being used, the URL
     must include a Shared Access Signature (SAS) granting write permissions to the container. The
     SAS duration must allow enough time for the upload to finish. The start time for SAS is
     optional and recommended to not be specified.
    :vartype container_url: str
    :ivar start_time: Required. Any log file containing a log message in the time range will be
     uploaded. This means that the operation might retrieve more logs than have been requested since
     the entire log file is always uploaded, but the operation should not retrieve fewer logs than
     have been requested.
    :vartype start_time: ~datetime.datetime
    :ivar end_time: Any log file containing a log message in the time range will be uploaded. This
     means that the operation might retrieve more logs than have been requested since the entire log
     file is always uploaded, but the operation should not retrieve fewer logs than have been
     requested. If omitted, the default is to upload all logs available after the startTime.
    :vartype end_time: ~datetime.datetime
    :ivar identity_reference: The identity must have write access to the Azure Blob Storage
     container.
    :vartype identity_reference: ~azure-batch.models.ComputeNodeIdentityReference
    """

    _validation = {
        'container_url': {'required': True},
        'start_time': {'required': True},
    }

    _attribute_map = {
        'container_url': {'key': 'containerUrl', 'type': 'str'},
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'end_time': {'key': 'endTime', 'type': 'iso-8601'},
        'identity_reference': {'key': 'identityReference', 'type': 'ComputeNodeIdentityReference'},
    }

    def __init__(
        self,
        *,
        container_url: str,
        start_time: datetime.datetime,
        end_time: Optional[datetime.datetime] = None,
        identity_reference: Optional["_models.ComputeNodeIdentityReference"] = None,
        **kwargs
    ):
        """
        :keyword container_url: Required. If a user assigned managed identity is not being used, the
         URL must include a Shared Access Signature (SAS) granting write permissions to the container.
         The SAS duration must allow enough time for the upload to finish. The start time for SAS is
         optional and recommended to not be specified.
        :paramtype container_url: str
        :keyword start_time: Required. Any log file containing a log message in the time range will be
         uploaded. This means that the operation might retrieve more logs than have been requested since
         the entire log file is always uploaded, but the operation should not retrieve fewer logs than
         have been requested.
        :paramtype start_time: ~datetime.datetime
        :keyword end_time: Any log file containing a log message in the time range will be uploaded.
         This means that the operation might retrieve more logs than have been requested since the
         entire log file is always uploaded, but the operation should not retrieve fewer logs than have
         been requested. If omitted, the default is to upload all logs available after the startTime.
        :paramtype end_time: ~datetime.datetime
        :keyword identity_reference: The identity must have write access to the Azure Blob Storage
         container.
        :paramtype identity_reference: ~azure-batch.models.ComputeNodeIdentityReference
        """
        super(UploadBatchServiceLogsConfiguration, self).__init__(**kwargs)
        self.container_url = container_url
        self.start_time = start_time
        self.end_time = end_time
        self.identity_reference = identity_reference


class UploadBatchServiceLogsResult(msrest.serialization.Model):
    """The result of uploading Batch service log files from a specific Compute Node.

    All required parameters must be populated in order to send to Azure.

    :ivar virtual_directory_name: Required. The virtual directory name is part of the blob name for
     each log file uploaded, and it is built based poolId, nodeId and a unique identifier.
    :vartype virtual_directory_name: str
    :ivar number_of_files_uploaded: Required. The number of log files which will be uploaded.
    :vartype number_of_files_uploaded: int
    """

    _validation = {
        'virtual_directory_name': {'required': True},
        'number_of_files_uploaded': {'required': True},
    }

    _attribute_map = {
        'virtual_directory_name': {'key': 'virtualDirectoryName', 'type': 'str'},
        'number_of_files_uploaded': {'key': 'numberOfFilesUploaded', 'type': 'int'},
    }

    def __init__(
        self,
        *,
        virtual_directory_name: str,
        number_of_files_uploaded: int,
        **kwargs
    ):
        """
        :keyword virtual_directory_name: Required. The virtual directory name is part of the blob name
         for each log file uploaded, and it is built based poolId, nodeId and a unique identifier.
        :paramtype virtual_directory_name: str
        :keyword number_of_files_uploaded: Required. The number of log files which will be uploaded.
        :paramtype number_of_files_uploaded: int
        """
        super(UploadBatchServiceLogsResult, self).__init__(**kwargs)
        self.virtual_directory_name = virtual_directory_name
        self.number_of_files_uploaded = number_of_files_uploaded


class UsageStatistics(msrest.serialization.Model):
    """Statistics related to Pool usage information.

    All required parameters must be populated in order to send to Azure.

    :ivar start_time: Required. The start time of the time range covered by the statistics.
    :vartype start_time: ~datetime.datetime
    :ivar last_update_time: Required. The time at which the statistics were last updated. All
     statistics are limited to the range between startTime and lastUpdateTime.
    :vartype last_update_time: ~datetime.datetime
    :ivar dedicated_core_time: Required. The aggregated wall-clock time of the dedicated Compute
     Node cores being part of the Pool.
    :vartype dedicated_core_time: ~datetime.timedelta
    """

    _validation = {
        'start_time': {'required': True},
        'last_update_time': {'required': True},
        'dedicated_core_time': {'required': True},
    }

    _attribute_map = {
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'last_update_time': {'key': 'lastUpdateTime', 'type': 'iso-8601'},
        'dedicated_core_time': {'key': 'dedicatedCoreTime', 'type': 'duration'},
    }

    def __init__(
        self,
        *,
        start_time: datetime.datetime,
        last_update_time: datetime.datetime,
        dedicated_core_time: datetime.timedelta,
        **kwargs
    ):
        """
        :keyword start_time: Required. The start time of the time range covered by the statistics.
        :paramtype start_time: ~datetime.datetime
        :keyword last_update_time: Required. The time at which the statistics were last updated. All
         statistics are limited to the range between startTime and lastUpdateTime.
        :paramtype last_update_time: ~datetime.datetime
        :keyword dedicated_core_time: Required. The aggregated wall-clock time of the dedicated Compute
         Node cores being part of the Pool.
        :paramtype dedicated_core_time: ~datetime.timedelta
        """
        super(UsageStatistics, self).__init__(**kwargs)
        self.start_time = start_time
        self.last_update_time = last_update_time
        self.dedicated_core_time = dedicated_core_time


class UserAccount(msrest.serialization.Model):
    """Properties used to create a user used to execute Tasks on an Azure Batch Compute Node.

    All required parameters must be populated in order to send to Azure.

    :ivar name: Required. The name of the user Account.
    :vartype name: str
    :ivar password: Required. The password for the user Account.
    :vartype password: str
    :ivar elevation_level: The default value is nonAdmin. Known values are: "nonadmin", "admin".
    :vartype elevation_level: str or ~azure-batch.models.ElevationLevel
    :ivar linux_user_configuration: This property is ignored if specified on a Windows Pool. If not
     specified, the user is created with the default options.
    :vartype linux_user_configuration: ~azure-batch.models.LinuxUserConfiguration
    :ivar windows_user_configuration: This property can only be specified if the user is on a
     Windows Pool. If not specified and on a Windows Pool, the user is created with the default
     options.
    :vartype windows_user_configuration: ~azure-batch.models.WindowsUserConfiguration
    """

    _validation = {
        'name': {'required': True},
        'password': {'required': True},
    }

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'password': {'key': 'password', 'type': 'str'},
        'elevation_level': {'key': 'elevationLevel', 'type': 'str'},
        'linux_user_configuration': {'key': 'linuxUserConfiguration', 'type': 'LinuxUserConfiguration'},
        'windows_user_configuration': {'key': 'windowsUserConfiguration', 'type': 'WindowsUserConfiguration'},
    }

    def __init__(
        self,
        *,
        name: str,
        password: str,
        elevation_level: Optional[Union[str, "_models.ElevationLevel"]] = None,
        linux_user_configuration: Optional["_models.LinuxUserConfiguration"] = None,
        windows_user_configuration: Optional["_models.WindowsUserConfiguration"] = None,
        **kwargs
    ):
        """
        :keyword name: Required. The name of the user Account.
        :paramtype name: str
        :keyword password: Required. The password for the user Account.
        :paramtype password: str
        :keyword elevation_level: The default value is nonAdmin. Known values are: "nonadmin", "admin".
        :paramtype elevation_level: str or ~azure-batch.models.ElevationLevel
        :keyword linux_user_configuration: This property is ignored if specified on a Windows Pool. If
         not specified, the user is created with the default options.
        :paramtype linux_user_configuration: ~azure-batch.models.LinuxUserConfiguration
        :keyword windows_user_configuration: This property can only be specified if the user is on a
         Windows Pool. If not specified and on a Windows Pool, the user is created with the default
         options.
        :paramtype windows_user_configuration: ~azure-batch.models.WindowsUserConfiguration
        """
        super(UserAccount, self).__init__(**kwargs)
        self.name = name
        self.password = password
        self.elevation_level = elevation_level
        self.linux_user_configuration = linux_user_configuration
        self.windows_user_configuration = windows_user_configuration


class UserAssignedIdentity(msrest.serialization.Model):
    """The user assigned Identity.

    Variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar resource_id: Required. The ARM resource id of the user assigned identity.
    :vartype resource_id: str
    :ivar client_id: The client id of the user assigned identity.
    :vartype client_id: str
    :ivar principal_id: The principal id of the user assigned identity.
    :vartype principal_id: str
    """

    _validation = {
        'resource_id': {'required': True},
        'client_id': {'readonly': True},
        'principal_id': {'readonly': True},
    }

    _attribute_map = {
        'resource_id': {'key': 'resourceId', 'type': 'str'},
        'client_id': {'key': 'clientId', 'type': 'str'},
        'principal_id': {'key': 'principalId', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        resource_id: str,
        **kwargs
    ):
        """
        :keyword resource_id: Required. The ARM resource id of the user assigned identity.
        :paramtype resource_id: str
        """
        super(UserAssignedIdentity, self).__init__(**kwargs)
        self.resource_id = resource_id
        self.client_id = None
        self.principal_id = None


class UserIdentity(msrest.serialization.Model):
    """Specify either the userName or autoUser property, but not both.

    :ivar user_name: The userName and autoUser properties are mutually exclusive; you must specify
     one but not both.
    :vartype user_name: str
    :ivar auto_user: The userName and autoUser properties are mutually exclusive; you must specify
     one but not both.
    :vartype auto_user: ~azure-batch.models.AutoUserSpecification
    """

    _attribute_map = {
        'user_name': {'key': 'username', 'type': 'str'},
        'auto_user': {'key': 'autoUser', 'type': 'AutoUserSpecification'},
    }

    def __init__(
        self,
        *,
        user_name: Optional[str] = None,
        auto_user: Optional["_models.AutoUserSpecification"] = None,
        **kwargs
    ):
        """
        :keyword user_name: The userName and autoUser properties are mutually exclusive; you must
         specify one but not both.
        :paramtype user_name: str
        :keyword auto_user: The userName and autoUser properties are mutually exclusive; you must
         specify one but not both.
        :paramtype auto_user: ~azure-batch.models.AutoUserSpecification
        """
        super(UserIdentity, self).__init__(**kwargs)
        self.user_name = user_name
        self.auto_user = auto_user


class VirtualMachineConfiguration(msrest.serialization.Model):
    """The configuration for Compute Nodes in a Pool based on the Azure Virtual Machines infrastructure.

    All required parameters must be populated in order to send to Azure.

    :ivar image_reference: Required. A reference to an Azure Virtual Machines Marketplace Image or
     a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references
     verified by Azure Batch, see the 'List Supported Images' operation.
    :vartype image_reference: ~azure-batch.models.ImageReference
    :ivar node_agent_sku_id: Required. The Batch Compute Node agent is a program that runs on each
     Compute Node in the Pool, and provides the command-and-control interface between the Compute
     Node and the Batch service. There are different implementations of the Compute Node agent,
     known as SKUs, for different operating systems. You must specify a Compute Node agent SKU which
     matches the selected Image reference. To get the list of supported Compute Node agent SKUs
     along with their list of verified Image references, see the 'List supported Compute Node agent
     SKUs' operation.
    :vartype node_agent_sku_id: str
    :ivar windows_configuration: This property must not be specified if the imageReference property
     specifies a Linux OS Image.
    :vartype windows_configuration: ~azure-batch.models.WindowsConfiguration
    :ivar data_disks: This property must be specified if the Compute Nodes in the Pool need to have
     empty data disks attached to them. This cannot be updated. Each Compute Node gets its own disk
     (the disk is not a file share). Existing disks cannot be attached, each attached disk is empty.
     When the Compute Node is removed from the Pool, the disk and all data associated with it is
     also deleted. The disk is not formatted after being attached, it must be formatted before use -
     for more information see
     https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux
     and
     https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
    :vartype data_disks: list[~azure-batch.models.DataDisk]
    :ivar license_type: This only applies to Images that contain the Windows operating system, and
     should only be used when you hold valid on-premises licenses for the Compute Nodes which will
     be deployed. If omitted, no on-premises licensing discount is applied. Values are:
    
      Windows_Server - The on-premises license is for Windows Server.
      Windows_Client - The on-premises license is for Windows Client.
    :vartype license_type: str
    :ivar container_configuration: If specified, setup is performed on each Compute Node in the
     Pool to allow Tasks to run in containers. All regular Tasks and Job manager Tasks run on this
     Pool must specify the containerSettings property, and all other Tasks may specify it.
    :vartype container_configuration: ~azure-batch.models.ContainerConfiguration
    :ivar disk_encryption_configuration: If specified, encryption is performed on each node in the
     pool during node provisioning.
    :vartype disk_encryption_configuration: ~azure-batch.models.DiskEncryptionConfiguration
    :ivar node_placement_configuration: This configuration will specify rules on how nodes in the
     pool will be physically allocated.
    :vartype node_placement_configuration: ~azure-batch.models.NodePlacementConfiguration
    :ivar extensions: If specified, the extensions mentioned in this configuration will be
     installed on each node.
    :vartype extensions: list[~azure-batch.models.VMExtension]
    :ivar os_disk: Settings for the operating system disk of the compute node (VM).
    :vartype os_disk: ~azure-batch.models.OSDisk
    """

    _validation = {
        'image_reference': {'required': True},
        'node_agent_sku_id': {'required': True},
    }

    _attribute_map = {
        'image_reference': {'key': 'imageReference', 'type': 'ImageReference'},
        'node_agent_sku_id': {'key': 'nodeAgentSKUId', 'type': 'str'},
        'windows_configuration': {'key': 'windowsConfiguration', 'type': 'WindowsConfiguration'},
        'data_disks': {'key': 'dataDisks', 'type': '[DataDisk]'},
        'license_type': {'key': 'licenseType', 'type': 'str'},
        'container_configuration': {'key': 'containerConfiguration', 'type': 'ContainerConfiguration'},
        'disk_encryption_configuration': {'key': 'diskEncryptionConfiguration', 'type': 'DiskEncryptionConfiguration'},
        'node_placement_configuration': {'key': 'nodePlacementConfiguration', 'type': 'NodePlacementConfiguration'},
        'extensions': {'key': 'extensions', 'type': '[VMExtension]'},
        'os_disk': {'key': 'osDisk', 'type': 'OSDisk'},
    }

    def __init__(
        self,
        *,
        image_reference: "_models.ImageReference",
        node_agent_sku_id: str,
        windows_configuration: Optional["_models.WindowsConfiguration"] = None,
        data_disks: Optional[List["_models.DataDisk"]] = None,
        license_type: Optional[str] = None,
        container_configuration: Optional["_models.ContainerConfiguration"] = None,
        disk_encryption_configuration: Optional["_models.DiskEncryptionConfiguration"] = None,
        node_placement_configuration: Optional["_models.NodePlacementConfiguration"] = None,
        extensions: Optional[List["_models.VMExtension"]] = None,
        os_disk: Optional["_models.OSDisk"] = None,
        **kwargs
    ):
        """
        :keyword image_reference: Required. A reference to an Azure Virtual Machines Marketplace Image
         or a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references
         verified by Azure Batch, see the 'List Supported Images' operation.
        :paramtype image_reference: ~azure-batch.models.ImageReference
        :keyword node_agent_sku_id: Required. The Batch Compute Node agent is a program that runs on
         each Compute Node in the Pool, and provides the command-and-control interface between the
         Compute Node and the Batch service. There are different implementations of the Compute Node
         agent, known as SKUs, for different operating systems. You must specify a Compute Node agent
         SKU which matches the selected Image reference. To get the list of supported Compute Node agent
         SKUs along with their list of verified Image references, see the 'List supported Compute Node
         agent SKUs' operation.
        :paramtype node_agent_sku_id: str
        :keyword windows_configuration: This property must not be specified if the imageReference
         property specifies a Linux OS Image.
        :paramtype windows_configuration: ~azure-batch.models.WindowsConfiguration
        :keyword data_disks: This property must be specified if the Compute Nodes in the Pool need to
         have empty data disks attached to them. This cannot be updated. Each Compute Node gets its own
         disk (the disk is not a file share). Existing disks cannot be attached, each attached disk is
         empty. When the Compute Node is removed from the Pool, the disk and all data associated with it
         is also deleted. The disk is not formatted after being attached, it must be formatted before
         use - for more information see
         https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux
         and
         https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
        :paramtype data_disks: list[~azure-batch.models.DataDisk]
        :keyword license_type: This only applies to Images that contain the Windows operating system,
         and should only be used when you hold valid on-premises licenses for the Compute Nodes which
         will be deployed. If omitted, no on-premises licensing discount is applied. Values are:
        
          Windows_Server - The on-premises license is for Windows Server.
          Windows_Client - The on-premises license is for Windows Client.
        :paramtype license_type: str
        :keyword container_configuration: If specified, setup is performed on each Compute Node in the
         Pool to allow Tasks to run in containers. All regular Tasks and Job manager Tasks run on this
         Pool must specify the containerSettings property, and all other Tasks may specify it.
        :paramtype container_configuration: ~azure-batch.models.ContainerConfiguration
        :keyword disk_encryption_configuration: If specified, encryption is performed on each node in
         the pool during node provisioning.
        :paramtype disk_encryption_configuration: ~azure-batch.models.DiskEncryptionConfiguration
        :keyword node_placement_configuration: This configuration will specify rules on how nodes in
         the pool will be physically allocated.
        :paramtype node_placement_configuration: ~azure-batch.models.NodePlacementConfiguration
        :keyword extensions: If specified, the extensions mentioned in this configuration will be
         installed on each node.
        :paramtype extensions: list[~azure-batch.models.VMExtension]
        :keyword os_disk: Settings for the operating system disk of the compute node (VM).
        :paramtype os_disk: ~azure-batch.models.OSDisk
        """
        super(VirtualMachineConfiguration, self).__init__(**kwargs)
        self.image_reference = image_reference
        self.node_agent_sku_id = node_agent_sku_id
        self.windows_configuration = windows_configuration
        self.data_disks = data_disks
        self.license_type = license_type
        self.container_configuration = container_configuration
        self.disk_encryption_configuration = disk_encryption_configuration
        self.node_placement_configuration = node_placement_configuration
        self.extensions = extensions
        self.os_disk = os_disk


class VirtualMachineInfo(msrest.serialization.Model):
    """Info about the current state of the virtual machine.

    :ivar image_reference: A reference to an Azure Virtual Machines Marketplace Image or a Shared
     Image Gallery Image. To get the list of all Azure Marketplace Image references verified by
     Azure Batch, see the 'List Supported Images' operation.
    :vartype image_reference: ~azure-batch.models.ImageReference
    """

    _attribute_map = {
        'image_reference': {'key': 'imageReference', 'type': 'ImageReference'},
    }

    def __init__(
        self,
        *,
        image_reference: Optional["_models.ImageReference"] = None,
        **kwargs
    ):
        """
        :keyword image_reference: A reference to an Azure Virtual Machines Marketplace Image or a
         Shared Image Gallery Image. To get the list of all Azure Marketplace Image references verified
         by Azure Batch, see the 'List Supported Images' operation.
        :paramtype image_reference: ~azure-batch.models.ImageReference
        """
        super(VirtualMachineInfo, self).__init__(**kwargs)
        self.image_reference = image_reference


class VMExtension(msrest.serialization.Model):
    """The configuration for virtual machine extensions.

    All required parameters must be populated in order to send to Azure.

    :ivar name: Required. The name of the virtual machine extension.
    :vartype name: str
    :ivar publisher: Required. The name of the extension handler publisher.
    :vartype publisher: str
    :ivar type: Required. The type of the extension.
    :vartype type: str
    :ivar type_handler_version: The version of script handler.
    :vartype type_handler_version: str
    :ivar auto_upgrade_minor_version: Indicates whether the extension should use a newer minor
     version if one is available at deployment time. Once deployed, however, the extension will not
     upgrade minor versions unless redeployed, even with this property set to true.
    :vartype auto_upgrade_minor_version: bool
    :ivar settings: Any object.
    :vartype settings: any
    :ivar protected_settings: The extension can contain either protectedSettings or
     protectedSettingsFromKeyVault or no protected settings at all.
    :vartype protected_settings: any
    :ivar provision_after_extensions: Collection of extension names after which this extension
     needs to be provisioned.
    :vartype provision_after_extensions: list[str]
    """

    _validation = {
        'name': {'required': True},
        'publisher': {'required': True},
        'type': {'required': True},
    }

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'publisher': {'key': 'publisher', 'type': 'str'},
        'type': {'key': 'type', 'type': 'str'},
        'type_handler_version': {'key': 'typeHandlerVersion', 'type': 'str'},
        'auto_upgrade_minor_version': {'key': 'autoUpgradeMinorVersion', 'type': 'bool'},
        'settings': {'key': 'settings', 'type': 'object'},
        'protected_settings': {'key': 'protectedSettings', 'type': 'object'},
        'provision_after_extensions': {'key': 'provisionAfterExtensions', 'type': '[str]'},
    }

    def __init__(
        self,
        *,
        name: str,
        publisher: str,
        type: str,
        type_handler_version: Optional[str] = None,
        auto_upgrade_minor_version: Optional[bool] = None,
        settings: Optional[Any] = None,
        protected_settings: Optional[Any] = None,
        provision_after_extensions: Optional[List[str]] = None,
        **kwargs
    ):
        """
        :keyword name: Required. The name of the virtual machine extension.
        :paramtype name: str
        :keyword publisher: Required. The name of the extension handler publisher.
        :paramtype publisher: str
        :keyword type: Required. The type of the extension.
        :paramtype type: str
        :keyword type_handler_version: The version of script handler.
        :paramtype type_handler_version: str
        :keyword auto_upgrade_minor_version: Indicates whether the extension should use a newer minor
         version if one is available at deployment time. Once deployed, however, the extension will not
         upgrade minor versions unless redeployed, even with this property set to true.
        :paramtype auto_upgrade_minor_version: bool
        :keyword settings: Any object.
        :paramtype settings: any
        :keyword protected_settings: The extension can contain either protectedSettings or
         protectedSettingsFromKeyVault or no protected settings at all.
        :paramtype protected_settings: any
        :keyword provision_after_extensions: Collection of extension names after which this extension
         needs to be provisioned.
        :paramtype provision_after_extensions: list[str]
        """
        super(VMExtension, self).__init__(**kwargs)
        self.name = name
        self.publisher = publisher
        self.type = type
        self.type_handler_version = type_handler_version
        self.auto_upgrade_minor_version = auto_upgrade_minor_version
        self.settings = settings
        self.protected_settings = protected_settings
        self.provision_after_extensions = provision_after_extensions


class VMExtensionInstanceView(msrest.serialization.Model):
    """The vm extension instance view.

    :ivar name: The name of the vm extension instance view.
    :vartype name: str
    :ivar statuses: The resource status information.
    :vartype statuses: list[~azure-batch.models.InstanceViewStatus]
    :ivar sub_statuses: The resource status information.
    :vartype sub_statuses: list[~azure-batch.models.InstanceViewStatus]
    """

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'statuses': {'key': 'statuses', 'type': '[InstanceViewStatus]'},
        'sub_statuses': {'key': 'subStatuses', 'type': '[InstanceViewStatus]'},
    }

    def __init__(
        self,
        *,
        name: Optional[str] = None,
        statuses: Optional[List["_models.InstanceViewStatus"]] = None,
        sub_statuses: Optional[List["_models.InstanceViewStatus"]] = None,
        **kwargs
    ):
        """
        :keyword name: The name of the vm extension instance view.
        :paramtype name: str
        :keyword statuses: The resource status information.
        :paramtype statuses: list[~azure-batch.models.InstanceViewStatus]
        :keyword sub_statuses: The resource status information.
        :paramtype sub_statuses: list[~azure-batch.models.InstanceViewStatus]
        """
        super(VMExtensionInstanceView, self).__init__(**kwargs)
        self.name = name
        self.statuses = statuses
        self.sub_statuses = sub_statuses


class WindowsConfiguration(msrest.serialization.Model):
    """Windows operating system settings to apply to the virtual machine.

    :ivar enable_automatic_updates: If omitted, the default value is true.
    :vartype enable_automatic_updates: bool
    """

    _attribute_map = {
        'enable_automatic_updates': {'key': 'enableAutomaticUpdates', 'type': 'bool'},
    }

    def __init__(
        self,
        *,
        enable_automatic_updates: Optional[bool] = None,
        **kwargs
    ):
        """
        :keyword enable_automatic_updates: If omitted, the default value is true.
        :paramtype enable_automatic_updates: bool
        """
        super(WindowsConfiguration, self).__init__(**kwargs)
        self.enable_automatic_updates = enable_automatic_updates


class WindowsUserConfiguration(msrest.serialization.Model):
    """Properties used to create a user Account on a Windows Compute Node.

    :ivar login_mode: The default value for VirtualMachineConfiguration Pools is 'batch' and for
     CloudServiceConfiguration Pools is 'interactive'. Known values are: "batch", "interactive".
    :vartype login_mode: str or ~azure-batch.models.LoginMode
    """

    _attribute_map = {
        'login_mode': {'key': 'loginMode', 'type': 'str'},
    }

    def __init__(
        self,
        *,
        login_mode: Optional[Union[str, "_models.LoginMode"]] = None,
        **kwargs
    ):
        """
        :keyword login_mode: The default value for VirtualMachineConfiguration Pools is 'batch' and for
         CloudServiceConfiguration Pools is 'interactive'. Known values are: "batch", "interactive".
        :paramtype login_mode: str or ~azure-batch.models.LoginMode
        """
        super(WindowsUserConfiguration, self).__init__(**kwargs)
        self.login_mode = login_mode
